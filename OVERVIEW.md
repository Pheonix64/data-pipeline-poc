# ğŸ¯ Data Pipeline POC BCEAO - Vue d'Ensemble Rapide

**Version**: 1.0.0 | **Statut**: âœ… Production Ready

---

## ğŸ“Š En Un Coup d'Å’il

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         DATA PIPELINE POC - BCEAO                       â”‚
â”‚    Architecture Moderne Data Lakehouse                  â”‚
â”‚    Pattern MÃ©daillon (Bronze â†’ Silver â†’ Gold)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ¥‰ BRONZE          â†’    ğŸ¥ˆ SILVER         â†’    ğŸ¥‡ GOLD
DonnÃ©es Brutes          DonnÃ©es NettoyÃ©es     DonnÃ©es Analytiques
20 Ã©vÃ©nements           20 Ã©vÃ©nements         20 Ã©vÃ©nements enrichis
6 utilisateurs          6 utilisateurs        + info utilisateurs
```

---

## âš¡ DÃ©marrage en 3 Minutes

```bash
# 1. CrÃ©er le fichier .env (copier .env.example)
MINIO_ROOT_USER=admin
MINIO_ROOT_PASSWORD=password123

# 2. DÃ©marrer les services
docker-compose up -d

# 3. VÃ©rifier l'installation
docker-compose ps
```

**AccÃ¨s rapides** :
- ğŸ–¥ï¸ MinIO Console: http://localhost:9001
- ğŸ““ Jupyter: http://localhost:8888
- âš¡ Spark UI: http://localhost:4040

---

## ğŸ—ï¸ Stack Technique

| Composant | RÃ´le | Port |
|-----------|------|------|
| **MinIO** | Stockage S3 | 9000, 9001 |
| **Apache Iceberg** | Format de table ACID | - |
| **Spark 3.5** | Traitement de donnÃ©es | 4040, 8888, 10000 |
| **dbt** | Transformations SQL | - |
| **TimescaleDB** | Time-series DB | 5433 |
| **ChromaDB** | Vector DB | 8010 |

---

## ğŸ“ˆ Architecture des DonnÃ©es

### Flux de DonnÃ©es

```
Sources de DonnÃ©es
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BRONZE LAYER    â”‚  â† DonnÃ©es brutes (append-only)
â”‚  bronze.*        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ dbt clean & validate
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SILVER LAYER    â”‚  â† DonnÃ©es nettoyÃ©es (validated)
â”‚  default_silver.*â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ dbt enrich & aggregate
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   GOLD LAYER     â”‚  â† DonnÃ©es analytiques (enriched)
â”‚  default_gold.*  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
   BI / Analytics / ML
```

### Tables Principales

#### ğŸ¥‰ Bronze
```sql
bronze.raw_events    -- Ã‰vÃ©nements systÃ¨me bruts
bronze.raw_users     -- Utilisateurs systÃ¨me bruts
```

#### ğŸ¥ˆ Silver
```sql
default_silver.stg_events  -- Ã‰vÃ©nements nettoyÃ©s
default_silver.stg_users   -- Utilisateurs nettoyÃ©s
```

#### ğŸ¥‡ Gold
```sql
default_gold.fct_events_enriched  -- Ã‰vÃ©nements + donnÃ©es utilisateurs
```

---

## ğŸš€ Commandes Essentielles

### Gestion SystÃ¨me

```bash
# DÃ©marrer
docker-compose up -d

# Statut
docker-compose ps

# Logs
docker-compose logs -f spark-iceberg

# ArrÃªter
docker-compose down
```

### dbt - Transformations

```bash
# ExÃ©cuter toutes les transformations
docker exec dbt dbt run

# Tests de qualitÃ©
docker exec dbt dbt test

# VÃ©rifier la connexion
docker exec dbt dbt debug
```

### Spark SQL

```bash
# Lister les namespaces
docker exec spark-iceberg beeline -u jdbc:hive2://localhost:10000 \
  -e "SHOW NAMESPACES;"

# Compter les Ã©vÃ©nements
docker exec spark-iceberg beeline -u jdbc:hive2://localhost:10000 \
  -e "SELECT COUNT(*) FROM bronze.raw_events;"

# RequÃªte sur Gold
docker exec spark-iceberg beeline -u jdbc:hive2://localhost:10000 \
  -e "SELECT * FROM default_gold.fct_events_enriched LIMIT 5;"
```

---

## ğŸ“š Documentation

| Document | Description | Temps |
|----------|-------------|-------|
| [ğŸ“– DOCUMENTATION_INDEX.md](./DOCUMENTATION_INDEX.md) | Index de toute la documentation | 5 min |
| [âš¡ QUICKSTART_FR.md](./QUICKSTART_FR.md) | Guide de dÃ©marrage | 15 min |
| [ğŸ“˜ README_FR.md](./README_FR.md) | Documentation complÃ¨te | 45 min |
| [ğŸ”„ TRANSFORMATION_GUIDE_FR.md](./TRANSFORMATION_GUIDE_FR.md) | Guide transformations | 30 min |
| [ğŸ“‹ VERSION_INFO.md](./VERSION_INFO.md) | Informations de version | 20 min |

---

## ğŸ¯ Cas d'Usage Typiques

### 1. Charger des DonnÃ©es Brutes

```python
# Via Jupyter Notebook (http://localhost:8888)
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("Data Loader").getOrCreate()

# InsÃ©rer dans Bronze
spark.sql("""
    INSERT INTO bronze.raw_events VALUES 
    (100, 'login', 101, current_timestamp(), 'ip:192.168.1.100')
""")
```

### 2. Transformer Bronze â†’ Silver â†’ Gold

```bash
# ExÃ©cuter les transformations dbt
docker exec dbt dbt run

# RÃ©sultat:
# âœ“ stg_events (Bronze â†’ Silver)
# âœ“ stg_users (Bronze â†’ Silver)
# âœ“ fct_events_enriched (Silver â†’ Gold)
```

### 3. Analyser les DonnÃ©es Gold

```python
# Via Jupyter ou Beeline
df = spark.sql("""
    SELECT 
        event_type,
        user_name,
        COUNT(*) as event_count
    FROM default_gold.fct_events_enriched
    GROUP BY event_type, user_name
    ORDER BY event_count DESC
""")
df.show()
```

### 4. Explorer avec Time Travel

```python
# Iceberg Time Travel - voir donnÃ©es Ã  un moment prÃ©cis
spark.sql("""
    SELECT * FROM bronze.raw_events
    VERSION AS OF 1234567890
""").show()

# Voir l'historique des snapshots
spark.sql("""
    SELECT * FROM bronze.raw_events.snapshots
""").show()
```

---

## ğŸ“Š MÃ©triques ClÃ©s

### DonnÃ©es de DÃ©monstration

| Layer | Tables | Lignes | Format |
|-------|--------|--------|--------|
| Bronze | 2 | 26 | Iceberg/Parquet |
| Silver | 2 | 26 | Iceberg/Parquet |
| Gold | 1 | 20 | Iceberg/Parquet |

### Performance

| OpÃ©ration | Temps Moyen |
|-----------|-------------|
| DÃ©marrage systÃ¨me | ~2 minutes |
| Transformation dbt | ~10-30 secondes |
| RequÃªte simple SQL | < 1 seconde |
| Time travel query | < 2 secondes |

---

## ğŸ”§ Configuration Minimale

### Fichier .env

```env
MINIO_ROOT_USER=admin
MINIO_ROOT_PASSWORD=password123
POSTGRES_DB=datamart
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres123
```

### PrÃ©requis SystÃ¨me

- **RAM** : 8 GB minimum (16 GB recommandÃ©)
- **CPU** : 4 cores minimum
- **Disque** : 10 GB espace libre
- **Docker** : Version 20.10+
- **Docker Compose** : Version 2.0+

---

## ğŸ¨ FonctionnalitÃ©s ClÃ©s

### âœ… ImplÃ©mentÃ©

- [x] Architecture MÃ©daillon (Bronze/Silver/Gold)
- [x] Tables Iceberg avec ACID
- [x] Transformations dbt
- [x] Jupyter Notebooks
- [x] Spark Thrift Server (JDBC/ODBC)
- [x] MinIO S3-compatible storage
- [x] TimescaleDB pour time-series
- [x] ChromaDB pour embeddings
- [x] DonnÃ©es de test prÃ©-chargÃ©es
- [x] Documentation complÃ¨te

### ğŸ”„ IntÃ©grations Possibles

- [ ] Airbyte pour ingestion (guide disponible)
- [ ] Airflow pour orchestration
- [ ] Superset pour visualisation
- [ ] Great Expectations pour data quality
- [ ] MLflow pour ML lifecycle

---

## ğŸ› DÃ©pannage Rapide

| ProblÃ¨me | Solution |
|----------|----------|
| Services ne dÃ©marrent pas | `docker-compose down && docker-compose up -d` |
| dbt ne se connecte pas | Attendre 2 min aprÃ¨s dÃ©marrage Spark |
| Jupyter inaccessible | `docker-compose restart spark-iceberg` |
| DonnÃ©es manquantes | VÃ©rifier logs: `docker-compose logs spark-iceberg` |

â†’ Plus de dÃ©tails : [QUICKSTART_FR.md - DÃ©pannage](./QUICKSTART_FR.md#rÃ©solution-des-problÃ¨mes-courants)

---

## ğŸ“ Apprentissage Rapide

### Parcours 30 Minutes

1. **5 min** : Lire ce document
2. **10 min** : DÃ©marrer le systÃ¨me ([QUICKSTART_FR.md](./QUICKSTART_FR.md))
3. **10 min** : Explorer Jupyter et MinIO
4. **5 min** : ExÃ©cuter premiÃ¨re transformation dbt

### Parcours 2 Heures

1. Suivre parcours 30 minutes
2. Lire [TRANSFORMATION_GUIDE_FR.md](./TRANSFORMATION_GUIDE_FR.md)
3. CrÃ©er modÃ¨le dbt personnalisÃ©
4. Explorer requÃªtes Spark SQL avancÃ©es

---

## ğŸ“ Support

### Documentation
- ğŸ“– [Index Documentation](./DOCUMENTATION_INDEX.md)
- ğŸ“˜ [Documentation ComplÃ¨te](./README_FR.md)
- âš¡ [Quick Start](./QUICKSTART_FR.md)

### Ressources Externes
- [Apache Iceberg](https://iceberg.apache.org/)
- [Apache Spark](https://spark.apache.org/)
- [dbt](https://docs.getdbt.com/)
- [MinIO](https://min.io/docs/)

---

## ğŸ† Quick Wins

**Premier jour** :
- âœ… SystÃ¨me opÃ©rationnel
- âœ… PremiÃ¨re transformation dbt
- âœ… PremiÃ¨re requÃªte SQL sur Gold layer

**PremiÃ¨re semaine** :
- âœ… ModÃ¨les dbt personnalisÃ©s
- âœ… IntÃ©gration Airbyte (optionnel)
- âœ… Dashboards de donnÃ©es

---

## ğŸ“… DerniÃ¨re Mise Ã  Jour

**Date** : 27 octobre 2025  
**Version** : 1.0.0  
**Statut** : Production Ready

Voir [CHANGELOG.md](./CHANGELOG.md) pour l'historique complet.

---

**ğŸš€ PrÃªt Ã  commencer ?**

```bash
# Clone ou navigue vers le projet
cd data-pipeline-poc

# CrÃ©e le fichier .env
cp .env.example .env

# DÃ©marre tout !
docker-compose up -d

# Explore !
open http://localhost:9001  # MinIO
open http://localhost:8888  # Jupyter
open http://localhost:4040  # Spark UI
```

**Bon voyage dans le monde des Data Lakehouses ! ğŸ‰**
