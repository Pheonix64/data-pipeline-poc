[0m12:37:24.187764 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7af073f58250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7af073f5aa90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7af073f5a350>]}


============================== 12:37:24.190839 | 89d1e40e-684c-4a21-833d-1191c5840000 ==============================
[0m12:37:24.190839 [info ] [MainThread]: Running with dbt=1.9.0
[0m12:37:24.192549 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/app/dbt', 'version_check': 'True', 'debug': 'False', 'log_path': '/usr/app/dbt/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt debug', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m12:37:24.208528 [info ] [MainThread]: dbt version: 1.9.0
[0m12:37:24.209433 [info ] [MainThread]: python version: 3.11.2
[0m12:37:24.212174 [info ] [MainThread]: python path: /usr/local/bin/python
[0m12:37:24.212973 [info ] [MainThread]: os info: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.31
[0m12:37:24.258015 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:37:24.259263 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:37:24.260216 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:37:24.282162 [info ] [MainThread]: Using profiles dir at /usr/app/dbt
[0m12:37:24.283526 [info ] [MainThread]: Using profiles.yml file at /usr/app/dbt/profiles.yml
[0m12:37:24.284492 [info ] [MainThread]: Using dbt_project.yml file at /usr/app/dbt/dbt_project.yml
[0m12:37:24.285253 [info ] [MainThread]: adapter type: spark
[0m12:37:24.286227 [info ] [MainThread]: adapter version: 1.9.0
[0m12:37:24.356987 [info ] [MainThread]: Configuration:
[0m12:37:24.358466 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m12:37:24.359522 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m12:37:24.360352 [info ] [MainThread]: Required dependencies:
[0m12:37:24.361230 [debug] [MainThread]: Executing "git --help"
[0m12:37:24.372311 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone             Clone a repository into a new directory\n   init              Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add               Add file contents to the index\n   mv                Move or rename a file, a directory, or a symlink\n   restore           Restore working tree files\n   rm                Remove files from the working tree and from the index\n   sparse-checkout   Initialize and modify the sparse-checkout\n\nexamine the history and state (see also: git help revisions)\n   bisect            Use binary search to find the commit that introduced a bug\n   diff              Show changes between commits, commit and working tree, etc\n   grep              Print lines matching a pattern\n   log               Show commit logs\n   show              Show various types of objects\n   status            Show the working tree status\n\ngrow, mark and tweak your common history\n   branch            List, create, or delete branches\n   commit            Record changes to the repository\n   merge             Join two or more development histories together\n   rebase            Reapply commits on top of another base tip\n   reset             Reset current HEAD to the specified state\n   switch            Switch branches\n   tag               Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch             Download objects and refs from another repository\n   pull              Fetch from and integrate with another repository or a local branch\n   push              Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m12:37:24.373367 [debug] [MainThread]: STDERR: "b''"
[0m12:37:24.374299 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m12:37:24.375407 [info ] [MainThread]: Connection:
[0m12:37:24.376321 [info ] [MainThread]:   host: spark-iceberg
[0m12:37:24.377009 [info ] [MainThread]:   port: 10000
[0m12:37:24.377785 [info ] [MainThread]:   cluster: None
[0m12:37:24.378509 [info ] [MainThread]:   endpoint: None
[0m12:37:24.379205 [info ] [MainThread]:   schema: default
[0m12:37:24.379915 [info ] [MainThread]:   organization: 0
[0m12:37:24.380805 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m12:37:24.419715 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m12:37:24.420485 [debug] [MainThread]: Using spark connection "debug"
[0m12:37:24.421126 [debug] [MainThread]: On debug: select 1 as id
[0m12:37:24.421686 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:37:26.097646 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m12:37:26.098928 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m12:37:26.099977 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m12:37:26.101149 [info ] [MainThread]: [31m1 check failed:[0m
[0m12:37:26.102512 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m12:37:26.105211 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 1.9690187, "process_in_blocks": "1456", "process_kernel_time": 0.140245, "process_mem_max_rss": "98960", "process_out_blocks": "24", "process_user_time": 0.96569}
[0m12:37:26.106703 [debug] [MainThread]: Command `dbt debug` failed at 12:37:26.106475 after 1.97 seconds
[0m12:37:26.107994 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m12:37:26.108962 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7af073f5a490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7af0778d92d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7af07783a5d0>]}
[0m12:37:26.109792 [debug] [MainThread]: Flushing usage events
[0m12:37:30.257991 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:37:31.465790 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ca993ee8c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ca993eeb450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ca993eebcd0>]}


============================== 12:37:31.468449 | 2ecad8cb-bebd-4c21-86ca-bff8d596d20c ==============================
[0m12:37:31.468449 [info ] [MainThread]: Running with dbt=1.9.0
[0m12:37:31.469435 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/usr/app/dbt/logs', 'profiles_dir': '/usr/app/dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt deps', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m12:37:31.548881 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2ecad8cb-bebd-4c21-86ca-bff8d596d20c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ca993d89250>]}
[0m12:37:31.563870 [debug] [MainThread]: Set downloads directory='/tmp/dbt-downloads-_8szoh32'
[0m12:37:31.564762 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m12:37:41.508428 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m12:37:41.510184 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m12:37:41.799668 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m12:37:41.805782 [error] [MainThread]: Encountered an error:
Package dbt-labs/dbt_expectations was not found in the package index
[0m12:37:41.808640 [error] [MainThread]: Traceback (most recent call last):
  File "/usr/local/lib/python3.11/site-packages/dbt/cli/requires.py", line 153, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/dbt/cli/requires.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/dbt/cli/requires.py", line 218, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/dbt/cli/requires.py", line 264, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/dbt/cli/main.py", line 455, in deps
    results = task.run()
              ^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/dbt/task/deps.py", line 211, in run
    self.lock()
  File "/usr/local/lib/python3.11/site-packages/dbt/task/deps.py", line 187, in lock
    resolved_deps = resolve_packages(packages, self.project, self.cli_vars)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/dbt/deps/resolver.py", line 131, in resolve_packages
    target = final[package].resolved().fetch_metadata(project, renderer)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/dbt/deps/registry.py", line 98, in resolved
    self._check_in_index()
  File "/usr/local/lib/python3.11/site-packages/dbt/deps/registry.py", line 77, in _check_in_index
    raise PackageNotFoundError(self.package)
dbt.exceptions.PackageNotFoundError: Package dbt-labs/dbt_expectations was not found in the package index

[0m12:37:41.809998 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": false, "command_wall_clock_time": 10.388398, "process_in_blocks": "712", "process_kernel_time": 0.122414, "process_mem_max_rss": "90104", "process_out_blocks": "8", "process_user_time": 0.817412}
[0m12:37:41.810802 [debug] [MainThread]: Command `dbt deps` failed at 12:37:41.810720 after 10.39 seconds
[0m12:37:41.812013 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ca993eeaa90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ca993e1e9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ca997869650>]}
[0m12:37:41.812688 [debug] [MainThread]: Flushing usage events
[0m12:37:42.419290 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:41:26.167881 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76522443e250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x765224493cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76522443fd10>]}


============================== 12:41:26.170686 | 6a31db74-14dc-4d82-af47-2fca6f6ec31a ==============================
[0m12:41:26.170686 [info ] [MainThread]: Running with dbt=1.9.0
[0m12:41:26.172466 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/app/dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/usr/app/dbt/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt debug', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m12:41:26.179810 [info ] [MainThread]: dbt version: 1.9.0
[0m12:41:26.180787 [info ] [MainThread]: python version: 3.11.2
[0m12:41:26.181668 [info ] [MainThread]: python path: /usr/local/bin/python
[0m12:41:26.182599 [info ] [MainThread]: os info: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.31
[0m12:41:26.229305 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:41:26.230772 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:41:26.231771 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:41:26.255140 [info ] [MainThread]: Using profiles dir at /usr/app/dbt
[0m12:41:26.256577 [info ] [MainThread]: Using profiles.yml file at /usr/app/dbt/profiles.yml
[0m12:41:26.257806 [info ] [MainThread]: Using dbt_project.yml file at /usr/app/dbt/dbt_project.yml
[0m12:41:26.258585 [info ] [MainThread]: adapter type: spark
[0m12:41:26.259293 [info ] [MainThread]: adapter version: 1.9.0
[0m12:41:26.326283 [info ] [MainThread]: Configuration:
[0m12:41:26.327568 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m12:41:26.328609 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m12:41:26.329423 [info ] [MainThread]: Required dependencies:
[0m12:41:26.330176 [debug] [MainThread]: Executing "git --help"
[0m12:41:26.332428 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone             Clone a repository into a new directory\n   init              Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add               Add file contents to the index\n   mv                Move or rename a file, a directory, or a symlink\n   restore           Restore working tree files\n   rm                Remove files from the working tree and from the index\n   sparse-checkout   Initialize and modify the sparse-checkout\n\nexamine the history and state (see also: git help revisions)\n   bisect            Use binary search to find the commit that introduced a bug\n   diff              Show changes between commits, commit and working tree, etc\n   grep              Print lines matching a pattern\n   log               Show commit logs\n   show              Show various types of objects\n   status            Show the working tree status\n\ngrow, mark and tweak your common history\n   branch            List, create, or delete branches\n   commit            Record changes to the repository\n   merge             Join two or more development histories together\n   rebase            Reapply commits on top of another base tip\n   reset             Reset current HEAD to the specified state\n   switch            Switch branches\n   tag               Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch             Download objects and refs from another repository\n   pull              Fetch from and integrate with another repository or a local branch\n   push              Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m12:41:26.333491 [debug] [MainThread]: STDERR: "b''"
[0m12:41:26.334166 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m12:41:26.335157 [info ] [MainThread]: Connection:
[0m12:41:26.337163 [info ] [MainThread]:   host: spark-iceberg
[0m12:41:26.338155 [info ] [MainThread]:   port: 10000
[0m12:41:26.339324 [info ] [MainThread]:   cluster: None
[0m12:41:26.340241 [info ] [MainThread]:   endpoint: None
[0m12:41:26.341209 [info ] [MainThread]:   schema: default
[0m12:41:26.342092 [info ] [MainThread]:   organization: 0
[0m12:41:26.343070 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m12:41:26.376613 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m12:41:26.377460 [debug] [MainThread]: Using spark connection "debug"
[0m12:41:26.378196 [debug] [MainThread]: On debug: select 1 as id
[0m12:41:26.378816 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:41:27.543262 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m12:41:27.544799 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m12:41:27.546269 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m12:41:27.547408 [info ] [MainThread]: [31m1 check failed:[0m
[0m12:41:27.548648 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m12:41:27.550466 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 1.429126, "process_in_blocks": "0", "process_kernel_time": 0.12654, "process_mem_max_rss": "98884", "process_out_blocks": "0", "process_user_time": 0.897645}
[0m12:41:27.551761 [debug] [MainThread]: Command `dbt debug` failed at 12:41:27.551631 after 1.43 seconds
[0m12:41:27.552955 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m12:41:27.554386 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x765227dc1410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x765223e7a3d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x765227dc1510>]}
[0m12:41:27.555539 [debug] [MainThread]: Flushing usage events
[0m12:41:30.800270 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:48:22.920566 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x713cbf749c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x713cbf7494d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x713cbf74bf90>]}


============================== 12:48:22.923513 | 9b6bbe8d-a6aa-4544-98dc-c759148f9c71 ==============================
[0m12:48:22.923513 [info ] [MainThread]: Running with dbt=1.9.0
[0m12:48:22.925173 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/usr/app/dbt/logs', 'profiles_dir': '/usr/app/dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt debug', 'send_anonymous_usage_stats': 'True'}
[0m12:48:22.931603 [info ] [MainThread]: dbt version: 1.9.0
[0m12:48:22.932462 [info ] [MainThread]: python version: 3.11.2
[0m12:48:22.933769 [info ] [MainThread]: python path: /usr/local/bin/python
[0m12:48:22.935165 [info ] [MainThread]: os info: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.31
[0m12:48:22.986766 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:48:22.988081 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:48:22.989090 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:48:23.012580 [info ] [MainThread]: Using profiles dir at /usr/app/dbt
[0m12:48:23.014292 [info ] [MainThread]: Using profiles.yml file at /usr/app/dbt/profiles.yml
[0m12:48:23.015418 [info ] [MainThread]: Using dbt_project.yml file at /usr/app/dbt/dbt_project.yml
[0m12:48:23.016478 [info ] [MainThread]: adapter type: spark
[0m12:48:23.017217 [info ] [MainThread]: adapter version: 1.9.0
[0m12:48:23.093335 [info ] [MainThread]: Configuration:
[0m12:48:23.094589 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m12:48:23.095819 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m12:48:23.096618 [info ] [MainThread]: Required dependencies:
[0m12:48:23.097490 [debug] [MainThread]: Executing "git --help"
[0m12:48:23.099659 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone             Clone a repository into a new directory\n   init              Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add               Add file contents to the index\n   mv                Move or rename a file, a directory, or a symlink\n   restore           Restore working tree files\n   rm                Remove files from the working tree and from the index\n   sparse-checkout   Initialize and modify the sparse-checkout\n\nexamine the history and state (see also: git help revisions)\n   bisect            Use binary search to find the commit that introduced a bug\n   diff              Show changes between commits, commit and working tree, etc\n   grep              Print lines matching a pattern\n   log               Show commit logs\n   show              Show various types of objects\n   status            Show the working tree status\n\ngrow, mark and tweak your common history\n   branch            List, create, or delete branches\n   commit            Record changes to the repository\n   merge             Join two or more development histories together\n   rebase            Reapply commits on top of another base tip\n   reset             Reset current HEAD to the specified state\n   switch            Switch branches\n   tag               Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch             Download objects and refs from another repository\n   pull              Fetch from and integrate with another repository or a local branch\n   push              Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m12:48:23.100421 [debug] [MainThread]: STDERR: "b''"
[0m12:48:23.101128 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m12:48:23.101918 [info ] [MainThread]: Connection:
[0m12:48:23.103337 [info ] [MainThread]:   host: spark-iceberg
[0m12:48:23.104376 [info ] [MainThread]:   port: 10000
[0m12:48:23.105268 [info ] [MainThread]:   cluster: None
[0m12:48:23.106110 [info ] [MainThread]:   endpoint: None
[0m12:48:23.107011 [info ] [MainThread]:   schema: default
[0m12:48:23.107904 [info ] [MainThread]:   organization: 0
[0m12:48:23.108926 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m12:48:23.146347 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m12:48:23.147377 [debug] [MainThread]: Using spark connection "debug"
[0m12:48:23.148072 [debug] [MainThread]: On debug: select 1 as id
[0m12:48:23.148700 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:48:23.185249 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m12:48:23.186456 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m12:48:23.187978 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m12:48:23.189132 [info ] [MainThread]: [31m1 check failed:[0m
[0m12:48:23.190096 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m12:48:23.191569 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 0.31958392, "process_in_blocks": "0", "process_kernel_time": 0.119626, "process_mem_max_rss": "99312", "process_out_blocks": "0", "process_user_time": 1.012838}
[0m12:48:23.192503 [debug] [MainThread]: Command `dbt debug` failed at 12:48:23.192397 after 0.32 seconds
[0m12:48:23.193209 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m12:48:23.193890 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x713cbf7a3bd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x713cbf2ace10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x713cbf79fa50>]}
[0m12:48:23.194582 [debug] [MainThread]: Flushing usage events
[0m12:48:25.316890 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:49:40.300261 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x766320745590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76632079b850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76632079bc90>]}


============================== 12:49:40.303013 | a4e81be3-9227-4441-abc2-7487c2f77073 ==============================
[0m12:49:40.303013 [info ] [MainThread]: Running with dbt=1.9.0
[0m12:49:40.304232 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/app/dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': '/usr/app/dbt/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt debug', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m12:49:40.311457 [info ] [MainThread]: dbt version: 1.9.0
[0m12:49:40.312796 [info ] [MainThread]: python version: 3.11.2
[0m12:49:40.313604 [info ] [MainThread]: python path: /usr/local/bin/python
[0m12:49:40.314414 [info ] [MainThread]: os info: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.31
[0m12:49:40.359915 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:49:40.361473 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:49:40.362157 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:49:40.382587 [info ] [MainThread]: Using profiles dir at /usr/app/dbt
[0m12:49:40.384191 [info ] [MainThread]: Using profiles.yml file at /usr/app/dbt/profiles.yml
[0m12:49:40.385071 [info ] [MainThread]: Using dbt_project.yml file at /usr/app/dbt/dbt_project.yml
[0m12:49:40.385796 [info ] [MainThread]: adapter type: spark
[0m12:49:40.386932 [info ] [MainThread]: adapter version: 1.9.0
[0m12:49:40.455889 [info ] [MainThread]: Configuration:
[0m12:49:40.458037 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m12:49:40.459326 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m12:49:40.460457 [info ] [MainThread]: Required dependencies:
[0m12:49:40.461466 [debug] [MainThread]: Executing "git --help"
[0m12:49:40.463414 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone             Clone a repository into a new directory\n   init              Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add               Add file contents to the index\n   mv                Move or rename a file, a directory, or a symlink\n   restore           Restore working tree files\n   rm                Remove files from the working tree and from the index\n   sparse-checkout   Initialize and modify the sparse-checkout\n\nexamine the history and state (see also: git help revisions)\n   bisect            Use binary search to find the commit that introduced a bug\n   diff              Show changes between commits, commit and working tree, etc\n   grep              Print lines matching a pattern\n   log               Show commit logs\n   show              Show various types of objects\n   status            Show the working tree status\n\ngrow, mark and tweak your common history\n   branch            List, create, or delete branches\n   commit            Record changes to the repository\n   merge             Join two or more development histories together\n   rebase            Reapply commits on top of another base tip\n   reset             Reset current HEAD to the specified state\n   switch            Switch branches\n   tag               Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch             Download objects and refs from another repository\n   pull              Fetch from and integrate with another repository or a local branch\n   push              Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m12:49:40.464703 [debug] [MainThread]: STDERR: "b''"
[0m12:49:40.465464 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m12:49:40.466480 [info ] [MainThread]: Connection:
[0m12:49:40.467280 [info ] [MainThread]:   host: spark-iceberg
[0m12:49:40.468196 [info ] [MainThread]:   port: 10000
[0m12:49:40.468976 [info ] [MainThread]:   cluster: None
[0m12:49:40.469859 [info ] [MainThread]:   endpoint: None
[0m12:49:40.470693 [info ] [MainThread]:   schema: default
[0m12:49:40.471749 [info ] [MainThread]:   organization: 0
[0m12:49:40.472706 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m12:49:40.506064 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m12:49:40.507982 [debug] [MainThread]: Using spark connection "debug"
[0m12:49:40.508957 [debug] [MainThread]: On debug: select 1 as id
[0m12:49:40.509993 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:49:40.541645 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m12:49:40.542924 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m12:49:40.544112 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m12:49:40.544997 [info ] [MainThread]: [31m1 check failed:[0m
[0m12:49:40.546091 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m12:49:40.547423 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 0.29426196, "process_in_blocks": "0", "process_kernel_time": 0.131699, "process_mem_max_rss": "99108", "process_out_blocks": "0", "process_user_time": 0.889972}
[0m12:49:40.548241 [debug] [MainThread]: Command `dbt debug` failed at 12:49:40.548153 after 0.30 seconds
[0m12:49:40.548853 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m12:49:40.549483 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76632079bdd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x766320745b90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7663200b0090>]}
[0m12:49:40.550257 [debug] [MainThread]: Flushing usage events
[0m12:49:43.541790 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:36:52.598971 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x779ea409f810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x779ea409f2d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x779ea409f210>]}


============================== 13:36:52.603767 | aa884cce-597f-45e9-8b0e-13f9ba02114f ==============================
[0m13:36:52.603767 [info ] [MainThread]: Running with dbt=1.9.0
[0m13:36:52.605342 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/usr/app/dbt/logs', 'profiles_dir': '/usr/app/dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt run', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m13:36:52.655457 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:36:52.657014 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:36:52.658133 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:36:52.755720 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'aa884cce-597f-45e9-8b0e-13f9ba02114f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x779ea4102cd0>]}
[0m13:36:52.794607 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'aa884cce-597f-45e9-8b0e-13f9ba02114f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x779ea45e6f10>]}
[0m13:36:52.795936 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m13:36:52.836002 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found 2 package(s) specified in packages.yml, but only 0 package(s) installed in dbt_packages. Run "dbt deps" to install package dependencies.
[0m13:36:52.837997 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.29578948, "process_in_blocks": "1224", "process_kernel_time": 0.144069, "process_mem_max_rss": "98876", "process_out_blocks": "0", "process_user_time": 0.852411}
[0m13:36:52.839077 [debug] [MainThread]: Command `dbt run` failed at 13:36:52.838966 after 0.30 seconds
[0m13:36:52.839899 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x779ea41008d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x779ea3a7ec10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x779ea79f9510>]}
[0m13:36:52.840727 [debug] [MainThread]: Flushing usage events
[0m13:37:13.105905 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:37:29.941807 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7457aee23b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7457af221ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7457af2211d0>]}


============================== 13:37:29.944150 | e39c6694-a175-488f-8bce-d25ae07e7f23 ==============================
[0m13:37:29.944150 [info ] [MainThread]: Running with dbt=1.9.0
[0m13:37:29.945404 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/app/dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/usr/app/dbt/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m13:37:29.981901 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:37:29.983478 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:37:29.984761 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:37:30.069959 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e39c6694-a175-488f-8bce-d25ae07e7f23', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7457ae975d90>]}
[0m13:37:30.104572 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e39c6694-a175-488f-8bce-d25ae07e7f23', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7457afbc8950>]}
[0m13:37:30.106264 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m13:37:30.139340 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found 2 package(s) specified in packages.yml, but only 0 package(s) installed in dbt_packages. Run "dbt deps" to install package dependencies.
[0m13:37:30.141940 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.23982829, "process_in_blocks": "0", "process_kernel_time": 0.122594, "process_mem_max_rss": "98324", "process_out_blocks": "0", "process_user_time": 0.830482}
[0m13:37:30.143615 [debug] [MainThread]: Command `dbt run` failed at 13:37:30.143476 after 0.24 seconds
[0m13:37:30.144880 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7457b277d3d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7457af2211d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7457ae5923d0>]}
[0m13:37:30.146410 [debug] [MainThread]: Flushing usage events
[0m13:38:07.845561 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d8805da850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d8805d8b10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d8805d9c50>]}


============================== 13:38:07.847986 | d5d3c157-4f3f-4530-9795-52d5b1cec30e ==============================
[0m13:38:07.847986 [info ] [MainThread]: Running with dbt=1.9.0
[0m13:38:07.849103 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/app/dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/usr/app/dbt/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt deps', 'send_anonymous_usage_stats': 'True'}
[0m13:38:07.920791 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd5d3c157-4f3f-4530-9795-52d5b1cec30e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d880456450>]}
[0m13:38:07.943533 [debug] [MainThread]: Set downloads directory='/tmp/dbt-downloads-5xmflmba'
[0m13:38:07.944758 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m13:38:08.362008 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m13:38:08.363824 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m13:38:08.644915 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m13:38:08.654998 [error] [MainThread]: Encountered an error:
Package dbt-labs/dbt_expectations was not found in the package index
[0m13:38:08.667648 [error] [MainThread]: Traceback (most recent call last):
  File "/usr/local/lib/python3.11/site-packages/dbt/cli/requires.py", line 153, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/dbt/cli/requires.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/dbt/cli/requires.py", line 218, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/dbt/cli/requires.py", line 264, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/dbt/cli/main.py", line 455, in deps
    results = task.run()
              ^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/dbt/task/deps.py", line 211, in run
    self.lock()
  File "/usr/local/lib/python3.11/site-packages/dbt/task/deps.py", line 187, in lock
    resolved_deps = resolve_packages(packages, self.project, self.cli_vars)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/dbt/deps/resolver.py", line 131, in resolve_packages
    target = final[package].resolved().fetch_metadata(project, renderer)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/dbt/deps/registry.py", line 98, in resolved
    self._check_in_index()
  File "/usr/local/lib/python3.11/site-packages/dbt/deps/registry.py", line 77, in _check_in_index
    raise PackageNotFoundError(self.package)
dbt.exceptions.PackageNotFoundError: Package dbt-labs/dbt_expectations was not found in the package index

[0m13:38:08.669948 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": false, "command_wall_clock_time": 0.8642403, "process_in_blocks": "1224", "process_kernel_time": 0.092193, "process_mem_max_rss": "90788", "process_out_blocks": "8", "process_user_time": 0.789654}
[0m13:38:08.671321 [debug] [MainThread]: Command `dbt deps` failed at 13:38:08.671196 after 0.87 seconds
[0m13:38:08.672301 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d880630e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d883f2d510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d883f2d410>]}
[0m13:38:08.674041 [debug] [MainThread]: Flushing usage events
[0m13:38:18.874082 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:39:05.611874 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eccd7a360d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eccd7a8be90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eccd7a35e90>]}


============================== 13:39:05.614721 | c8df0aeb-1b32-4be6-9ae7-40b3177505d5 ==============================
[0m13:39:05.614721 [info ] [MainThread]: Running with dbt=1.9.0
[0m13:39:05.616004 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/app/dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/usr/app/dbt/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt deps', 'send_anonymous_usage_stats': 'True'}
[0m13:39:05.685037 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c8df0aeb-1b32-4be6-9ae7-40b3177505d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eccd7a36190>]}
[0m13:39:05.696507 [debug] [MainThread]: Set downloads directory='/tmp/dbt-downloads-e36vhjl9'
[0m13:39:05.698102 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m13:39:05.975013 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m13:39:05.977691 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m13:39:06.938842 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m13:39:06.952116 [info ] [MainThread]: Updating lock file in file path: /usr/app/dbt/package-lock.yml
[0m13:39:06.958223 [debug] [MainThread]: Set downloads directory='/tmp/dbt-downloads-ht6yaqfk'
[0m13:39:06.962146 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m13:39:10.003838 [info ] [MainThread]: Installed from version 1.1.1
[0m13:39:10.005176 [info ] [MainThread]: Updated version available: 1.3.1
[0m13:39:10.006375 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': 'c8df0aeb-1b32-4be6-9ae7-40b3177505d5', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eccd78f3a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eccd89b42d0>]}
[0m13:39:10.007107 [info ] [MainThread]: 
[0m13:39:10.008126 [info ] [MainThread]: Updates available for packages: ['dbt-labs/dbt_utils']                 
Update your versions in packages.yml, then run dbt deps
[0m13:39:10.010039 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 4.4435964, "process_in_blocks": "168", "process_kernel_time": 0.279291, "process_mem_max_rss": "90496", "process_out_blocks": "200", "process_user_time": 0.907697}
[0m13:39:10.011283 [debug] [MainThread]: Command `dbt deps` succeeded at 13:39:10.011065 after 4.44 seconds
[0m13:39:10.012040 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eccd7a661d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eccdb352710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eccdb294d50>]}
[0m13:39:10.012807 [debug] [MainThread]: Flushing usage events
[0m13:39:20.182171 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:40:19.693114 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x713cc365f8d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x713cc36b2e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x713cc365d0d0>]}


============================== 13:40:19.696651 | 6b5d511c-4937-4315-be82-e8cb9aee4666 ==============================
[0m13:40:19.696651 [info ] [MainThread]: Running with dbt=1.9.0
[0m13:40:19.698099 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/app/dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/usr/app/dbt/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt debug', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m13:40:19.708959 [info ] [MainThread]: dbt version: 1.9.0
[0m13:40:19.709903 [info ] [MainThread]: python version: 3.11.2
[0m13:40:19.711007 [info ] [MainThread]: python path: /usr/local/bin/python
[0m13:40:19.711890 [info ] [MainThread]: os info: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.31
[0m13:40:19.758962 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:40:19.760030 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:40:19.760998 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:40:19.782608 [info ] [MainThread]: Using profiles dir at /usr/app/dbt
[0m13:40:19.784004 [info ] [MainThread]: Using profiles.yml file at /usr/app/dbt/profiles.yml
[0m13:40:19.785485 [info ] [MainThread]: Using dbt_project.yml file at /usr/app/dbt/dbt_project.yml
[0m13:40:19.786483 [info ] [MainThread]: adapter type: spark
[0m13:40:19.787330 [info ] [MainThread]: adapter version: 1.9.0
[0m13:40:19.856254 [info ] [MainThread]: Configuration:
[0m13:40:19.857655 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m13:40:19.859020 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m13:40:19.860084 [info ] [MainThread]: Required dependencies:
[0m13:40:19.861106 [debug] [MainThread]: Executing "git --help"
[0m13:40:19.872194 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone             Clone a repository into a new directory\n   init              Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add               Add file contents to the index\n   mv                Move or rename a file, a directory, or a symlink\n   restore           Restore working tree files\n   rm                Remove files from the working tree and from the index\n   sparse-checkout   Initialize and modify the sparse-checkout\n\nexamine the history and state (see also: git help revisions)\n   bisect            Use binary search to find the commit that introduced a bug\n   diff              Show changes between commits, commit and working tree, etc\n   grep              Print lines matching a pattern\n   log               Show commit logs\n   show              Show various types of objects\n   status            Show the working tree status\n\ngrow, mark and tweak your common history\n   branch            List, create, or delete branches\n   commit            Record changes to the repository\n   merge             Join two or more development histories together\n   rebase            Reapply commits on top of another base tip\n   reset             Reset current HEAD to the specified state\n   switch            Switch branches\n   tag               Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch             Download objects and refs from another repository\n   pull              Fetch from and integrate with another repository or a local branch\n   push              Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m13:40:19.873160 [debug] [MainThread]: STDERR: "b''"
[0m13:40:19.873842 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m13:40:19.874671 [info ] [MainThread]: Connection:
[0m13:40:19.875792 [info ] [MainThread]:   host: spark-iceberg
[0m13:40:19.876595 [info ] [MainThread]:   port: 10000
[0m13:40:19.877283 [info ] [MainThread]:   cluster: None
[0m13:40:19.877950 [info ] [MainThread]:   endpoint: None
[0m13:40:19.878703 [info ] [MainThread]:   schema: default
[0m13:40:19.879621 [info ] [MainThread]:   organization: 0
[0m13:40:19.880587 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m13:40:19.916781 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m13:40:19.917814 [debug] [MainThread]: Using spark connection "debug"
[0m13:40:19.918536 [debug] [MainThread]: On debug: select 1 as id
[0m13:40:19.919178 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:40:21.640653 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m13:40:21.641840 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m13:40:21.642703 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m13:40:21.643941 [info ] [MainThread]: [31m1 check failed:[0m
[0m13:40:21.645223 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m13:40:21.646950 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 1.9987814, "process_in_blocks": "112", "process_kernel_time": 0.144258, "process_mem_max_rss": "99224", "process_out_blocks": "0", "process_user_time": 0.849521}
[0m13:40:21.648186 [debug] [MainThread]: Command `dbt debug` failed at 13:40:21.648061 after 2.00 seconds
[0m13:40:21.649108 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m13:40:21.650371 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x713cc365ec90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x713cc36b5350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x713cc2e06250>]}
[0m13:40:21.651898 [debug] [MainThread]: Flushing usage events
[0m13:40:31.951976 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:44:20.272103 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x707fecd97850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x707fecd97c90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x707fecd97d90>]}


============================== 13:44:20.274747 | db806628-37a8-4602-9509-44ae62389539 ==============================
[0m13:44:20.274747 [info ] [MainThread]: Running with dbt=1.9.0
[0m13:44:20.277116 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/usr/app/dbt/logs', 'profiles_dir': '/usr/app/dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'invocation_command': 'dbt debug', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m13:44:20.292222 [info ] [MainThread]: dbt version: 1.9.0
[0m13:44:20.294264 [info ] [MainThread]: python version: 3.11.2
[0m13:44:20.295386 [info ] [MainThread]: python path: /usr/local/bin/python
[0m13:44:20.296541 [info ] [MainThread]: os info: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.31
[0m13:44:20.342082 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:44:20.343860 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:44:20.344619 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:44:20.366082 [info ] [MainThread]: Using profiles dir at /usr/app/dbt
[0m13:44:20.367685 [info ] [MainThread]: Using profiles.yml file at /usr/app/dbt/profiles.yml
[0m13:44:20.368602 [info ] [MainThread]: Using dbt_project.yml file at /usr/app/dbt/dbt_project.yml
[0m13:44:20.369513 [info ] [MainThread]: adapter type: spark
[0m13:44:20.370589 [info ] [MainThread]: adapter version: 1.9.0
[0m13:44:20.437297 [info ] [MainThread]: Configuration:
[0m13:44:20.438614 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m13:44:20.439549 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m13:44:20.440492 [info ] [MainThread]: Required dependencies:
[0m13:44:20.441338 [debug] [MainThread]: Executing "git --help"
[0m13:44:20.451698 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone             Clone a repository into a new directory\n   init              Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add               Add file contents to the index\n   mv                Move or rename a file, a directory, or a symlink\n   restore           Restore working tree files\n   rm                Remove files from the working tree and from the index\n   sparse-checkout   Initialize and modify the sparse-checkout\n\nexamine the history and state (see also: git help revisions)\n   bisect            Use binary search to find the commit that introduced a bug\n   diff              Show changes between commits, commit and working tree, etc\n   grep              Print lines matching a pattern\n   log               Show commit logs\n   show              Show various types of objects\n   status            Show the working tree status\n\ngrow, mark and tweak your common history\n   branch            List, create, or delete branches\n   commit            Record changes to the repository\n   merge             Join two or more development histories together\n   rebase            Reapply commits on top of another base tip\n   reset             Reset current HEAD to the specified state\n   switch            Switch branches\n   tag               Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch             Download objects and refs from another repository\n   pull              Fetch from and integrate with another repository or a local branch\n   push              Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m13:44:20.452783 [debug] [MainThread]: STDERR: "b''"
[0m13:44:20.453502 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m13:44:20.454614 [info ] [MainThread]: Connection:
[0m13:44:20.455490 [info ] [MainThread]:   host: spark-iceberg
[0m13:44:20.456588 [info ] [MainThread]:   port: 10000
[0m13:44:20.457425 [info ] [MainThread]:   cluster: None
[0m13:44:20.458168 [info ] [MainThread]:   endpoint: None
[0m13:44:20.459203 [info ] [MainThread]:   schema: default
[0m13:44:20.460534 [info ] [MainThread]:   organization: 0
[0m13:44:20.461801 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m13:44:20.500139 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m13:44:20.501010 [debug] [MainThread]: Using spark connection "debug"
[0m13:44:20.501747 [debug] [MainThread]: On debug: select 1 as id
[0m13:44:20.502427 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:44:20.535160 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m13:44:20.536466 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m13:44:20.537280 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m13:44:20.538270 [info ] [MainThread]: [31m1 check failed:[0m
[0m13:44:20.538901 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m13:44:20.540893 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 0.31613076, "process_in_blocks": "1440", "process_kernel_time": 0.145942, "process_mem_max_rss": "99268", "process_out_blocks": "0", "process_user_time": 0.8716}
[0m13:44:20.541731 [debug] [MainThread]: Command `dbt debug` failed at 13:44:20.541644 after 0.32 seconds
[0m13:44:20.542623 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m13:44:20.543367 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x707ff06954d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x707ff0801750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x707fec716c10>]}
[0m13:44:20.544296 [debug] [MainThread]: Flushing usage events
[0m13:44:40.805307 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:10:49.821446 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bacd75ab350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bacd75ab450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bacd7959950>]}


============================== 15:10:49.824986 | c37a7045-fb1c-490f-8f8e-e3fac8fefc8c ==============================
[0m15:10:49.824986 [info ] [MainThread]: Running with dbt=1.9.0
[0m15:10:49.826089 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/usr/app/dbt/logs', 'profiles_dir': '/usr/app/dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt debug', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:10:49.843237 [info ] [MainThread]: dbt version: 1.9.0
[0m15:10:49.844417 [info ] [MainThread]: python version: 3.11.2
[0m15:10:49.845464 [info ] [MainThread]: python path: /usr/local/bin/python
[0m15:10:49.846377 [info ] [MainThread]: os info: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.31
[0m15:10:49.895912 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m15:10:49.897011 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m15:10:49.898384 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m15:10:49.923525 [info ] [MainThread]: Using profiles dir at /usr/app/dbt
[0m15:10:49.924620 [info ] [MainThread]: Using profiles.yml file at /usr/app/dbt/profiles.yml
[0m15:10:49.925479 [info ] [MainThread]: Using dbt_project.yml file at /usr/app/dbt/dbt_project.yml
[0m15:10:49.926327 [info ] [MainThread]: adapter type: spark
[0m15:10:49.927107 [info ] [MainThread]: adapter version: 1.9.0
[0m15:10:49.995449 [info ] [MainThread]: Configuration:
[0m15:10:49.996868 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m15:10:49.997784 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m15:10:49.998740 [info ] [MainThread]: Required dependencies:
[0m15:10:50.000145 [debug] [MainThread]: Executing "git --help"
[0m15:10:50.013017 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone             Clone a repository into a new directory\n   init              Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add               Add file contents to the index\n   mv                Move or rename a file, a directory, or a symlink\n   restore           Restore working tree files\n   rm                Remove files from the working tree and from the index\n   sparse-checkout   Initialize and modify the sparse-checkout\n\nexamine the history and state (see also: git help revisions)\n   bisect            Use binary search to find the commit that introduced a bug\n   diff              Show changes between commits, commit and working tree, etc\n   grep              Print lines matching a pattern\n   log               Show commit logs\n   show              Show various types of objects\n   status            Show the working tree status\n\ngrow, mark and tweak your common history\n   branch            List, create, or delete branches\n   commit            Record changes to the repository\n   merge             Join two or more development histories together\n   rebase            Reapply commits on top of another base tip\n   reset             Reset current HEAD to the specified state\n   switch            Switch branches\n   tag               Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch             Download objects and refs from another repository\n   pull              Fetch from and integrate with another repository or a local branch\n   push              Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m15:10:50.013773 [debug] [MainThread]: STDERR: "b''"
[0m15:10:50.014380 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m15:10:50.016135 [info ] [MainThread]: Connection:
[0m15:10:50.017542 [info ] [MainThread]:   host: spark-iceberg
[0m15:10:50.018644 [info ] [MainThread]:   port: 10000
[0m15:10:50.019724 [info ] [MainThread]:   cluster: None
[0m15:10:50.020544 [info ] [MainThread]:   endpoint: None
[0m15:10:50.021538 [info ] [MainThread]:   schema: default
[0m15:10:50.022406 [info ] [MainThread]:   organization: 0
[0m15:10:50.023499 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m15:10:50.074566 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m15:10:50.075779 [debug] [MainThread]: Using spark connection "debug"
[0m15:10:50.076824 [debug] [MainThread]: On debug: select 1 as id
[0m15:10:50.077746 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:10:52.175515 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m15:10:52.176567 [debug] [MainThread]: SQL status: OK in 2.099 seconds
[0m15:10:52.177824 [debug] [MainThread]: On debug: Close
[0m15:10:52.203220 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m15:10:52.204485 [info ] [MainThread]: [32mAll checks passed![0m
[0m15:10:52.207013 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 2.4388652, "process_in_blocks": "1456", "process_kernel_time": 0.171828, "process_mem_max_rss": "99196", "process_out_blocks": "24", "process_user_time": 0.931068}
[0m15:10:52.208076 [debug] [MainThread]: Command `dbt debug` succeeded at 15:10:52.207967 after 2.44 seconds
[0m15:10:52.208892 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m15:10:52.209797 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bacd6fa0190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bacd794a4d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bacdae3e6d0>]}
[0m15:10:52.210608 [debug] [MainThread]: Flushing usage events
[0m15:10:53.079168 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:11:07.561977 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x769a384f12d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x769a384eaf50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x769a3854b650>]}


============================== 15:11:07.565185 | b9cba657-fdec-450a-98ad-e9f93a6794ae ==============================
[0m15:11:07.565185 [info ] [MainThread]: Running with dbt=1.9.0
[0m15:11:07.566136 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/app/dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/usr/app/dbt/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m15:11:07.612863 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m15:11:07.614108 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m15:11:07.614935 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m15:11:07.727346 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b9cba657-fdec-450a-98ad-e9f93a6794ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x769a37dbe210>]}
[0m15:11:07.765761 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b9cba657-fdec-450a-98ad-e9f93a6794ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x769a3929f150>]}
[0m15:11:07.766916 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m15:11:07.825193 [debug] [MainThread]: checksum: c99e828bba267739642b5a3ce85f17518764ea526e0e6c4fdc649171c1a66bff, vars: {}, profile: , target: , version: 1.9.0
[0m15:11:07.830062 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m15:11:07.831000 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'b9cba657-fdec-450a-98ad-e9f93a6794ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x769a37c5ce90>]}
[0m15:11:09.580395 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- seeds.data_pipeline_poc
- snapshots.data_pipeline_poc
[0m15:11:09.588508 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b9cba657-fdec-450a-98ad-e9f93a6794ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x769a37c3f490>]}
[0m15:11:09.666061 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/app/dbt/target/manifest.json
[0m15:11:09.671354 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/app/dbt/target/semantic_manifest.json
[0m15:11:09.695594 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b9cba657-fdec-450a-98ad-e9f93a6794ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x769a346a2dd0>]}
[0m15:11:09.696614 [info ] [MainThread]: Found 3 models, 4 data tests, 2 sources, 583 macros
[0m15:11:09.697740 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b9cba657-fdec-450a-98ad-e9f93a6794ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x769a34765f50>]}
[0m15:11:09.699458 [info ] [MainThread]: 
[0m15:11:09.700420 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:11:09.701160 [info ] [MainThread]: 
[0m15:11:09.702136 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m15:11:09.707606 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m15:11:09.715356 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m15:11:09.716074 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m15:11:09.716738 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:11:10.421740 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:11:10.423062 [debug] [ThreadPool]: SQL status: OK in 0.706 seconds
[0m15:11:10.474751 [debug] [ThreadPool]: On list_schemas: Close
[0m15:11:10.480643 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m15:11:10.481635 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m15:11:10.482402 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:11:10.549794 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:11:10.550840 [debug] [ThreadPool]: SQL status: OK in 0.068 seconds
[0m15:11:10.554347 [debug] [ThreadPool]: On list_schemas: Close
[0m15:11:10.558551 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__default_default_gold)
[0m15:11:10.559584 [debug] [ThreadPool]: Creating schema "schema: "default_default_gold"
"
[0m15:11:10.562947 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:11:10.563659 [debug] [ThreadPool]: Using spark connection "create__default_default_gold"
[0m15:11:10.564321 [debug] [ThreadPool]: On create__default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "create__default_default_gold"} */
create schema if not exists default_default_gold
  
[0m15:11:10.564930 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:11:10.669958 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:11:10.671277 [debug] [ThreadPool]: SQL status: OK in 0.106 seconds
[0m15:11:10.672802 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m15:11:10.673781 [debug] [ThreadPool]: On create__default_default_gold: ROLLBACK
[0m15:11:10.674759 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m15:11:10.675666 [debug] [ThreadPool]: On create__default_default_gold: Close
[0m15:11:10.680344 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create__default_default_gold, now create__default_default_silver)
[0m15:11:10.681645 [debug] [ThreadPool]: Creating schema "schema: "default_default_silver"
"
[0m15:11:10.684029 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:11:10.685066 [debug] [ThreadPool]: Using spark connection "create__default_default_silver"
[0m15:11:10.685868 [debug] [ThreadPool]: On create__default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "create__default_default_silver"} */
create schema if not exists default_default_silver
  
[0m15:11:10.686744 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:11:10.777162 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:11:10.778844 [debug] [ThreadPool]: SQL status: OK in 0.092 seconds
[0m15:11:10.780595 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m15:11:10.781812 [debug] [ThreadPool]: On create__default_default_silver: ROLLBACK
[0m15:11:10.783190 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m15:11:10.784279 [debug] [ThreadPool]: On create__default_default_silver: Close
[0m15:11:10.794163 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create__default_default_silver, now list_None_default_default_gold)
[0m15:11:10.800090 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:11:10.801099 [debug] [ThreadPool]: Using spark connection "list_None_default_default_gold"
[0m15:11:10.801884 [debug] [ThreadPool]: On list_None_default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show table extended in default_default_gold like '*'
  
[0m15:11:10.802973 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:11:10.887770 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#28, tableName#29, isTemporary#30, information#31]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@64947ef7, [default_default_gold]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#28, tableName#29, isTemporary#30, information#31]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@64947ef7, [default_default_gold]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m15:11:10.889700 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m15:11:10.891024 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show table extended in default_default_gold like '*'
  
[0m15:11:10.893946 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#28, tableName#29, isTemporary#30, information#31]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@64947ef7, [default_default_gold]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#28, tableName#29, isTemporary#30, information#31]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@64947ef7, [default_default_gold]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m15:11:10.895971 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m15:11:10.898083 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#28, tableName#29, isTemporary#30, information#31]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@64947ef7, [default_default_gold]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#28, tableName#29, isTemporary#30, information#31]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@64947ef7, [default_default_gold]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m15:11:10.906821 [debug] [ThreadPool]: Using spark connection "list_None_default_default_gold"
[0m15:11:10.910095 [debug] [ThreadPool]: On list_None_default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show tables in default_default_gold like '*'
  
[0m15:11:10.965350 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:11:10.966817 [debug] [ThreadPool]: SQL status: OK in 0.055 seconds
[0m15:11:10.973201 [debug] [ThreadPool]: On list_None_default_default_gold: ROLLBACK
[0m15:11:10.974467 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m15:11:10.975485 [debug] [ThreadPool]: On list_None_default_default_gold: Close
[0m15:11:10.982773 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default_default_gold, now list_None_default_default_silver)
[0m15:11:10.985970 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:11:10.988380 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m15:11:10.991939 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show table extended in default_default_silver like '*'
  
[0m15:11:10.993366 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:11:11.040753 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#43, tableName#44, isTemporary#45, information#46]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@4011a307, [default_default_silver]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#43, tableName#44, isTemporary#45, information#46]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@4011a307, [default_default_silver]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m15:11:11.042319 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m15:11:11.043333 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show table extended in default_default_silver like '*'
  
[0m15:11:11.044502 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#43, tableName#44, isTemporary#45, information#46]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@4011a307, [default_default_silver]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#43, tableName#44, isTemporary#45, information#46]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@4011a307, [default_default_silver]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m15:11:11.045716 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m15:11:11.046916 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#43, tableName#44, isTemporary#45, information#46]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@4011a307, [default_default_silver]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#43, tableName#44, isTemporary#45, information#46]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@4011a307, [default_default_silver]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m15:11:11.050669 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m15:11:11.051991 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show tables in default_default_silver like '*'
  
[0m15:11:11.080587 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:11:11.081701 [debug] [ThreadPool]: SQL status: OK in 0.028 seconds
[0m15:11:11.084687 [debug] [ThreadPool]: On list_None_default_default_silver: ROLLBACK
[0m15:11:11.085512 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m15:11:11.086248 [debug] [ThreadPool]: On list_None_default_default_silver: Close
[0m15:11:11.090381 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b9cba657-fdec-450a-98ad-e9f93a6794ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x769a346e7d50>]}
[0m15:11:11.091594 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:11:11.092406 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:11:11.094599 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.stg_events
[0m15:11:11.095483 [info ] [Thread-1 (]: 1 of 3 START sql view model default_default_silver.stg_events .................. [RUN]
[0m15:11:11.096472 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default_default_silver, now model.data_pipeline_poc.stg_events)
[0m15:11:11.097304 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_poc.stg_events
[0m15:11:11.103581 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_poc.stg_events"
[0m15:11:11.117124 [debug] [Thread-1 (]: Began executing node model.data_pipeline_poc.stg_events
[0m15:11:11.135482 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_poc.stg_events"
[0m15:11:11.148712 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:11:11.149587 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.stg_events"
[0m15:11:11.150360 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_events: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.stg_events"} */
create or replace view default_default_silver.stg_events
  
  
  as
    

-- Staging model for raw events from bronze layer
-- This model cleans and standardizes the raw event data

SELECT
    event_id,
    event_type,
    user_id,
    event_timestamp,
    event_data,
    current_timestamp() as dbt_loaded_at
FROM bronze.raw_events
WHERE event_id IS NOT NULL

[0m15:11:11.151062 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:11:12.153453 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.UnsupportedOperationException: Server does not support endpoint: POST /v1/{prefix}/namespaces/{namespace}/views\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:46)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.UnsupportedOperationException: Server does not support endpoint: POST /v1/{prefix}/namespaces/{namespace}/views\n\tat org.apache.iceberg.rest.Endpoint.check(Endpoint.java:140)\n\tat org.apache.iceberg.rest.RESTSessionCatalog$RESTViewBuilder.create(RESTSessionCatalog.java:1400)\n\tat org.apache.iceberg.rest.RESTSessionCatalog$RESTViewBuilder.createOrReplace(RESTSessionCatalog.java:1448)\n\tat org.apache.iceberg.spark.SparkCatalog.replaceView(SparkCatalog.java:641)\n\tat org.apache.spark.sql.execution.datasources.v2.CreateV2ViewExec.replaceView(CreateV2ViewExec.scala:103)\n\tat org.apache.spark.sql.execution.datasources.v2.CreateV2ViewExec.run(CreateV2ViewExec.scala:64)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m15:11:12.154864 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m15:11:12.156232 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.stg_events"} */
create or replace view default_default_silver.stg_events
  
  
  as
    

-- Staging model for raw events from bronze layer
-- This model cleans and standardizes the raw event data

SELECT
    event_id,
    event_type,
    user_id,
    event_timestamp,
    event_data,
    current_timestamp() as dbt_loaded_at
FROM bronze.raw_events
WHERE event_id IS NOT NULL

[0m15:11:12.157519 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.UnsupportedOperationException: Server does not support endpoint: POST /v1/{prefix}/namespaces/{namespace}/views
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:46)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: java.lang.UnsupportedOperationException: Server does not support endpoint: POST /v1/{prefix}/namespaces/{namespace}/views
  	at org.apache.iceberg.rest.Endpoint.check(Endpoint.java:140)
  	at org.apache.iceberg.rest.RESTSessionCatalog$RESTViewBuilder.create(RESTSessionCatalog.java:1400)
  	at org.apache.iceberg.rest.RESTSessionCatalog$RESTViewBuilder.createOrReplace(RESTSessionCatalog.java:1448)
  	at org.apache.iceberg.spark.SparkCatalog.replaceView(SparkCatalog.java:641)
  	at org.apache.spark.sql.execution.datasources.v2.CreateV2ViewExec.replaceView(CreateV2ViewExec.scala:103)
  	at org.apache.spark.sql.execution.datasources.v2.CreateV2ViewExec.run(CreateV2ViewExec.scala:64)
  	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
  	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
  	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
  	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
  	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
  	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
  	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
  	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
  	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
  	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
  	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
  	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m15:11:12.158711 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_events: ROLLBACK
[0m15:11:12.159542 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:11:12.160361 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_events: Close
[0m15:11:12.176756 [debug] [Thread-1 (]: Runtime Error in model stg_events (models/staging/stg_events.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.UnsupportedOperationException: Server does not support endpoint: POST /v1/{prefix}/namespaces/{namespace}/views
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:46)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: java.lang.UnsupportedOperationException: Server does not support endpoint: POST /v1/{prefix}/namespaces/{namespace}/views
    	at org.apache.iceberg.rest.Endpoint.check(Endpoint.java:140)
    	at org.apache.iceberg.rest.RESTSessionCatalog$RESTViewBuilder.create(RESTSessionCatalog.java:1400)
    	at org.apache.iceberg.rest.RESTSessionCatalog$RESTViewBuilder.createOrReplace(RESTSessionCatalog.java:1448)
    	at org.apache.iceberg.spark.SparkCatalog.replaceView(SparkCatalog.java:641)
    	at org.apache.spark.sql.execution.datasources.v2.CreateV2ViewExec.replaceView(CreateV2ViewExec.scala:103)
    	at org.apache.spark.sql.execution.datasources.v2.CreateV2ViewExec.run(CreateV2ViewExec.scala:64)
    	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
    	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
    	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
    	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
    	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
    	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
    	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
    	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
    	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
    	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
    	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
    	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
    	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
    	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m15:11:12.183114 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b9cba657-fdec-450a-98ad-e9f93a6794ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x769a37a60c10>]}
[0m15:11:12.184421 [error] [Thread-1 (]: 1 of 3 ERROR creating sql view model default_default_silver.stg_events ......... [[31mERROR[0m in 1.09s]
[0m15:11:12.186088 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.stg_events
[0m15:11:12.187065 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.stg_users
[0m15:11:12.187801 [debug] [Thread-4 (]: Marking all children of 'model.data_pipeline_poc.stg_events' to be skipped because of status 'error'.  Reason: Runtime Error in model stg_events (models/staging/stg_events.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.UnsupportedOperationException: Server does not support endpoint: POST /v1/{prefix}/namespaces/{namespace}/views
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:46)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: java.lang.UnsupportedOperationException: Server does not support endpoint: POST /v1/{prefix}/namespaces/{namespace}/views
    	at org.apache.iceberg.rest.Endpoint.check(Endpoint.java:140)
    	at org.apache.iceberg.rest.RESTSessionCatalog$RESTViewBuilder.create(RESTSessionCatalog.java:1400)
    	at org.apache.iceberg.rest.RESTSessionCatalog$RESTViewBuilder.createOrReplace(RESTSessionCatalog.java:1448)
    	at org.apache.iceberg.spark.SparkCatalog.replaceView(SparkCatalog.java:641)
    	at org.apache.spark.sql.execution.datasources.v2.CreateV2ViewExec.replaceView(CreateV2ViewExec.scala:103)
    	at org.apache.spark.sql.execution.datasources.v2.CreateV2ViewExec.run(CreateV2ViewExec.scala:64)
    	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
    	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
    	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
    	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
    	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
    	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
    	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
    	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
    	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
    	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
    	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
    	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
    	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
    	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    .
[0m15:11:12.188884 [info ] [Thread-1 (]: 2 of 3 START sql view model default_default_silver.stg_users ................... [RUN]
[0m15:11:12.191505 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline_poc.stg_events, now model.data_pipeline_poc.stg_users)
[0m15:11:12.192339 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_poc.stg_users
[0m15:11:12.195570 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_poc.stg_users"
[0m15:11:12.201789 [debug] [Thread-1 (]: Began executing node model.data_pipeline_poc.stg_users
[0m15:11:12.204319 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_poc.stg_users"
[0m15:11:12.212518 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:11:12.213514 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.stg_users"
[0m15:11:12.214350 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_users: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.stg_users"} */
create or replace view default_default_silver.stg_users
  
  
  as
    

-- Staging model for raw users from bronze layer
-- This model cleans and standardizes the raw user data

SELECT
    user_id,
    user_name,
    user_email,
    created_at,
    current_timestamp() as dbt_loaded_at
FROM bronze.raw_users
WHERE user_id IS NOT NULL

[0m15:11:12.215085 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:11:12.325185 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='42703', errorCode=0, errorMessage="org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `user_email` cannot be resolved. Did you mean one of the following? [`email`, `user_id`, `user_name`, `created_at`].; line 14 pos 4;\n'CreateIcebergView SELECT\n    user_id,\n    user_name,\n    user_email,\n    created_at,\n    current_timestamp() as dbt_loaded_at\nFROM bronze.raw_users\nWHERE user_id IS NOT NULL, false, true, false\n:- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@766d4dfe, default_default_silver.stg_users\n+- 'Project [user_id#76, user_name#77, 'user_email, created_at#79, current_timestamp() AS dbt_loaded_at#75]\n   +- Filter isnotnull(user_id#76)\n      +- SubqueryAlias spark_catalog.bronze.raw_users\n         +- RelationV2[user_id#76, user_name#77, email#78, created_at#79] spark_catalog.bronze.raw_users spark_catalog.bronze.raw_users\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `user_email` cannot be resolved. Did you mean one of the following? [`email`, `user_id`, `user_name`, `created_at`].; line 14 pos 4;\n'CreateIcebergView SELECT\n    user_id,\n    user_name,\n    user_email,\n    created_at,\n    current_timestamp() as dbt_loaded_at\nFROM bronze.raw_users\nWHERE user_id IS NOT NULL, false, true, false\n:- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@766d4dfe, default_default_silver.stg_users\n+- 'Project [user_id#76, user_name#77, 'user_email, created_at#79, current_timestamp() AS dbt_loaded_at#75]\n   +- Filter isnotnull(user_id#76)\n      +- SubqueryAlias spark_catalog.bronze.raw_users\n         +- RelationV2[user_id#76, user_name#77, email#78, created_at#79] spark_catalog.bronze.raw_users spark_catalog.bronze.raw_users\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)\n\tat scala.collection.immutable.Stream.foreach(Stream.scala:533)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n", taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m15:11:12.326764 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m15:11:12.327871 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.stg_users"} */
create or replace view default_default_silver.stg_users
  
  
  as
    

-- Staging model for raw users from bronze layer
-- This model cleans and standardizes the raw user data

SELECT
    user_id,
    user_name,
    user_email,
    created_at,
    current_timestamp() as dbt_loaded_at
FROM bronze.raw_users
WHERE user_id IS NOT NULL

[0m15:11:12.329089 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `user_email` cannot be resolved. Did you mean one of the following? [`email`, `user_id`, `user_name`, `created_at`].; line 14 pos 4;
  'CreateIcebergView SELECT
      user_id,
      user_name,
      user_email,
      created_at,
      current_timestamp() as dbt_loaded_at
  FROM bronze.raw_users
  WHERE user_id IS NOT NULL, false, true, false
  :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@766d4dfe, default_default_silver.stg_users
  +- 'Project [user_id#76, user_name#77, 'user_email, created_at#79, current_timestamp() AS dbt_loaded_at#75]
     +- Filter isnotnull(user_id#76)
        +- SubqueryAlias spark_catalog.bronze.raw_users
           +- RelationV2[user_id#76, user_name#77, email#78, created_at#79] spark_catalog.bronze.raw_users spark_catalog.bronze.raw_users
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `user_email` cannot be resolved. Did you mean one of the following? [`email`, `user_id`, `user_name`, `created_at`].; line 14 pos 4;
  'CreateIcebergView SELECT
      user_id,
      user_name,
      user_email,
      created_at,
      current_timestamp() as dbt_loaded_at
  FROM bronze.raw_users
  WHERE user_id IS NOT NULL, false, true, false
  :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@766d4dfe, default_default_silver.stg_users
  +- 'Project [user_id#76, user_name#77, 'user_email, created_at#79, current_timestamp() AS dbt_loaded_at#75]
     +- Filter isnotnull(user_id#76)
        +- SubqueryAlias spark_catalog.bronze.raw_users
           +- RelationV2[user_id#76, user_name#77, email#78, created_at#79] spark_catalog.bronze.raw_users spark_catalog.bronze.raw_users
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
  	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m15:11:12.330396 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_users: ROLLBACK
[0m15:11:12.331250 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:11:12.332283 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_users: Close
[0m15:11:12.338917 [debug] [Thread-1 (]: Runtime Error in model stg_users (models/staging/stg_users.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `user_email` cannot be resolved. Did you mean one of the following? [`email`, `user_id`, `user_name`, `created_at`].; line 14 pos 4;
    'CreateIcebergView SELECT
        user_id,
        user_name,
        user_email,
        created_at,
        current_timestamp() as dbt_loaded_at
    FROM bronze.raw_users
    WHERE user_id IS NOT NULL, false, true, false
    :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@766d4dfe, default_default_silver.stg_users
    +- 'Project [user_id#76, user_name#77, 'user_email, created_at#79, current_timestamp() AS dbt_loaded_at#75]
       +- Filter isnotnull(user_id#76)
          +- SubqueryAlias spark_catalog.bronze.raw_users
             +- RelationV2[user_id#76, user_name#77, email#78, created_at#79] spark_catalog.bronze.raw_users spark_catalog.bronze.raw_users
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `user_email` cannot be resolved. Did you mean one of the following? [`email`, `user_id`, `user_name`, `created_at`].; line 14 pos 4;
    'CreateIcebergView SELECT
        user_id,
        user_name,
        user_email,
        created_at,
        current_timestamp() as dbt_loaded_at
    FROM bronze.raw_users
    WHERE user_id IS NOT NULL, false, true, false
    :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@766d4dfe, default_default_silver.stg_users
    +- 'Project [user_id#76, user_name#77, 'user_email, created_at#79, current_timestamp() AS dbt_loaded_at#75]
       +- Filter isnotnull(user_id#76)
          +- SubqueryAlias spark_catalog.bronze.raw_users
             +- RelationV2[user_id#76, user_name#77, email#78, created_at#79] spark_catalog.bronze.raw_users spark_catalog.bronze.raw_users
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m15:11:12.340562 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b9cba657-fdec-450a-98ad-e9f93a6794ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x769a3454bcd0>]}
[0m15:11:12.341784 [error] [Thread-1 (]: 2 of 3 ERROR creating sql view model default_default_silver.stg_users .......... [[31mERROR[0m in 0.15s]
[0m15:11:12.343251 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.stg_users
[0m15:11:12.344549 [debug] [Thread-4 (]: Marking all children of 'model.data_pipeline_poc.stg_users' to be skipped because of status 'error'.  Reason: Runtime Error in model stg_users (models/staging/stg_users.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `user_email` cannot be resolved. Did you mean one of the following? [`email`, `user_id`, `user_name`, `created_at`].; line 14 pos 4;
    'CreateIcebergView SELECT
        user_id,
        user_name,
        user_email,
        created_at,
        current_timestamp() as dbt_loaded_at
    FROM bronze.raw_users
    WHERE user_id IS NOT NULL, false, true, false
    :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@766d4dfe, default_default_silver.stg_users
    +- 'Project [user_id#76, user_name#77, 'user_email, created_at#79, current_timestamp() AS dbt_loaded_at#75]
       +- Filter isnotnull(user_id#76)
          +- SubqueryAlias spark_catalog.bronze.raw_users
             +- RelationV2[user_id#76, user_name#77, email#78, created_at#79] spark_catalog.bronze.raw_users spark_catalog.bronze.raw_users
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `user_email` cannot be resolved. Did you mean one of the following? [`email`, `user_id`, `user_name`, `created_at`].; line 14 pos 4;
    'CreateIcebergView SELECT
        user_id,
        user_name,
        user_email,
        created_at,
        current_timestamp() as dbt_loaded_at
    FROM bronze.raw_users
    WHERE user_id IS NOT NULL, false, true, false
    :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@766d4dfe, default_default_silver.stg_users
    +- 'Project [user_id#76, user_name#77, 'user_email, created_at#79, current_timestamp() AS dbt_loaded_at#75]
       +- Filter isnotnull(user_id#76)
          +- SubqueryAlias spark_catalog.bronze.raw_users
             +- RelationV2[user_id#76, user_name#77, email#78, created_at#79] spark_catalog.bronze.raw_users spark_catalog.bronze.raw_users
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    .
[0m15:11:12.346331 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.fct_events_enriched
[0m15:11:12.347190 [info ] [Thread-1 (]: 3 of 3 SKIP relation default_default_gold.fct_events_enriched .................. [[33mSKIP[0m]
[0m15:11:12.348147 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.fct_events_enriched
[0m15:11:12.350091 [debug] [MainThread]: On master: ROLLBACK
[0m15:11:12.351180 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:11:12.381124 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m15:11:12.382561 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:11:12.383382 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:11:12.384069 [debug] [MainThread]: On master: ROLLBACK
[0m15:11:12.384769 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m15:11:12.385448 [debug] [MainThread]: On master: Close
[0m15:11:12.388052 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:11:12.388819 [debug] [MainThread]: Connection 'model.data_pipeline_poc.stg_users' was properly closed.
[0m15:11:12.390363 [info ] [MainThread]: 
[0m15:11:12.391405 [info ] [MainThread]: Finished running 1 table model, 2 view models in 0 hours 0 minutes and 2.69 seconds (2.69s).
[0m15:11:12.393143 [debug] [MainThread]: Command end result
[0m15:11:12.424721 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/app/dbt/target/manifest.json
[0m15:11:12.428756 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/app/dbt/target/semantic_manifest.json
[0m15:11:12.435327 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/app/dbt/target/run_results.json
[0m15:11:12.436023 [info ] [MainThread]: 
[0m15:11:12.437096 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m15:11:12.438101 [info ] [MainThread]: 
[0m15:11:12.439246 [error] [MainThread]:   Runtime Error in model stg_events (models/staging/stg_events.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.UnsupportedOperationException: Server does not support endpoint: POST /v1/{prefix}/namespaces/{namespace}/views
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:46)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: java.lang.UnsupportedOperationException: Server does not support endpoint: POST /v1/{prefix}/namespaces/{namespace}/views
    	at org.apache.iceberg.rest.Endpoint.check(Endpoint.java:140)
    	at org.apache.iceberg.rest.RESTSessionCatalog$RESTViewBuilder.create(RESTSessionCatalog.java:1400)
    	at org.apache.iceberg.rest.RESTSessionCatalog$RESTViewBuilder.createOrReplace(RESTSessionCatalog.java:1448)
    	at org.apache.iceberg.spark.SparkCatalog.replaceView(SparkCatalog.java:641)
    	at org.apache.spark.sql.execution.datasources.v2.CreateV2ViewExec.replaceView(CreateV2ViewExec.scala:103)
    	at org.apache.spark.sql.execution.datasources.v2.CreateV2ViewExec.run(CreateV2ViewExec.scala:64)
    	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
    	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
    	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
    	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
    	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
    	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
    	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
    	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
    	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
    	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
    	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
    	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
    	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
    	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m15:11:12.441113 [info ] [MainThread]: 
[0m15:11:12.442503 [error] [MainThread]:   Runtime Error in model stg_users (models/staging/stg_users.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `user_email` cannot be resolved. Did you mean one of the following? [`email`, `user_id`, `user_name`, `created_at`].; line 14 pos 4;
    'CreateIcebergView SELECT
        user_id,
        user_name,
        user_email,
        created_at,
        current_timestamp() as dbt_loaded_at
    FROM bronze.raw_users
    WHERE user_id IS NOT NULL, false, true, false
    :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@766d4dfe, default_default_silver.stg_users
    +- 'Project [user_id#76, user_name#77, 'user_email, created_at#79, current_timestamp() AS dbt_loaded_at#75]
       +- Filter isnotnull(user_id#76)
          +- SubqueryAlias spark_catalog.bronze.raw_users
             +- RelationV2[user_id#76, user_name#77, email#78, created_at#79] spark_catalog.bronze.raw_users spark_catalog.bronze.raw_users
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `user_email` cannot be resolved. Did you mean one of the following? [`email`, `user_id`, `user_name`, `created_at`].; line 14 pos 4;
    'CreateIcebergView SELECT
        user_id,
        user_name,
        user_email,
        created_at,
        current_timestamp() as dbt_loaded_at
    FROM bronze.raw_users
    WHERE user_id IS NOT NULL, false, true, false
    :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@766d4dfe, default_default_silver.stg_users
    +- 'Project [user_id#76, user_name#77, 'user_email, created_at#79, current_timestamp() AS dbt_loaded_at#75]
       +- Filter isnotnull(user_id#76)
          +- SubqueryAlias spark_catalog.bronze.raw_users
             +- RelationV2[user_id#76, user_name#77, email#78, created_at#79] spark_catalog.bronze.raw_users spark_catalog.bronze.raw_users
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m15:11:12.443774 [info ] [MainThread]: 
[0m15:11:12.444784 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=1 TOTAL=3
[0m15:11:12.446142 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 4.9311676, "process_in_blocks": "3184", "process_kernel_time": 0.275551, "process_mem_max_rss": "112564", "process_out_blocks": "120", "process_user_time": 2.301668}
[0m15:11:12.447161 [debug] [MainThread]: Command `dbt run` failed at 15:11:12.447059 after 4.93 seconds
[0m15:11:12.448015 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x769a384f3110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x769a3bcc3d50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x769a384ebfd0>]}
[0m15:11:12.448818 [debug] [MainThread]: Flushing usage events
[0m15:11:14.213974 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:13:09.367638 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d7cd22af90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d7cd6291d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d7cd22aa10>]}


============================== 15:13:09.370825 | 3f88ad71-bcc9-401e-b443-6191e649cd50 ==============================
[0m15:13:09.370825 [info ] [MainThread]: Running with dbt=1.9.0
[0m15:13:09.371826 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/usr/app/dbt/logs', 'debug': 'False', 'profiles_dir': '/usr/app/dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt run', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:13:09.418010 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m15:13:09.419182 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m15:13:09.419950 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m15:13:09.516128 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3f88ad71-bcc9-401e-b443-6191e649cd50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d7ccb54590>]}
[0m15:13:09.556731 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3f88ad71-bcc9-401e-b443-6191e649cd50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d7cd76f050>]}
[0m15:13:09.558174 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m15:13:09.619368 [debug] [MainThread]: checksum: c99e828bba267739642b5a3ce85f17518764ea526e0e6c4fdc649171c1a66bff, vars: {}, profile: , target: , version: 1.9.0
[0m15:13:09.701563 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m15:13:09.702657 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '3f88ad71-bcc9-401e-b443-6191e649cd50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d7ce0036d0>]}
[0m15:13:11.551377 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- seeds.data_pipeline_poc
- snapshots.data_pipeline_poc
[0m15:13:11.560296 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3f88ad71-bcc9-401e-b443-6191e649cd50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d7c554d250>]}
[0m15:13:11.632823 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/app/dbt/target/manifest.json
[0m15:13:11.637859 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/app/dbt/target/semantic_manifest.json
[0m15:13:11.651838 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3f88ad71-bcc9-401e-b443-6191e649cd50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d7c523b890>]}
[0m15:13:11.652734 [info ] [MainThread]: Found 3 models, 4 data tests, 2 sources, 583 macros
[0m15:13:11.653806 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3f88ad71-bcc9-401e-b443-6191e649cd50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d7c54cf8d0>]}
[0m15:13:11.655668 [info ] [MainThread]: 
[0m15:13:11.656570 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:13:11.657381 [info ] [MainThread]: 
[0m15:13:11.658566 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m15:13:11.664545 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m15:13:11.671854 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m15:13:11.672619 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m15:13:11.673301 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:13:11.727607 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:13:11.729735 [debug] [ThreadPool]: SQL status: OK in 0.056 seconds
[0m15:13:11.733763 [debug] [ThreadPool]: On list_schemas: Close
[0m15:13:11.738805 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m15:13:11.739893 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m15:13:11.740597 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:13:11.795595 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:13:11.797793 [debug] [ThreadPool]: SQL status: OK in 0.057 seconds
[0m15:13:11.802261 [debug] [ThreadPool]: On list_schemas: Close
[0m15:13:11.807394 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default_default_gold)
[0m15:13:11.812043 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:13:11.813021 [debug] [ThreadPool]: Using spark connection "list_None_default_default_gold"
[0m15:13:11.813834 [debug] [ThreadPool]: On list_None_default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show table extended in default_default_gold like '*'
  
[0m15:13:11.814610 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:13:11.849206 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#105, tableName#106, isTemporary#107, information#108]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@37c0e9f8, [default_default_gold]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#105, tableName#106, isTemporary#107, information#108]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@37c0e9f8, [default_default_gold]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m15:13:11.850390 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m15:13:11.851298 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show table extended in default_default_gold like '*'
  
[0m15:13:11.852315 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#105, tableName#106, isTemporary#107, information#108]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@37c0e9f8, [default_default_gold]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#105, tableName#106, isTemporary#107, information#108]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@37c0e9f8, [default_default_gold]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m15:13:11.853423 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m15:13:11.854448 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#105, tableName#106, isTemporary#107, information#108]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@37c0e9f8, [default_default_gold]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#105, tableName#106, isTemporary#107, information#108]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@37c0e9f8, [default_default_gold]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m15:13:11.858114 [debug] [ThreadPool]: Using spark connection "list_None_default_default_gold"
[0m15:13:11.859048 [debug] [ThreadPool]: On list_None_default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show tables in default_default_gold like '*'
  
[0m15:13:11.878202 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:13:11.879325 [debug] [ThreadPool]: SQL status: OK in 0.019 seconds
[0m15:13:11.882692 [debug] [ThreadPool]: On list_None_default_default_gold: ROLLBACK
[0m15:13:11.883455 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m15:13:11.884213 [debug] [ThreadPool]: On list_None_default_default_gold: Close
[0m15:13:11.888330 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default_default_gold, now list_None_default_default_silver)
[0m15:13:11.890663 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:13:11.891393 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m15:13:11.892364 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show table extended in default_default_silver like '*'
  
[0m15:13:11.893305 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:13:11.927425 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#120, tableName#121, isTemporary#122, information#123]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7a9597bb, [default_default_silver]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#120, tableName#121, isTemporary#122, information#123]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7a9597bb, [default_default_silver]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m15:13:11.928678 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m15:13:11.929869 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show table extended in default_default_silver like '*'
  
[0m15:13:11.930990 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#120, tableName#121, isTemporary#122, information#123]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7a9597bb, [default_default_silver]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#120, tableName#121, isTemporary#122, information#123]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7a9597bb, [default_default_silver]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m15:13:11.931935 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m15:13:11.932856 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#120, tableName#121, isTemporary#122, information#123]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7a9597bb, [default_default_silver]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#120, tableName#121, isTemporary#122, information#123]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7a9597bb, [default_default_silver]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m15:13:11.935373 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m15:13:11.936119 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show tables in default_default_silver like '*'
  
[0m15:13:11.956258 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:13:11.957253 [debug] [ThreadPool]: SQL status: OK in 0.020 seconds
[0m15:13:11.960562 [debug] [ThreadPool]: On list_None_default_default_silver: ROLLBACK
[0m15:13:11.961359 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m15:13:11.962192 [debug] [ThreadPool]: On list_None_default_default_silver: Close
[0m15:13:11.967008 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3f88ad71-bcc9-401e-b443-6191e649cd50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d7cd070610>]}
[0m15:13:11.968320 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:13:11.969326 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:13:11.971694 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.stg_events
[0m15:13:11.972582 [info ] [Thread-1 (]: 1 of 3 START sql table model default_default_silver.stg_events ................. [RUN]
[0m15:13:11.973880 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default_default_silver, now model.data_pipeline_poc.stg_events)
[0m15:13:11.974907 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_poc.stg_events
[0m15:13:11.981360 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_poc.stg_events"
[0m15:13:11.987752 [debug] [Thread-1 (]: Began executing node model.data_pipeline_poc.stg_events
[0m15:13:12.003253 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.stg_events"
[0m15:13:12.004203 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_events: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.stg_events"} */
drop table if exists default_default_silver.stg_events
[0m15:13:12.005062 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:13:12.057629 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:13:12.061775 [debug] [Thread-1 (]: SQL status: OK in 0.057 seconds
[0m15:13:12.089037 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_poc.stg_events"
[0m15:13:12.096479 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:13:12.097251 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.stg_events"
[0m15:13:12.097972 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_events: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.stg_events"} */

  
    
        create or replace table default_default_silver.stg_events
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Staging model for raw events from bronze layer
-- This model cleans and standardizes the raw event data

SELECT
    event_id,
    event_type,
    user_id,
    event_timestamp,
    event_data,
    current_timestamp() as dbt_loaded_at
FROM bronze.raw_events
WHERE event_id IS NOT NULL
  
[0m15:13:13.880570 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:13:13.881701 [debug] [Thread-1 (]: SQL status: OK in 1.783 seconds
[0m15:13:13.894908 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_events: ROLLBACK
[0m15:13:13.895965 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:13:13.896743 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_events: Close
[0m15:13:13.905038 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3f88ad71-bcc9-401e-b443-6191e649cd50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d7c52191d0>]}
[0m15:13:13.906500 [info ] [Thread-1 (]: 1 of 3 OK created sql table model default_default_silver.stg_events ............ [[32mOK[0m in 1.93s]
[0m15:13:13.907663 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.stg_events
[0m15:13:13.909346 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.stg_users
[0m15:13:13.910500 [info ] [Thread-1 (]: 2 of 3 START sql table model default_default_silver.stg_users .................. [RUN]
[0m15:13:13.911867 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline_poc.stg_events, now model.data_pipeline_poc.stg_users)
[0m15:13:13.912782 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_poc.stg_users
[0m15:13:13.916032 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_poc.stg_users"
[0m15:13:13.922261 [debug] [Thread-1 (]: Began executing node model.data_pipeline_poc.stg_users
[0m15:13:13.926365 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.stg_users"
[0m15:13:13.927330 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_users: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.stg_users"} */
drop table if exists default_default_silver.stg_users
[0m15:13:13.928129 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:13:13.982407 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:13:13.983682 [debug] [Thread-1 (]: SQL status: OK in 0.055 seconds
[0m15:13:13.986207 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_poc.stg_users"
[0m15:13:13.993647 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:13:13.994616 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.stg_users"
[0m15:13:13.995414 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_users: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.stg_users"} */

  
    
        create or replace table default_default_silver.stg_users
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Staging model for raw users from bronze layer
-- This model cleans and standardizes the raw user data

SELECT
    user_id,
    user_name,
    email as user_email,
    created_at,
    current_timestamp() as dbt_loaded_at
FROM bronze.raw_users
WHERE user_id IS NOT NULL
  
[0m15:13:14.614660 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:13:14.617821 [debug] [Thread-1 (]: SQL status: OK in 0.622 seconds
[0m15:13:14.619950 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_users: ROLLBACK
[0m15:13:14.620981 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:13:14.621927 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_users: Close
[0m15:13:14.627677 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3f88ad71-bcc9-401e-b443-6191e649cd50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d7cd786890>]}
[0m15:13:14.629053 [info ] [Thread-1 (]: 2 of 3 OK created sql table model default_default_silver.stg_users ............. [[32mOK[0m in 0.72s]
[0m15:13:14.630276 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.stg_users
[0m15:13:14.631911 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.fct_events_enriched
[0m15:13:14.633308 [info ] [Thread-1 (]: 3 of 3 START sql table model default_default_gold.fct_events_enriched .......... [RUN]
[0m15:13:14.634693 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline_poc.stg_users, now model.data_pipeline_poc.fct_events_enriched)
[0m15:13:14.635654 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_poc.fct_events_enriched
[0m15:13:14.639681 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_poc.fct_events_enriched"
[0m15:13:14.652690 [debug] [Thread-1 (]: Began executing node model.data_pipeline_poc.fct_events_enriched
[0m15:13:14.658114 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.fct_events_enriched"
[0m15:13:14.659713 [debug] [Thread-1 (]: On model.data_pipeline_poc.fct_events_enriched: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.fct_events_enriched"} */
drop table if exists default_default_gold.fct_events_enriched
[0m15:13:14.661201 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:13:14.737062 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:13:14.738724 [debug] [Thread-1 (]: SQL status: OK in 0.078 seconds
[0m15:13:14.742148 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_poc.fct_events_enriched"
[0m15:13:14.757194 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:13:14.758591 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.fct_events_enriched"
[0m15:13:14.760699 [debug] [Thread-1 (]: On model.data_pipeline_poc.fct_events_enriched: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.fct_events_enriched"} */

  
    
        create or replace table default_default_gold.fct_events_enriched
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Mart model: Events enriched with user information
-- This model joins event data with user data for analytics

WITH events AS (
    SELECT * FROM default_default_silver.stg_events
),

users AS (
    SELECT * FROM default_default_silver.stg_users
),

enriched AS (
    SELECT
        e.event_id,
        e.event_type,
        e.event_timestamp,
        e.event_data,
        e.user_id,
        u.user_name,
        u.user_email,
        u.created_at as user_created_at,
        e.dbt_loaded_at
    FROM events e
    LEFT JOIN users u ON e.user_id = u.user_id
)

SELECT * FROM enriched
  
[0m15:13:15.731623 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:13:15.733415 [debug] [Thread-1 (]: SQL status: OK in 0.970 seconds
[0m15:13:15.735959 [debug] [Thread-1 (]: On model.data_pipeline_poc.fct_events_enriched: ROLLBACK
[0m15:13:15.737235 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:13:15.738389 [debug] [Thread-1 (]: On model.data_pipeline_poc.fct_events_enriched: Close
[0m15:13:15.743409 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3f88ad71-bcc9-401e-b443-6191e649cd50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d7c41151d0>]}
[0m15:13:15.744799 [info ] [Thread-1 (]: 3 of 3 OK created sql table model default_default_gold.fct_events_enriched ..... [[32mOK[0m in 1.11s]
[0m15:13:15.746197 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.fct_events_enriched
[0m15:13:15.748507 [debug] [MainThread]: On master: ROLLBACK
[0m15:13:15.749708 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:13:15.784025 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m15:13:15.785225 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:13:15.786109 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:13:15.787041 [debug] [MainThread]: On master: ROLLBACK
[0m15:13:15.788073 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m15:13:15.788898 [debug] [MainThread]: On master: Close
[0m15:13:15.796595 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:13:15.797761 [debug] [MainThread]: Connection 'model.data_pipeline_poc.fct_events_enriched' was properly closed.
[0m15:13:15.798716 [info ] [MainThread]: 
[0m15:13:15.799576 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 4.14 seconds (4.14s).
[0m15:13:15.801362 [debug] [MainThread]: Command end result
[0m15:13:15.835104 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/app/dbt/target/manifest.json
[0m15:13:15.839158 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/app/dbt/target/semantic_manifest.json
[0m15:13:15.847343 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/app/dbt/target/run_results.json
[0m15:13:15.848222 [info ] [MainThread]: 
[0m15:13:15.849382 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:13:15.850372 [info ] [MainThread]: 
[0m15:13:15.851289 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
[0m15:13:15.852758 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 6.537694, "process_in_blocks": "0", "process_kernel_time": 0.324311, "process_mem_max_rss": "112376", "process_out_blocks": "0", "process_user_time": 2.525579}
[0m15:13:15.853809 [debug] [MainThread]: Command `dbt run` succeeded at 15:13:15.853698 after 6.54 seconds
[0m15:13:15.854945 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d7cd0912d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d7d0a28dd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d7d0ae6590>]}
[0m15:13:15.855840 [debug] [MainThread]: Flushing usage events
[0m15:13:23.511023 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:13:56.639209 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77105efee7d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77105f3e9410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77105f043d10>]}


============================== 15:13:56.642171 | c097a02e-7d7d-4291-894c-ae020b7173ff ==============================
[0m15:13:56.642171 [info ] [MainThread]: Running with dbt=1.9.0
[0m15:13:56.643399 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/app/dbt', 'debug': 'False', 'version_check': 'True', 'log_path': '/usr/app/dbt/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt test', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m15:13:56.685832 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m15:13:56.686720 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m15:13:56.688406 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m15:13:56.776093 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c097a02e-7d7d-4291-894c-ae020b7173ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77105e910e10>]}
[0m15:13:56.815403 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c097a02e-7d7d-4291-894c-ae020b7173ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77105f73b350>]}
[0m15:13:56.816621 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m15:13:56.870038 [debug] [MainThread]: checksum: c99e828bba267739642b5a3ce85f17518764ea526e0e6c4fdc649171c1a66bff, vars: {}, profile: , target: , version: 1.9.0
[0m15:13:57.680272 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:13:57.681058 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:13:57.686238 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- seeds.data_pipeline_poc
- snapshots.data_pipeline_poc
[0m15:13:57.714763 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c097a02e-7d7d-4291-894c-ae020b7173ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77105e8fef50>]}
[0m15:13:57.789345 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/app/dbt/target/manifest.json
[0m15:13:57.793810 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/app/dbt/target/semantic_manifest.json
[0m15:13:57.820001 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c097a02e-7d7d-4291-894c-ae020b7173ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77105e33a190>]}
[0m15:13:57.821374 [info ] [MainThread]: Found 3 models, 4 data tests, 2 sources, 583 macros
[0m15:13:57.822441 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c097a02e-7d7d-4291-894c-ae020b7173ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77105e331450>]}
[0m15:13:57.824997 [info ] [MainThread]: 
[0m15:13:57.825850 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:13:57.826624 [info ] [MainThread]: 
[0m15:13:57.827629 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m15:13:57.832679 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_default_default_silver'
[0m15:13:57.841075 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:13:57.842129 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m15:13:57.842847 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show table extended in default_default_silver like '*'
  
[0m15:13:57.843553 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:13:57.887884 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#228, tableName#229, isTemporary#230, information#231]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7dd7ff85, [default_default_silver]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#228, tableName#229, isTemporary#230, information#231]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7dd7ff85, [default_default_silver]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m15:13:57.890129 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m15:13:57.891363 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show table extended in default_default_silver like '*'
  
[0m15:13:57.892839 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#228, tableName#229, isTemporary#230, information#231]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7dd7ff85, [default_default_silver]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#228, tableName#229, isTemporary#230, information#231]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7dd7ff85, [default_default_silver]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m15:13:57.894079 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m15:13:57.894959 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#228, tableName#229, isTemporary#230, information#231]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7dd7ff85, [default_default_silver]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#228, tableName#229, isTemporary#230, information#231]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7dd7ff85, [default_default_silver]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m15:13:57.898627 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m15:13:57.899952 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show tables in default_default_silver like '*'
  
[0m15:13:57.958883 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:13:57.959940 [debug] [ThreadPool]: SQL status: OK in 0.059 seconds
[0m15:13:57.966143 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m15:13:57.967007 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
describe extended default_default_silver.stg_events
  
[0m15:13:58.050600 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:13:58.051616 [debug] [ThreadPool]: SQL status: OK in 0.084 seconds
[0m15:13:58.058491 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m15:13:58.059377 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
describe extended default_default_silver.stg_users
  
[0m15:13:58.091099 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:13:58.092089 [debug] [ThreadPool]: SQL status: OK in 0.032 seconds
[0m15:13:58.095744 [debug] [ThreadPool]: On list_None_default_default_silver: ROLLBACK
[0m15:13:58.096496 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m15:13:58.097381 [debug] [ThreadPool]: On list_None_default_default_silver: Close
[0m15:13:58.101286 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default_default_silver, now list_None_default_default_gold)
[0m15:13:58.103873 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:13:58.104710 [debug] [ThreadPool]: Using spark connection "list_None_default_default_gold"
[0m15:13:58.105629 [debug] [ThreadPool]: On list_None_default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show table extended in default_default_gold like '*'
  
[0m15:13:58.106317 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:13:58.134439 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#304, tableName#305, isTemporary#306, information#307]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@98c720a, [default_default_gold]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#304, tableName#305, isTemporary#306, information#307]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@98c720a, [default_default_gold]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m15:13:58.135850 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m15:13:58.136648 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show table extended in default_default_gold like '*'
  
[0m15:13:58.138448 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#304, tableName#305, isTemporary#306, information#307]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@98c720a, [default_default_gold]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#304, tableName#305, isTemporary#306, information#307]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@98c720a, [default_default_gold]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m15:13:58.139407 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m15:13:58.140259 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#304, tableName#305, isTemporary#306, information#307]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@98c720a, [default_default_gold]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#304, tableName#305, isTemporary#306, information#307]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@98c720a, [default_default_gold]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m15:13:58.142501 [debug] [ThreadPool]: Using spark connection "list_None_default_default_gold"
[0m15:13:58.143219 [debug] [ThreadPool]: On list_None_default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show tables in default_default_gold like '*'
  
[0m15:13:58.168914 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:13:58.170045 [debug] [ThreadPool]: SQL status: OK in 0.026 seconds
[0m15:13:58.177003 [debug] [ThreadPool]: Using spark connection "list_None_default_default_gold"
[0m15:13:58.178005 [debug] [ThreadPool]: On list_None_default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
describe extended default_default_gold.fct_events_enriched
  
[0m15:13:58.212288 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:13:58.217494 [debug] [ThreadPool]: SQL status: OK in 0.039 seconds
[0m15:13:58.222391 [debug] [ThreadPool]: On list_None_default_default_gold: ROLLBACK
[0m15:13:58.223335 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m15:13:58.224054 [debug] [ThreadPool]: On list_None_default_default_gold: Close
[0m15:13:58.227631 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c097a02e-7d7d-4291-894c-ae020b7173ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77105e874290>]}
[0m15:13:58.228478 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:13:58.229183 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:13:58.231298 [debug] [Thread-1 (]: Began running node test.data_pipeline_poc.not_null_fct_events_enriched_event_id.fe5b63d652
[0m15:13:58.232048 [info ] [Thread-1 (]: 1 of 4 START test not_null_fct_events_enriched_event_id ........................ [RUN]
[0m15:13:58.233083 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default_default_gold, now test.data_pipeline_poc.not_null_fct_events_enriched_event_id.fe5b63d652)
[0m15:13:58.233903 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_poc.not_null_fct_events_enriched_event_id.fe5b63d652
[0m15:13:58.245668 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_poc.not_null_fct_events_enriched_event_id.fe5b63d652"
[0m15:13:58.255963 [debug] [Thread-1 (]: Began executing node test.data_pipeline_poc.not_null_fct_events_enriched_event_id.fe5b63d652
[0m15:13:58.265789 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_poc.not_null_fct_events_enriched_event_id.fe5b63d652"
[0m15:13:58.277416 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:13:58.278204 [debug] [Thread-1 (]: Using spark connection "test.data_pipeline_poc.not_null_fct_events_enriched_event_id.fe5b63d652"
[0m15:13:58.278961 [debug] [Thread-1 (]: On test.data_pipeline_poc.not_null_fct_events_enriched_event_id.fe5b63d652: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "test.data_pipeline_poc.not_null_fct_events_enriched_event_id.fe5b63d652"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select event_id
from default_default_gold.fct_events_enriched
where event_id is null



      
    ) dbt_internal_test
[0m15:13:58.279638 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:13:58.552539 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:13:58.554706 [debug] [Thread-1 (]: SQL status: OK in 0.275 seconds
[0m15:13:58.563228 [debug] [Thread-1 (]: On test.data_pipeline_poc.not_null_fct_events_enriched_event_id.fe5b63d652: ROLLBACK
[0m15:13:58.564505 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:13:58.565485 [debug] [Thread-1 (]: On test.data_pipeline_poc.not_null_fct_events_enriched_event_id.fe5b63d652: Close
[0m15:13:58.569552 [info ] [Thread-1 (]: 1 of 4 PASS not_null_fct_events_enriched_event_id .............................. [[32mPASS[0m in 0.34s]
[0m15:13:58.571848 [debug] [Thread-1 (]: Finished running node test.data_pipeline_poc.not_null_fct_events_enriched_event_id.fe5b63d652
[0m15:13:58.572951 [debug] [Thread-1 (]: Began running node test.data_pipeline_poc.source_not_null_bronze_raw_events_event_id.54914cc225
[0m15:13:58.574057 [info ] [Thread-1 (]: 2 of 4 START test source_not_null_bronze_raw_events_event_id ................... [RUN]
[0m15:13:58.575263 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_poc.not_null_fct_events_enriched_event_id.fe5b63d652, now test.data_pipeline_poc.source_not_null_bronze_raw_events_event_id.54914cc225)
[0m15:13:58.576129 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_poc.source_not_null_bronze_raw_events_event_id.54914cc225
[0m15:13:58.580198 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_poc.source_not_null_bronze_raw_events_event_id.54914cc225"
[0m15:13:58.593849 [debug] [Thread-1 (]: Began executing node test.data_pipeline_poc.source_not_null_bronze_raw_events_event_id.54914cc225
[0m15:13:58.596471 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_poc.source_not_null_bronze_raw_events_event_id.54914cc225"
[0m15:13:58.609648 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:13:58.610554 [debug] [Thread-1 (]: Using spark connection "test.data_pipeline_poc.source_not_null_bronze_raw_events_event_id.54914cc225"
[0m15:13:58.611350 [debug] [Thread-1 (]: On test.data_pipeline_poc.source_not_null_bronze_raw_events_event_id.54914cc225: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "test.data_pipeline_poc.source_not_null_bronze_raw_events_event_id.54914cc225"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select event_id
from bronze.raw_events
where event_id is null



      
    ) dbt_internal_test
[0m15:13:58.612149 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:13:58.764083 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:13:58.765642 [debug] [Thread-1 (]: SQL status: OK in 0.153 seconds
[0m15:13:58.770673 [debug] [Thread-1 (]: On test.data_pipeline_poc.source_not_null_bronze_raw_events_event_id.54914cc225: ROLLBACK
[0m15:13:58.772051 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:13:58.773119 [debug] [Thread-1 (]: On test.data_pipeline_poc.source_not_null_bronze_raw_events_event_id.54914cc225: Close
[0m15:13:58.777379 [info ] [Thread-1 (]: 2 of 4 PASS source_not_null_bronze_raw_events_event_id ......................... [[32mPASS[0m in 0.20s]
[0m15:13:58.779158 [debug] [Thread-1 (]: Finished running node test.data_pipeline_poc.source_not_null_bronze_raw_events_event_id.54914cc225
[0m15:13:58.780196 [debug] [Thread-1 (]: Began running node test.data_pipeline_poc.source_not_null_bronze_raw_users_user_id.916ac5699f
[0m15:13:58.781230 [info ] [Thread-1 (]: 3 of 4 START test source_not_null_bronze_raw_users_user_id ..................... [RUN]
[0m15:13:58.782235 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_poc.source_not_null_bronze_raw_events_event_id.54914cc225, now test.data_pipeline_poc.source_not_null_bronze_raw_users_user_id.916ac5699f)
[0m15:13:58.783109 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_poc.source_not_null_bronze_raw_users_user_id.916ac5699f
[0m15:13:58.787084 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_poc.source_not_null_bronze_raw_users_user_id.916ac5699f"
[0m15:13:58.796378 [debug] [Thread-1 (]: Began executing node test.data_pipeline_poc.source_not_null_bronze_raw_users_user_id.916ac5699f
[0m15:13:58.799099 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_poc.source_not_null_bronze_raw_users_user_id.916ac5699f"
[0m15:13:58.807245 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:13:58.808212 [debug] [Thread-1 (]: Using spark connection "test.data_pipeline_poc.source_not_null_bronze_raw_users_user_id.916ac5699f"
[0m15:13:58.809036 [debug] [Thread-1 (]: On test.data_pipeline_poc.source_not_null_bronze_raw_users_user_id.916ac5699f: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "test.data_pipeline_poc.source_not_null_bronze_raw_users_user_id.916ac5699f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select user_id
from bronze.raw_users
where user_id is null



      
    ) dbt_internal_test
[0m15:13:58.809836 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:13:58.992824 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:13:58.994442 [debug] [Thread-1 (]: SQL status: OK in 0.185 seconds
[0m15:13:58.999268 [debug] [Thread-1 (]: On test.data_pipeline_poc.source_not_null_bronze_raw_users_user_id.916ac5699f: ROLLBACK
[0m15:13:59.000239 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:13:59.001045 [debug] [Thread-1 (]: On test.data_pipeline_poc.source_not_null_bronze_raw_users_user_id.916ac5699f: Close
[0m15:13:59.004934 [info ] [Thread-1 (]: 3 of 4 PASS source_not_null_bronze_raw_users_user_id ........................... [[32mPASS[0m in 0.22s]
[0m15:13:59.006614 [debug] [Thread-1 (]: Finished running node test.data_pipeline_poc.source_not_null_bronze_raw_users_user_id.916ac5699f
[0m15:13:59.007715 [debug] [Thread-1 (]: Began running node test.data_pipeline_poc.source_unique_bronze_raw_users_user_id.968e729bf0
[0m15:13:59.008766 [info ] [Thread-1 (]: 4 of 4 START test source_unique_bronze_raw_users_user_id ....................... [RUN]
[0m15:13:59.009995 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_poc.source_not_null_bronze_raw_users_user_id.916ac5699f, now test.data_pipeline_poc.source_unique_bronze_raw_users_user_id.968e729bf0)
[0m15:13:59.010876 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_poc.source_unique_bronze_raw_users_user_id.968e729bf0
[0m15:13:59.016232 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_poc.source_unique_bronze_raw_users_user_id.968e729bf0"
[0m15:13:59.025665 [debug] [Thread-1 (]: Began executing node test.data_pipeline_poc.source_unique_bronze_raw_users_user_id.968e729bf0
[0m15:13:59.028253 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_poc.source_unique_bronze_raw_users_user_id.968e729bf0"
[0m15:13:59.036029 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:13:59.036956 [debug] [Thread-1 (]: Using spark connection "test.data_pipeline_poc.source_unique_bronze_raw_users_user_id.968e729bf0"
[0m15:13:59.037791 [debug] [Thread-1 (]: On test.data_pipeline_poc.source_unique_bronze_raw_users_user_id.968e729bf0: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "test.data_pipeline_poc.source_unique_bronze_raw_users_user_id.968e729bf0"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    user_id as unique_field,
    count(*) as n_records

from bronze.raw_users
where user_id is not null
group by user_id
having count(*) > 1



      
    ) dbt_internal_test
[0m15:13:59.038557 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:13:59.657120 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:13:59.659752 [debug] [Thread-1 (]: SQL status: OK in 0.621 seconds
[0m15:13:59.664329 [debug] [Thread-1 (]: On test.data_pipeline_poc.source_unique_bronze_raw_users_user_id.968e729bf0: ROLLBACK
[0m15:13:59.665481 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:13:59.666311 [debug] [Thread-1 (]: On test.data_pipeline_poc.source_unique_bronze_raw_users_user_id.968e729bf0: Close
[0m15:13:59.669755 [error] [Thread-1 (]: 4 of 4 FAIL 3 source_unique_bronze_raw_users_user_id ........................... [[31mFAIL 3[0m in 0.66s]
[0m15:13:59.671249 [debug] [Thread-1 (]: Finished running node test.data_pipeline_poc.source_unique_bronze_raw_users_user_id.968e729bf0
[0m15:13:59.673233 [debug] [MainThread]: On master: ROLLBACK
[0m15:13:59.674170 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:13:59.702145 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m15:13:59.703410 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:13:59.704488 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:13:59.705511 [debug] [MainThread]: On master: ROLLBACK
[0m15:13:59.706449 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m15:13:59.707332 [debug] [MainThread]: On master: Close
[0m15:13:59.710304 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:13:59.711088 [debug] [MainThread]: Connection 'test.data_pipeline_poc.source_unique_bronze_raw_users_user_id.968e729bf0' was properly closed.
[0m15:13:59.711963 [info ] [MainThread]: 
[0m15:13:59.712905 [info ] [MainThread]: Finished running 4 data tests in 0 hours 0 minutes and 1.88 seconds (1.88s).
[0m15:13:59.714406 [debug] [MainThread]: Command end result
[0m15:13:59.749453 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/app/dbt/target/manifest.json
[0m15:13:59.754031 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/app/dbt/target/semantic_manifest.json
[0m15:13:59.764988 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/app/dbt/target/run_results.json
[0m15:13:59.766002 [info ] [MainThread]: 
[0m15:13:59.767246 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m15:13:59.768602 [info ] [MainThread]: 
[0m15:13:59.769889 [error] [MainThread]: [31mFailure in test source_unique_bronze_raw_users_user_id (models/staging/sources.yml)[0m
[0m15:13:59.771564 [error] [MainThread]:   Got 3 results, configured to fail if != 0
[0m15:13:59.772800 [info ] [MainThread]: 
[0m15:13:59.774053 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_poc/models/staging/sources.yml/source_unique_bronze_raw_users_user_id.sql
[0m15:13:59.775421 [info ] [MainThread]: 
[0m15:13:59.776412 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=1 SKIP=0 TOTAL=4
[0m15:13:59.777900 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 3.1906054, "process_in_blocks": "2064", "process_kernel_time": 0.208922, "process_mem_max_rss": "108484", "process_out_blocks": "0", "process_user_time": 1.43835}
[0m15:13:59.778902 [debug] [MainThread]: Command `dbt test` failed at 15:13:59.778791 after 3.19 seconds
[0m15:13:59.779755 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7710629a9390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77106284ce10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77106284cd90>]}
[0m15:13:59.780622 [debug] [MainThread]: Flushing usage events
[0m15:14:00.577576 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:32:34.191729 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73c68242d2d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73c68242da10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73c68242cf50>]}


============================== 15:32:34.194934 | 73dce678-6efd-40d6-86e8-bf9c3f4902a2 ==============================
[0m15:32:34.194934 [info ] [MainThread]: Running with dbt=1.9.0
[0m15:32:34.195998 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/app/dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/usr/app/dbt/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt debug', 'send_anonymous_usage_stats': 'True'}
[0m15:32:34.205950 [info ] [MainThread]: dbt version: 1.9.0
[0m15:32:34.207075 [info ] [MainThread]: python version: 3.11.2
[0m15:32:34.208392 [info ] [MainThread]: python path: /usr/local/bin/python
[0m15:32:34.209514 [info ] [MainThread]: os info: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.31
[0m15:32:34.261347 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m15:32:34.262713 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m15:32:34.263630 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m15:32:34.288274 [info ] [MainThread]: Using profiles dir at /usr/app/dbt
[0m15:32:34.289987 [info ] [MainThread]: Using profiles.yml file at /usr/app/dbt/profiles.yml
[0m15:32:34.291335 [info ] [MainThread]: Using dbt_project.yml file at /usr/app/dbt/dbt_project.yml
[0m15:32:34.292380 [info ] [MainThread]: adapter type: spark
[0m15:32:34.293255 [info ] [MainThread]: adapter version: 1.9.0
[0m15:32:34.367266 [info ] [MainThread]: Configuration:
[0m15:32:34.368845 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m15:32:34.370452 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m15:32:34.371407 [info ] [MainThread]: Required dependencies:
[0m15:32:34.372583 [debug] [MainThread]: Executing "git --help"
[0m15:32:34.375682 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone             Clone a repository into a new directory\n   init              Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add               Add file contents to the index\n   mv                Move or rename a file, a directory, or a symlink\n   restore           Restore working tree files\n   rm                Remove files from the working tree and from the index\n   sparse-checkout   Initialize and modify the sparse-checkout\n\nexamine the history and state (see also: git help revisions)\n   bisect            Use binary search to find the commit that introduced a bug\n   diff              Show changes between commits, commit and working tree, etc\n   grep              Print lines matching a pattern\n   log               Show commit logs\n   show              Show various types of objects\n   status            Show the working tree status\n\ngrow, mark and tweak your common history\n   branch            List, create, or delete branches\n   commit            Record changes to the repository\n   merge             Join two or more development histories together\n   rebase            Reapply commits on top of another base tip\n   reset             Reset current HEAD to the specified state\n   switch            Switch branches\n   tag               Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch             Download objects and refs from another repository\n   pull              Fetch from and integrate with another repository or a local branch\n   push              Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m15:32:34.377165 [debug] [MainThread]: STDERR: "b''"
[0m15:32:34.378098 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m15:32:34.379045 [info ] [MainThread]: Connection:
[0m15:32:34.380328 [info ] [MainThread]:   host: spark-iceberg
[0m15:32:34.381521 [info ] [MainThread]:   port: 10000
[0m15:32:34.382431 [info ] [MainThread]:   cluster: None
[0m15:32:34.383350 [info ] [MainThread]:   endpoint: None
[0m15:32:34.384293 [info ] [MainThread]:   schema: default
[0m15:32:34.385220 [info ] [MainThread]:   organization: 0
[0m15:32:34.386631 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m15:32:34.443672 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m15:32:34.444854 [debug] [MainThread]: Using spark connection "debug"
[0m15:32:34.445818 [debug] [MainThread]: On debug: select 1 as id
[0m15:32:34.446696 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:32:34.519346 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m15:32:34.520814 [debug] [MainThread]: SQL status: OK in 0.074 seconds
[0m15:32:34.522377 [debug] [MainThread]: On debug: Close
[0m15:32:34.526487 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m15:32:34.527794 [info ] [MainThread]: [32mAll checks passed![0m
[0m15:32:34.529445 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 0.39035648, "process_in_blocks": "0", "process_kernel_time": 0.156163, "process_mem_max_rss": "99544", "process_out_blocks": "0", "process_user_time": 1.089138}
[0m15:32:34.530519 [debug] [MainThread]: Command `dbt debug` succeeded at 15:32:34.530403 after 0.39 seconds
[0m15:32:34.531347 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m15:32:34.532180 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73c685eed750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73c681e58ed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73c685ce2710>]}
[0m15:32:34.532991 [debug] [MainThread]: Flushing usage events
[0m15:32:35.513903 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:19:41.583192 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x768243753dd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x768243752950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7682437531d0>]}


============================== 10:19:41.587744 | c8f9625e-8a37-4e16-b0b0-dc9cc299c6a2 ==============================
[0m10:19:41.587744 [info ] [MainThread]: Running with dbt=1.9.0
[0m10:19:41.589576 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/usr/app/dbt/logs', 'fail_fast': 'False', 'profiles_dir': '/usr/app/dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m10:19:41.638153 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m10:19:41.639923 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m10:19:41.641991 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m10:19:41.747120 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c8f9625e-8a37-4e16-b0b0-dc9cc299c6a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x768243752e50>]}
[0m10:19:41.784968 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c8f9625e-8a37-4e16-b0b0-dc9cc299c6a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7682430b5550>]}
[0m10:19:41.787363 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m10:19:41.871713 [debug] [MainThread]: checksum: c99e828bba267739642b5a3ce85f17518764ea526e0e6c4fdc649171c1a66bff, vars: {}, profile: , target: , version: 1.9.0
[0m10:19:44.612308 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 7 files added, 1 files changed.
[0m10:19:44.614345 [debug] [MainThread]: Partial parsing: added file: data_pipeline_poc://models/gold/gold_kpi_uemoa_growth_yoy.sql
[0m10:19:44.616177 [debug] [MainThread]: Partial parsing: added file: data_pipeline_poc://models/silver/schema.yml
[0m10:19:44.617702 [debug] [MainThread]: Partial parsing: added file: data_pipeline_poc://models/gold/gold_mart_uemoa_public_finance.sql
[0m10:19:44.619633 [debug] [MainThread]: Partial parsing: added file: data_pipeline_poc://models/gold/gold_mart_uemoa_monetary_dashboard.sql
[0m10:19:44.621535 [debug] [MainThread]: Partial parsing: added file: data_pipeline_poc://models/gold/gold_mart_uemoa_external_stability.sql
[0m10:19:44.623178 [debug] [MainThread]: Partial parsing: added file: data_pipeline_poc://models/gold/gold_mart_uemoa_external_trade.sql
[0m10:19:44.625197 [debug] [MainThread]: Partial parsing: added file: data_pipeline_poc://models/silver/dim_uemoa_indicators.sql
[0m10:19:44.627130 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_poc://models/staging/sources.yml
[0m10:19:45.011836 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- seeds.data_pipeline_poc
- snapshots.data_pipeline_poc
[0m10:19:45.064141 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c8f9625e-8a37-4e16-b0b0-dc9cc299c6a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76822b916990>]}
[0m10:19:45.181956 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/app/dbt/target/manifest.json
[0m10:19:45.193458 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/app/dbt/target/semantic_manifest.json
[0m10:19:45.237984 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c8f9625e-8a37-4e16-b0b0-dc9cc299c6a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x768244c1e8d0>]}
[0m10:19:45.239661 [info ] [MainThread]: Found 9 models, 13 data tests, 3 sources, 583 macros
[0m10:19:45.241326 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c8f9625e-8a37-4e16-b0b0-dc9cc299c6a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x768244c0de50>]}
[0m10:19:45.244107 [info ] [MainThread]: 
[0m10:19:45.245629 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m10:19:45.246700 [info ] [MainThread]: 
[0m10:19:45.247827 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m10:19:45.253176 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m10:19:45.260273 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m10:19:45.262580 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m10:19:45.264140 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:19:45.266319 [error] [ThreadPool]: Could not connect to any of [('172.20.0.8', 10000)]
[0m10:19:45.268010 [warn ] [ThreadPool]: Spark adapter: Warning: Could not connect to any of [('172.20.0.8', 10000)], retrying due to 'retry_all' configuration set to true.
	Retrying in 10 seconds (0 of 3)
[0m10:19:55.270174 [error] [ThreadPool]: Could not connect to any of [('172.20.0.8', 10000)]
[0m10:19:55.272221 [warn ] [ThreadPool]: Spark adapter: Warning: Could not connect to any of [('172.20.0.8', 10000)], retrying due to 'retry_all' configuration set to true.
	Retrying in 10 seconds (1 of 3)
[0m10:20:05.273855 [error] [ThreadPool]: Could not connect to any of [('172.20.0.8', 10000)]
[0m10:20:05.277464 [warn ] [ThreadPool]: Spark adapter: Warning: Could not connect to any of [('172.20.0.8', 10000)], retrying due to 'retry_all' configuration set to true.
	Retrying in 10 seconds (2 of 3)
[0m10:20:17.397233 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m10:20:17.399608 [debug] [ThreadPool]: SQL status: OK in 32.137 seconds
[0m10:20:17.458198 [debug] [ThreadPool]: On list_schemas: Close
[0m10:20:17.484523 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m10:20:17.486792 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m10:20:17.488538 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:20:17.558355 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m10:20:17.560387 [debug] [ThreadPool]: SQL status: OK in 0.072 seconds
[0m10:20:17.564983 [debug] [ThreadPool]: On list_schemas: Close
[0m10:20:17.571351 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m10:20:17.573653 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m10:20:17.574897 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:20:17.640726 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m10:20:17.643318 [debug] [ThreadPool]: SQL status: OK in 0.068 seconds
[0m10:20:17.648222 [debug] [ThreadPool]: On list_schemas: Close
[0m10:20:17.653680 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default_default_silver)
[0m10:20:17.659622 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m10:20:17.661267 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m10:20:17.662817 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show table extended in default_default_silver like '*'
  
[0m10:20:17.664614 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:20:17.719286 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#28, tableName#29, isTemporary#30, information#31]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@21c293d1, [default_default_silver]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#28, tableName#29, isTemporary#30, information#31]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@21c293d1, [default_default_silver]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m10:20:17.721858 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m10:20:17.723915 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show table extended in default_default_silver like '*'
  
[0m10:20:17.725941 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#28, tableName#29, isTemporary#30, information#31]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@21c293d1, [default_default_silver]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#28, tableName#29, isTemporary#30, information#31]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@21c293d1, [default_default_silver]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m10:20:17.728287 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m10:20:17.730262 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#28, tableName#29, isTemporary#30, information#31]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@21c293d1, [default_default_silver]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#28, tableName#29, isTemporary#30, information#31]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@21c293d1, [default_default_silver]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m10:20:17.734052 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m10:20:17.735722 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show tables in default_default_silver like '*'
  
[0m10:20:17.829720 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m10:20:17.833050 [debug] [ThreadPool]: SQL status: OK in 0.096 seconds
[0m10:20:17.842021 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m10:20:17.844266 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
describe extended default_default_silver.stg_events
  
[0m10:20:18.042486 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m10:20:18.045035 [debug] [ThreadPool]: SQL status: OK in 0.199 seconds
[0m10:20:18.052475 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m10:20:18.054268 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
describe extended default_default_silver.stg_users
  
[0m10:20:18.114817 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m10:20:18.116694 [debug] [ThreadPool]: SQL status: OK in 0.061 seconds
[0m10:20:18.124668 [debug] [ThreadPool]: On list_None_default_default_silver: ROLLBACK
[0m10:20:18.126645 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m10:20:18.128548 [debug] [ThreadPool]: On list_None_default_default_silver: Close
[0m10:20:18.134174 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default_default_silver, now list_None_default)
[0m10:20:18.137278 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m10:20:18.138932 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m10:20:18.140693 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m10:20:18.141920 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:20:18.184816 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#104, tableName#105, isTemporary#106, information#107]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@6d0d9242, [default]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#104, tableName#105, isTemporary#106, information#107]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@6d0d9242, [default]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m10:20:18.187647 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m10:20:18.190121 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m10:20:18.192447 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#104, tableName#105, isTemporary#106, information#107]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@6d0d9242, [default]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#104, tableName#105, isTemporary#106, information#107]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@6d0d9242, [default]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m10:20:18.194466 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m10:20:18.195835 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#104, tableName#105, isTemporary#106, information#107]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@6d0d9242, [default]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#104, tableName#105, isTemporary#106, information#107]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@6d0d9242, [default]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m10:20:18.199126 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m10:20:18.200965 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default"} */
show tables in default like '*'
  
[0m10:20:18.233117 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m10:20:18.235733 [debug] [ThreadPool]: SQL status: OK in 0.033 seconds
[0m10:20:18.243531 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m10:20:18.245291 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default"} */
describe extended default.airbyte_test
  
[0m10:20:18.282057 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.iceberg.exceptions.ServiceFailureException: Server error: NotFoundException: Location does not exist: s3a://bronze/airbyte/metadata/00000-4dd569d6-08c2-4c7a-89fe-a8cd75b41268.metadata.json\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:46)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.iceberg.exceptions.ServiceFailureException: Server error: NotFoundException: Location does not exist: s3a://bronze/airbyte/metadata/00000-4dd569d6-08c2-4c7a-89fe-a8cd75b41268.metadata.json\n\tat org.apache.iceberg.rest.ErrorHandlers$DefaultErrorHandler.accept(ErrorHandlers.java:217)\n\tat org.apache.iceberg.rest.ErrorHandlers$TableErrorHandler.accept(ErrorHandlers.java:118)\n\tat org.apache.iceberg.rest.ErrorHandlers$TableErrorHandler.accept(ErrorHandlers.java:102)\n\tat org.apache.iceberg.rest.HTTPClient.throwFailure(HTTPClient.java:224)\n\tat org.apache.iceberg.rest.HTTPClient.execute(HTTPClient.java:308)\n\tat org.apache.iceberg.rest.BaseHTTPClient.get(BaseHTTPClient.java:77)\n\tat org.apache.iceberg.rest.RESTClient.get(RESTClient.java:97)\n\tat org.apache.iceberg.rest.RESTSessionCatalog.loadInternal(RESTSessionCatalog.java:465)\n\tat org.apache.iceberg.rest.RESTSessionCatalog.loadTable(RESTSessionCatalog.java:489)\n\tat org.apache.iceberg.catalog.BaseSessionCatalog$AsCatalog.loadTable(BaseSessionCatalog.java:99)\n\tat org.apache.iceberg.rest.RESTCatalog.loadTable(RESTCatalog.java:102)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)\n\tat java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1916)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)\n\tat org.apache.iceberg.CachingCatalog.loadTable(CachingCatalog.java:147)\n\tat org.apache.iceberg.spark.SparkCatalog.load(SparkCatalog.java:844)\n\tat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:169)\n\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.getTable(CatalogV2Util.scala:363)\n\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:337)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$2(Analyzer.scala:1228)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1223)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1182)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1117)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:634)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1117)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1076)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m10:20:18.284725 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m10:20:18.289216 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default"} */
describe extended default.airbyte_test
  
[0m10:20:18.292105 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.iceberg.exceptions.ServiceFailureException: Server error: NotFoundException: Location does not exist: s3a://bronze/airbyte/metadata/00000-4dd569d6-08c2-4c7a-89fe-a8cd75b41268.metadata.json
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:46)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.iceberg.exceptions.ServiceFailureException: Server error: NotFoundException: Location does not exist: s3a://bronze/airbyte/metadata/00000-4dd569d6-08c2-4c7a-89fe-a8cd75b41268.metadata.json
  	at org.apache.iceberg.rest.ErrorHandlers$DefaultErrorHandler.accept(ErrorHandlers.java:217)
  	at org.apache.iceberg.rest.ErrorHandlers$TableErrorHandler.accept(ErrorHandlers.java:118)
  	at org.apache.iceberg.rest.ErrorHandlers$TableErrorHandler.accept(ErrorHandlers.java:102)
  	at org.apache.iceberg.rest.HTTPClient.throwFailure(HTTPClient.java:224)
  	at org.apache.iceberg.rest.HTTPClient.execute(HTTPClient.java:308)
  	at org.apache.iceberg.rest.BaseHTTPClient.get(BaseHTTPClient.java:77)
  	at org.apache.iceberg.rest.RESTClient.get(RESTClient.java:97)
  	at org.apache.iceberg.rest.RESTSessionCatalog.loadInternal(RESTSessionCatalog.java:465)
  	at org.apache.iceberg.rest.RESTSessionCatalog.loadTable(RESTSessionCatalog.java:489)
  	at org.apache.iceberg.catalog.BaseSessionCatalog$AsCatalog.loadTable(BaseSessionCatalog.java:99)
  	at org.apache.iceberg.rest.RESTCatalog.loadTable(RESTCatalog.java:102)
  	at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)
  	at java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1916)
  	at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)
  	at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)
  	at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)
  	at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)
  	at org.apache.iceberg.CachingCatalog.loadTable(CachingCatalog.java:147)
  	at org.apache.iceberg.spark.SparkCatalog.load(SparkCatalog.java:844)
  	at org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:169)
  	at org.apache.spark.sql.connector.catalog.CatalogV2Util$.getTable(CatalogV2Util.scala:363)
  	at org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:337)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$2(Analyzer.scala:1228)
  	at scala.Option.orElse(Option.scala:447)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1223)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1182)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1117)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)
  	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)
  	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
  	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
  	at org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:634)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1117)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1076)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
  	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
  	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
  	at scala.collection.immutable.List.foldLeft(List.scala:91)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
  	at scala.collection.immutable.List.foreach(List.scala:431)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
  	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m10:20:18.294557 [debug] [ThreadPool]: Spark adapter: Error while running:
macro describe_table_extended_without_caching
[0m10:20:18.297019 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.iceberg.exceptions.ServiceFailureException: Server error: NotFoundException: Location does not exist: s3a://bronze/airbyte/metadata/00000-4dd569d6-08c2-4c7a-89fe-a8cd75b41268.metadata.json
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:46)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.iceberg.exceptions.ServiceFailureException: Server error: NotFoundException: Location does not exist: s3a://bronze/airbyte/metadata/00000-4dd569d6-08c2-4c7a-89fe-a8cd75b41268.metadata.json
    	at org.apache.iceberg.rest.ErrorHandlers$DefaultErrorHandler.accept(ErrorHandlers.java:217)
    	at org.apache.iceberg.rest.ErrorHandlers$TableErrorHandler.accept(ErrorHandlers.java:118)
    	at org.apache.iceberg.rest.ErrorHandlers$TableErrorHandler.accept(ErrorHandlers.java:102)
    	at org.apache.iceberg.rest.HTTPClient.throwFailure(HTTPClient.java:224)
    	at org.apache.iceberg.rest.HTTPClient.execute(HTTPClient.java:308)
    	at org.apache.iceberg.rest.BaseHTTPClient.get(BaseHTTPClient.java:77)
    	at org.apache.iceberg.rest.RESTClient.get(RESTClient.java:97)
    	at org.apache.iceberg.rest.RESTSessionCatalog.loadInternal(RESTSessionCatalog.java:465)
    	at org.apache.iceberg.rest.RESTSessionCatalog.loadTable(RESTSessionCatalog.java:489)
    	at org.apache.iceberg.catalog.BaseSessionCatalog$AsCatalog.loadTable(BaseSessionCatalog.java:99)
    	at org.apache.iceberg.rest.RESTCatalog.loadTable(RESTCatalog.java:102)
    	at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)
    	at java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1916)
    	at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)
    	at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)
    	at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)
    	at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)
    	at org.apache.iceberg.CachingCatalog.loadTable(CachingCatalog.java:147)
    	at org.apache.iceberg.spark.SparkCatalog.load(SparkCatalog.java:844)
    	at org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:169)
    	at org.apache.spark.sql.connector.catalog.CatalogV2Util$.getTable(CatalogV2Util.scala:363)
    	at org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:337)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$2(Analyzer.scala:1228)
    	at scala.Option.orElse(Option.scala:447)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1223)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1182)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1117)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)
    	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
    	at org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:634)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1117)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1076)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
    	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
    	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
    	at scala.collection.immutable.List.foldLeft(List.scala:91)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
    	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m10:20:18.299741 [debug] [ThreadPool]: Spark adapter: Error while retrieving information about default.airbyte_test: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.iceberg.exceptions.ServiceFailureException: Server error: NotFoundException: Location does not exist: s3a://bronze/airbyte/metadata/00000-4dd569d6-08c2-4c7a-89fe-a8cd75b41268.metadata.json
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:46)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.iceberg.exceptions.ServiceFailureException: Server error: NotFoundException: Location does not exist: s3a://bronze/airbyte/metadata/00000-4dd569d6-08c2-4c7a-89fe-a8cd75b41268.metadata.json
    	at org.apache.iceberg.rest.ErrorHandlers$DefaultErrorHandler.accept(ErrorHandlers.java:217)
    	at org.apache.iceberg.rest.ErrorHandlers$TableErrorHandler.accept(ErrorHandlers.java:118)
    	at org.apache.iceberg.rest.ErrorHandlers$TableErrorHandler.accept(ErrorHandlers.java:102)
    	at org.apache.iceberg.rest.HTTPClient.throwFailure(HTTPClient.java:224)
    	at org.apache.iceberg.rest.HTTPClient.execute(HTTPClient.java:308)
    	at org.apache.iceberg.rest.BaseHTTPClient.get(BaseHTTPClient.java:77)
    	at org.apache.iceberg.rest.RESTClient.get(RESTClient.java:97)
    	at org.apache.iceberg.rest.RESTSessionCatalog.loadInternal(RESTSessionCatalog.java:465)
    	at org.apache.iceberg.rest.RESTSessionCatalog.loadTable(RESTSessionCatalog.java:489)
    	at org.apache.iceberg.catalog.BaseSessionCatalog$AsCatalog.loadTable(BaseSessionCatalog.java:99)
    	at org.apache.iceberg.rest.RESTCatalog.loadTable(RESTCatalog.java:102)
    	at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)
    	at java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1916)
    	at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)
    	at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)
    	at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)
    	at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)
    	at org.apache.iceberg.CachingCatalog.loadTable(CachingCatalog.java:147)
    	at org.apache.iceberg.spark.SparkCatalog.load(SparkCatalog.java:844)
    	at org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:169)
    	at org.apache.spark.sql.connector.catalog.CatalogV2Util$.getTable(CatalogV2Util.scala:363)
    	at org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:337)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$2(Analyzer.scala:1228)
    	at scala.Option.orElse(Option.scala:447)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1223)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1182)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1117)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)
    	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
    	at org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:634)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1117)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1076)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
    	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
    	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
    	at scala.collection.immutable.List.foldLeft(List.scala:91)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
    	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m10:20:18.303227 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m10:20:18.305743 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default"} */
describe extended default.fct_events_enriched
  
[0m10:20:18.364601 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m10:20:18.366514 [debug] [ThreadPool]: SQL status: OK in 0.059 seconds
[0m10:20:18.373217 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m10:20:18.374862 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default"} */
describe extended default.stg_events
  
[0m10:20:18.424429 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m10:20:18.426314 [debug] [ThreadPool]: SQL status: OK in 0.050 seconds
[0m10:20:18.432833 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m10:20:18.434631 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default"} */
describe extended default.stg_users
  
[0m10:20:18.478555 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m10:20:18.480674 [debug] [ThreadPool]: SQL status: OK in 0.045 seconds
[0m10:20:18.485871 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m10:20:18.487824 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m10:20:18.489731 [debug] [ThreadPool]: On list_None_default: Close
[0m10:20:18.495402 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default, now list_None_default_default_gold)
[0m10:20:18.499878 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m10:20:18.501454 [debug] [ThreadPool]: Using spark connection "list_None_default_default_gold"
[0m10:20:18.503194 [debug] [ThreadPool]: On list_None_default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show table extended in default_default_gold like '*'
  
[0m10:20:18.504888 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:20:18.542481 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#208, tableName#209, isTemporary#210, information#211]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@5e46a39f, [default_default_gold]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#208, tableName#209, isTemporary#210, information#211]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@5e46a39f, [default_default_gold]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m10:20:18.544882 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m10:20:18.546684 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show table extended in default_default_gold like '*'
  
[0m10:20:18.548668 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#208, tableName#209, isTemporary#210, information#211]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@5e46a39f, [default_default_gold]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#208, tableName#209, isTemporary#210, information#211]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@5e46a39f, [default_default_gold]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m10:20:18.550775 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m10:20:18.553198 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#208, tableName#209, isTemporary#210, information#211]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@5e46a39f, [default_default_gold]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#208, tableName#209, isTemporary#210, information#211]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@5e46a39f, [default_default_gold]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m10:20:18.556591 [debug] [ThreadPool]: Using spark connection "list_None_default_default_gold"
[0m10:20:18.558972 [debug] [ThreadPool]: On list_None_default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show tables in default_default_gold like '*'
  
[0m10:20:18.587935 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m10:20:18.590164 [debug] [ThreadPool]: SQL status: OK in 0.029 seconds
[0m10:20:18.596323 [debug] [ThreadPool]: Using spark connection "list_None_default_default_gold"
[0m10:20:18.598698 [debug] [ThreadPool]: On list_None_default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
describe extended default_default_gold.fct_events_enriched
  
[0m10:20:18.655474 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m10:20:18.657739 [debug] [ThreadPool]: SQL status: OK in 0.056 seconds
[0m10:20:18.662695 [debug] [ThreadPool]: On list_None_default_default_gold: ROLLBACK
[0m10:20:18.664803 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m10:20:18.666572 [debug] [ThreadPool]: On list_None_default_default_gold: Close
[0m10:20:18.672940 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c8f9625e-8a37-4e16-b0b0-dc9cc299c6a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x768246ccd590>]}
[0m10:20:18.675009 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m10:20:18.677005 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m10:20:18.680294 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.dim_uemoa_indicators
[0m10:20:18.682406 [info ] [Thread-1 (]: 1 of 9 START sql table model default.dim_uemoa_indicators ...................... [RUN]
[0m10:20:18.684311 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default_default_gold, now model.data_pipeline_poc.dim_uemoa_indicators)
[0m10:20:18.686163 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_poc.dim_uemoa_indicators
[0m10:20:18.693129 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_poc.dim_uemoa_indicators"
[0m10:20:18.722387 [debug] [Thread-1 (]: Began executing node model.data_pipeline_poc.dim_uemoa_indicators
[0m10:20:18.736100 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.dim_uemoa_indicators"
[0m10:20:18.738125 [debug] [Thread-1 (]: On model.data_pipeline_poc.dim_uemoa_indicators: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.dim_uemoa_indicators"} */
drop table if exists default.dim_uemoa_indicators
[0m10:20:18.740102 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m10:20:18.791835 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m10:20:18.793932 [debug] [Thread-1 (]: SQL status: OK in 0.054 seconds
[0m10:20:18.818391 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_poc.dim_uemoa_indicators"
[0m10:20:18.855195 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m10:20:18.857244 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.dim_uemoa_indicators"
[0m10:20:18.859427 [debug] [Thread-1 (]: On model.data_pipeline_poc.dim_uemoa_indicators: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.dim_uemoa_indicators"} */

  
    
        create or replace table default.dim_uemoa_indicators
      
      
    using iceberg
      
      
      
      
      
      

      as
      -- Ce modèle sert de "source de vérité" propre pour tous les modèles Gold.
-- Il sélectionne depuis la source et applique les premières transformations/nettoyages.
SELECT
    date,
    pib_nominal_milliards_fcfa,
    poids_secteur_primaire_pct,
    poids_secteur_secondaire_pct,
    poids_secteur_tertiaire_pct,
    taux_croissance_reel_pib_pct,
    taux_inflation_moyen_annuel_ipc_pct,
    recettes_fiscales,
    recettes_fiscales_pct_pib,
    depenses_totales_et_prets_nets,
    solde_budgetaire_global_avec_dons,
    solde_budgetaire_global_hors_dons,
    encours_de_la_dette,
    encours_de_la_dette_pct_pib,
    exportations_biens_fob,
    importations_biens_fob,
    balance_des_biens,
    compte_transactions_courantes,
    balance_courante_sur_pib_pct,
    agregats_monnaie_masse_monetaire_m2,
    taux_couverture_emission_monetaire
    -- Ajoutez toutes les autres colonnes dont vous avez besoin
FROM
    bronze.indicateurs_economiques_uemoa
WHERE
    pib_nominal_milliards_fcfa IS NOT NULL
  
[0m10:20:18.979735 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='42P01', errorCode=0, errorMessage="org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze`.`indicateurs_economiques_uemoa` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 43 pos 4;\n'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false\n:- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@2470b0ff, default.dim_uemoa_indicators\n+- 'Project ['date, 'pib_nominal_milliards_fcfa, 'poids_secteur_primaire_pct, 'poids_secteur_secondaire_pct, 'poids_secteur_tertiaire_pct, 'taux_croissance_reel_pib_pct, 'taux_inflation_moyen_annuel_ipc_pct, 'recettes_fiscales, 'recettes_fiscales_pct_pib, 'depenses_totales_et_prets_nets, 'solde_budgetaire_global_avec_dons, 'solde_budgetaire_global_hors_dons, 'encours_de_la_dette, 'encours_de_la_dette_pct_pib, 'exportations_biens_fob, 'importations_biens_fob, 'balance_des_biens, 'compte_transactions_courantes, 'balance_courante_sur_pib_pct, 'agregats_monnaie_masse_monetaire_m2, 'taux_couverture_emission_monetaire]\n   +- 'Filter isnotnull('pib_nominal_milliards_fcfa)\n      +- 'UnresolvedRelation [bronze, indicateurs_economiques_uemoa], [], false\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze`.`indicateurs_economiques_uemoa` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 43 pos 4;\n'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false\n:- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@2470b0ff, default.dim_uemoa_indicators\n+- 'Project ['date, 'pib_nominal_milliards_fcfa, 'poids_secteur_primaire_pct, 'poids_secteur_secondaire_pct, 'poids_secteur_tertiaire_pct, 'taux_croissance_reel_pib_pct, 'taux_inflation_moyen_annuel_ipc_pct, 'recettes_fiscales, 'recettes_fiscales_pct_pib, 'depenses_totales_et_prets_nets, 'solde_budgetaire_global_avec_dons, 'solde_budgetaire_global_hors_dons, 'encours_de_la_dette, 'encours_de_la_dette_pct_pib, 'exportations_biens_fob, 'importations_biens_fob, 'balance_des_biens, 'compte_transactions_courantes, 'balance_courante_sur_pib_pct, 'agregats_monnaie_masse_monetaire_m2, 'taux_couverture_emission_monetaire]\n   +- 'Filter isnotnull('pib_nominal_milliards_fcfa)\n      +- 'UnresolvedRelation [bronze, indicateurs_economiques_uemoa], [], false\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:235)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n", taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m10:20:18.981814 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m10:20:18.983273 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.dim_uemoa_indicators"} */

  
    
        create or replace table default.dim_uemoa_indicators
      
      
    using iceberg
      
      
      
      
      
      

      as
      -- Ce modèle sert de "source de vérité" propre pour tous les modèles Gold.
-- Il sélectionne depuis la source et applique les premières transformations/nettoyages.
SELECT
    date,
    pib_nominal_milliards_fcfa,
    poids_secteur_primaire_pct,
    poids_secteur_secondaire_pct,
    poids_secteur_tertiaire_pct,
    taux_croissance_reel_pib_pct,
    taux_inflation_moyen_annuel_ipc_pct,
    recettes_fiscales,
    recettes_fiscales_pct_pib,
    depenses_totales_et_prets_nets,
    solde_budgetaire_global_avec_dons,
    solde_budgetaire_global_hors_dons,
    encours_de_la_dette,
    encours_de_la_dette_pct_pib,
    exportations_biens_fob,
    importations_biens_fob,
    balance_des_biens,
    compte_transactions_courantes,
    balance_courante_sur_pib_pct,
    agregats_monnaie_masse_monetaire_m2,
    taux_couverture_emission_monetaire
    -- Ajoutez toutes les autres colonnes dont vous avez besoin
FROM
    bronze.indicateurs_economiques_uemoa
WHERE
    pib_nominal_milliards_fcfa IS NOT NULL
  
[0m10:20:18.984536 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze`.`indicateurs_economiques_uemoa` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 43 pos 4;
  'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false
  :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@2470b0ff, default.dim_uemoa_indicators
  +- 'Project ['date, 'pib_nominal_milliards_fcfa, 'poids_secteur_primaire_pct, 'poids_secteur_secondaire_pct, 'poids_secteur_tertiaire_pct, 'taux_croissance_reel_pib_pct, 'taux_inflation_moyen_annuel_ipc_pct, 'recettes_fiscales, 'recettes_fiscales_pct_pib, 'depenses_totales_et_prets_nets, 'solde_budgetaire_global_avec_dons, 'solde_budgetaire_global_hors_dons, 'encours_de_la_dette, 'encours_de_la_dette_pct_pib, 'exportations_biens_fob, 'importations_biens_fob, 'balance_des_biens, 'compte_transactions_courantes, 'balance_courante_sur_pib_pct, 'agregats_monnaie_masse_monetaire_m2, 'taux_couverture_emission_monetaire]
     +- 'Filter isnotnull('pib_nominal_milliards_fcfa)
        +- 'UnresolvedRelation [bronze, indicateurs_economiques_uemoa], [], false
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze`.`indicateurs_economiques_uemoa` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 43 pos 4;
  'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false
  :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@2470b0ff, default.dim_uemoa_indicators
  +- 'Project ['date, 'pib_nominal_milliards_fcfa, 'poids_secteur_primaire_pct, 'poids_secteur_secondaire_pct, 'poids_secteur_tertiaire_pct, 'taux_croissance_reel_pib_pct, 'taux_inflation_moyen_annuel_ipc_pct, 'recettes_fiscales, 'recettes_fiscales_pct_pib, 'depenses_totales_et_prets_nets, 'solde_budgetaire_global_avec_dons, 'solde_budgetaire_global_hors_dons, 'encours_de_la_dette, 'encours_de_la_dette_pct_pib, 'exportations_biens_fob, 'importations_biens_fob, 'balance_des_biens, 'compte_transactions_courantes, 'balance_courante_sur_pib_pct, 'agregats_monnaie_masse_monetaire_m2, 'taux_couverture_emission_monetaire]
     +- 'Filter isnotnull('pib_nominal_milliards_fcfa)
        +- 'UnresolvedRelation [bronze, indicateurs_economiques_uemoa], [], false
  
  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:87)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:235)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
  	at scala.collection.immutable.List.foreach(List.scala:431)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m10:20:18.986189 [debug] [Thread-1 (]: On model.data_pipeline_poc.dim_uemoa_indicators: ROLLBACK
[0m10:20:18.987351 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m10:20:18.988694 [debug] [Thread-1 (]: On model.data_pipeline_poc.dim_uemoa_indicators: Close
[0m10:20:19.001956 [debug] [Thread-1 (]: Runtime Error in model dim_uemoa_indicators (models/silver/dim_uemoa_indicators.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze`.`indicateurs_economiques_uemoa` cannot be found. Verify the spelling and correctness of the schema and catalog.
    If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
    To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 43 pos 4;
    'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false
    :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@2470b0ff, default.dim_uemoa_indicators
    +- 'Project ['date, 'pib_nominal_milliards_fcfa, 'poids_secteur_primaire_pct, 'poids_secteur_secondaire_pct, 'poids_secteur_tertiaire_pct, 'taux_croissance_reel_pib_pct, 'taux_inflation_moyen_annuel_ipc_pct, 'recettes_fiscales, 'recettes_fiscales_pct_pib, 'depenses_totales_et_prets_nets, 'solde_budgetaire_global_avec_dons, 'solde_budgetaire_global_hors_dons, 'encours_de_la_dette, 'encours_de_la_dette_pct_pib, 'exportations_biens_fob, 'importations_biens_fob, 'balance_des_biens, 'compte_transactions_courantes, 'balance_courante_sur_pib_pct, 'agregats_monnaie_masse_monetaire_m2, 'taux_couverture_emission_monetaire]
       +- 'Filter isnotnull('pib_nominal_milliards_fcfa)
          +- 'UnresolvedRelation [bronze, indicateurs_economiques_uemoa], [], false
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze`.`indicateurs_economiques_uemoa` cannot be found. Verify the spelling and correctness of the schema and catalog.
    If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
    To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 43 pos 4;
    'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false
    :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@2470b0ff, default.dim_uemoa_indicators
    +- 'Project ['date, 'pib_nominal_milliards_fcfa, 'poids_secteur_primaire_pct, 'poids_secteur_secondaire_pct, 'poids_secteur_tertiaire_pct, 'taux_croissance_reel_pib_pct, 'taux_inflation_moyen_annuel_ipc_pct, 'recettes_fiscales, 'recettes_fiscales_pct_pib, 'depenses_totales_et_prets_nets, 'solde_budgetaire_global_avec_dons, 'solde_budgetaire_global_hors_dons, 'encours_de_la_dette, 'encours_de_la_dette_pct_pib, 'exportations_biens_fob, 'importations_biens_fob, 'balance_des_biens, 'compte_transactions_courantes, 'balance_courante_sur_pib_pct, 'agregats_monnaie_masse_monetaire_m2, 'taux_couverture_emission_monetaire]
       +- 'Filter isnotnull('pib_nominal_milliards_fcfa)
          +- 'UnresolvedRelation [bronze, indicateurs_economiques_uemoa], [], false
    
    	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:87)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:235)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m10:20:19.005109 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c8f9625e-8a37-4e16-b0b0-dc9cc299c6a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x768243761750>]}
[0m10:20:19.006721 [error] [Thread-1 (]: 1 of 9 ERROR creating sql table model default.dim_uemoa_indicators ............. [[31mERROR[0m in 0.32s]
[0m10:20:19.008777 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.dim_uemoa_indicators
[0m10:20:19.010403 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.stg_events
[0m10:20:19.011083 [debug] [Thread-4 (]: Marking all children of 'model.data_pipeline_poc.dim_uemoa_indicators' to be skipped because of status 'error'.  Reason: Runtime Error in model dim_uemoa_indicators (models/silver/dim_uemoa_indicators.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze`.`indicateurs_economiques_uemoa` cannot be found. Verify the spelling and correctness of the schema and catalog.
    If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
    To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 43 pos 4;
    'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false
    :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@2470b0ff, default.dim_uemoa_indicators
    +- 'Project ['date, 'pib_nominal_milliards_fcfa, 'poids_secteur_primaire_pct, 'poids_secteur_secondaire_pct, 'poids_secteur_tertiaire_pct, 'taux_croissance_reel_pib_pct, 'taux_inflation_moyen_annuel_ipc_pct, 'recettes_fiscales, 'recettes_fiscales_pct_pib, 'depenses_totales_et_prets_nets, 'solde_budgetaire_global_avec_dons, 'solde_budgetaire_global_hors_dons, 'encours_de_la_dette, 'encours_de_la_dette_pct_pib, 'exportations_biens_fob, 'importations_biens_fob, 'balance_des_biens, 'compte_transactions_courantes, 'balance_courante_sur_pib_pct, 'agregats_monnaie_masse_monetaire_m2, 'taux_couverture_emission_monetaire]
       +- 'Filter isnotnull('pib_nominal_milliards_fcfa)
          +- 'UnresolvedRelation [bronze, indicateurs_economiques_uemoa], [], false
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze`.`indicateurs_economiques_uemoa` cannot be found. Verify the spelling and correctness of the schema and catalog.
    If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
    To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 43 pos 4;
    'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false
    :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@2470b0ff, default.dim_uemoa_indicators
    +- 'Project ['date, 'pib_nominal_milliards_fcfa, 'poids_secteur_primaire_pct, 'poids_secteur_secondaire_pct, 'poids_secteur_tertiaire_pct, 'taux_croissance_reel_pib_pct, 'taux_inflation_moyen_annuel_ipc_pct, 'recettes_fiscales, 'recettes_fiscales_pct_pib, 'depenses_totales_et_prets_nets, 'solde_budgetaire_global_avec_dons, 'solde_budgetaire_global_hors_dons, 'encours_de_la_dette, 'encours_de_la_dette_pct_pib, 'exportations_biens_fob, 'importations_biens_fob, 'balance_des_biens, 'compte_transactions_courantes, 'balance_courante_sur_pib_pct, 'agregats_monnaie_masse_monetaire_m2, 'taux_couverture_emission_monetaire]
       +- 'Filter isnotnull('pib_nominal_milliards_fcfa)
          +- 'UnresolvedRelation [bronze, indicateurs_economiques_uemoa], [], false
    
    	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:87)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:235)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    .
[0m10:20:19.013157 [info ] [Thread-1 (]: 2 of 9 START sql table model default_default_silver.stg_events ................. [RUN]
[0m10:20:19.016014 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline_poc.dim_uemoa_indicators, now model.data_pipeline_poc.stg_events)
[0m10:20:19.017042 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_poc.stg_events
[0m10:20:19.020409 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_poc.stg_events"
[0m10:20:19.028929 [debug] [Thread-1 (]: Began executing node model.data_pipeline_poc.stg_events
[0m10:20:19.033455 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_poc.stg_events"
[0m10:20:19.044379 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m10:20:19.045333 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.stg_events"
[0m10:20:19.046034 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_events: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.stg_events"} */

  
    
        create or replace table default_default_silver.stg_events
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Staging model for raw events from bronze layer
-- This model cleans and standardizes the raw event data

SELECT
    event_id,
    event_type,
    user_id,
    event_timestamp,
    event_data,
    current_timestamp() as dbt_loaded_at
FROM bronze.raw_events
WHERE event_id IS NOT NULL
  
[0m10:20:19.046979 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m10:20:22.154746 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m10:20:22.157218 [debug] [Thread-1 (]: SQL status: OK in 3.110 seconds
[0m10:20:22.172297 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_events: ROLLBACK
[0m10:20:22.174639 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m10:20:22.176998 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_events: Close
[0m10:20:22.183195 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c8f9625e-8a37-4e16-b0b0-dc9cc299c6a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7682437d7950>]}
[0m10:20:22.185822 [info ] [Thread-1 (]: 2 of 9 OK created sql table model default_default_silver.stg_events ............ [[32mOK[0m in 3.17s]
[0m10:20:22.187776 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.stg_events
[0m10:20:22.189558 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.stg_users
[0m10:20:22.190970 [info ] [Thread-1 (]: 3 of 9 START sql table model default_default_silver.stg_users .................. [RUN]
[0m10:20:22.192248 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline_poc.stg_events, now model.data_pipeline_poc.stg_users)
[0m10:20:22.193379 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_poc.stg_users
[0m10:20:22.196586 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_poc.stg_users"
[0m10:20:22.216102 [debug] [Thread-1 (]: Began executing node model.data_pipeline_poc.stg_users
[0m10:20:22.220188 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_poc.stg_users"
[0m10:20:22.241155 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m10:20:22.243279 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.stg_users"
[0m10:20:22.245475 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_users: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.stg_users"} */

  
    
        create or replace table default_default_silver.stg_users
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Staging model for raw users from bronze layer
-- This model cleans and standardizes the raw user data

SELECT
    user_id,
    user_name,
    email as user_email,
    created_at,
    current_timestamp() as dbt_loaded_at
FROM bronze.raw_users
WHERE user_id IS NOT NULL
  
[0m10:20:22.248002 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m10:20:23.179556 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m10:20:23.182135 [debug] [Thread-1 (]: SQL status: OK in 0.934 seconds
[0m10:20:23.186469 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_users: ROLLBACK
[0m10:20:23.187919 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m10:20:23.189927 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_users: Close
[0m10:20:23.193947 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c8f9625e-8a37-4e16-b0b0-dc9cc299c6a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x768244bdac50>]}
[0m10:20:23.196269 [info ] [Thread-1 (]: 3 of 9 OK created sql table model default_default_silver.stg_users ............. [[32mOK[0m in 1.00s]
[0m10:20:23.198138 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.stg_users
[0m10:20:23.200830 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.gold_kpi_uemoa_growth_yoy
[0m10:20:23.202266 [info ] [Thread-1 (]: 4 of 9 SKIP relation default.gold_kpi_uemoa_growth_yoy ......................... [[33mSKIP[0m]
[0m10:20:23.203518 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.gold_kpi_uemoa_growth_yoy
[0m10:20:23.204753 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.gold_mart_uemoa_external_stability
[0m10:20:23.206266 [info ] [Thread-1 (]: 5 of 9 SKIP relation default.gold_mart_uemoa_external_stability ................ [[33mSKIP[0m]
[0m10:20:23.207870 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.gold_mart_uemoa_external_stability
[0m10:20:23.211240 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.gold_mart_uemoa_external_trade
[0m10:20:23.213063 [info ] [Thread-1 (]: 6 of 9 SKIP relation default.gold_mart_uemoa_external_trade .................... [[33mSKIP[0m]
[0m10:20:23.214467 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.gold_mart_uemoa_external_trade
[0m10:20:23.215772 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.gold_mart_uemoa_monetary_dashboard
[0m10:20:23.219006 [info ] [Thread-1 (]: 7 of 9 SKIP relation default.gold_mart_uemoa_monetary_dashboard ................ [[33mSKIP[0m]
[0m10:20:23.221112 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.gold_mart_uemoa_monetary_dashboard
[0m10:20:23.222966 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.gold_mart_uemoa_public_finance
[0m10:20:23.224795 [info ] [Thread-1 (]: 8 of 9 SKIP relation default.gold_mart_uemoa_public_finance .................... [[33mSKIP[0m]
[0m10:20:23.227950 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.gold_mart_uemoa_public_finance
[0m10:20:23.229598 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.fct_events_enriched
[0m10:20:23.230983 [info ] [Thread-1 (]: 9 of 9 START sql table model default_default_gold.fct_events_enriched .......... [RUN]
[0m10:20:23.232454 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline_poc.stg_users, now model.data_pipeline_poc.fct_events_enriched)
[0m10:20:23.233877 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_poc.fct_events_enriched
[0m10:20:23.237724 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_poc.fct_events_enriched"
[0m10:20:23.256679 [debug] [Thread-1 (]: Began executing node model.data_pipeline_poc.fct_events_enriched
[0m10:20:23.261353 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_poc.fct_events_enriched"
[0m10:20:23.287354 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m10:20:23.289392 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.fct_events_enriched"
[0m10:20:23.291600 [debug] [Thread-1 (]: On model.data_pipeline_poc.fct_events_enriched: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.fct_events_enriched"} */

  
    
        create or replace table default_default_gold.fct_events_enriched
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Mart model: Events enriched with user information
-- This model joins event data with user data for analytics

WITH events AS (
    SELECT * FROM default_default_silver.stg_events
),

users AS (
    SELECT * FROM default_default_silver.stg_users
),

enriched AS (
    SELECT
        e.event_id,
        e.event_type,
        e.event_timestamp,
        e.event_data,
        e.user_id,
        u.user_name,
        u.user_email,
        u.created_at as user_created_at,
        e.dbt_loaded_at
    FROM events e
    LEFT JOIN users u ON e.user_id = u.user_id
)

SELECT * FROM enriched
  
[0m10:20:23.294287 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m10:20:24.638337 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m10:20:24.640211 [debug] [Thread-1 (]: SQL status: OK in 1.346 seconds
[0m10:20:24.642520 [debug] [Thread-1 (]: On model.data_pipeline_poc.fct_events_enriched: ROLLBACK
[0m10:20:24.644356 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m10:20:24.646383 [debug] [Thread-1 (]: On model.data_pipeline_poc.fct_events_enriched: Close
[0m10:20:24.651895 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c8f9625e-8a37-4e16-b0b0-dc9cc299c6a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76824302bb90>]}
[0m10:20:24.653874 [info ] [Thread-1 (]: 9 of 9 OK created sql table model default_default_gold.fct_events_enriched ..... [[32mOK[0m in 1.42s]
[0m10:20:24.656219 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.fct_events_enriched
[0m10:20:24.658998 [debug] [MainThread]: On master: ROLLBACK
[0m10:20:24.660604 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:20:24.685624 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m10:20:24.687006 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m10:20:24.688015 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m10:20:24.688805 [debug] [MainThread]: On master: ROLLBACK
[0m10:20:24.689527 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m10:20:24.690268 [debug] [MainThread]: On master: Close
[0m10:20:24.693533 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:20:24.694517 [debug] [MainThread]: Connection 'model.data_pipeline_poc.fct_events_enriched' was properly closed.
[0m10:20:24.696089 [info ] [MainThread]: 
[0m10:20:24.696934 [info ] [MainThread]: Finished running 9 table models in 0 hours 0 minutes and 39.45 seconds (39.45s).
[0m10:20:24.699246 [debug] [MainThread]: Command end result
[0m10:20:24.769610 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/app/dbt/target/manifest.json
[0m10:20:24.781196 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/app/dbt/target/semantic_manifest.json
[0m10:20:24.799467 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/app/dbt/target/run_results.json
[0m10:20:24.801754 [info ] [MainThread]: 
[0m10:20:24.803781 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m10:20:24.805651 [info ] [MainThread]: 
[0m10:20:24.808199 [error] [MainThread]:   Runtime Error in model dim_uemoa_indicators (models/silver/dim_uemoa_indicators.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze`.`indicateurs_economiques_uemoa` cannot be found. Verify the spelling and correctness of the schema and catalog.
    If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
    To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 43 pos 4;
    'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false
    :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@2470b0ff, default.dim_uemoa_indicators
    +- 'Project ['date, 'pib_nominal_milliards_fcfa, 'poids_secteur_primaire_pct, 'poids_secteur_secondaire_pct, 'poids_secteur_tertiaire_pct, 'taux_croissance_reel_pib_pct, 'taux_inflation_moyen_annuel_ipc_pct, 'recettes_fiscales, 'recettes_fiscales_pct_pib, 'depenses_totales_et_prets_nets, 'solde_budgetaire_global_avec_dons, 'solde_budgetaire_global_hors_dons, 'encours_de_la_dette, 'encours_de_la_dette_pct_pib, 'exportations_biens_fob, 'importations_biens_fob, 'balance_des_biens, 'compte_transactions_courantes, 'balance_courante_sur_pib_pct, 'agregats_monnaie_masse_monetaire_m2, 'taux_couverture_emission_monetaire]
       +- 'Filter isnotnull('pib_nominal_milliards_fcfa)
          +- 'UnresolvedRelation [bronze, indicateurs_economiques_uemoa], [], false
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze`.`indicateurs_economiques_uemoa` cannot be found. Verify the spelling and correctness of the schema and catalog.
    If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
    To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 43 pos 4;
    'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false
    :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@2470b0ff, default.dim_uemoa_indicators
    +- 'Project ['date, 'pib_nominal_milliards_fcfa, 'poids_secteur_primaire_pct, 'poids_secteur_secondaire_pct, 'poids_secteur_tertiaire_pct, 'taux_croissance_reel_pib_pct, 'taux_inflation_moyen_annuel_ipc_pct, 'recettes_fiscales, 'recettes_fiscales_pct_pib, 'depenses_totales_et_prets_nets, 'solde_budgetaire_global_avec_dons, 'solde_budgetaire_global_hors_dons, 'encours_de_la_dette, 'encours_de_la_dette_pct_pib, 'exportations_biens_fob, 'importations_biens_fob, 'balance_des_biens, 'compte_transactions_courantes, 'balance_courante_sur_pib_pct, 'agregats_monnaie_masse_monetaire_m2, 'taux_couverture_emission_monetaire]
       +- 'Filter isnotnull('pib_nominal_milliards_fcfa)
          +- 'UnresolvedRelation [bronze, indicateurs_economiques_uemoa], [], false
    
    	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:87)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:235)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m10:20:24.810124 [info ] [MainThread]: 
[0m10:20:24.811308 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=1 SKIP=5 TOTAL=9
[0m10:20:24.815548 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 43.315445, "process_in_blocks": "4736", "process_kernel_time": 0.305828, "process_mem_max_rss": "112364", "process_out_blocks": "0", "process_user_time": 1.810825}
[0m10:20:24.817561 [debug] [MainThread]: Command `dbt run` failed at 10:20:24.817443 after 43.32 seconds
[0m10:20:24.818838 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7682437b3f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7682470dd490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7682470dcb50>]}
[0m10:20:24.820768 [debug] [MainThread]: Flushing usage events
[0m10:20:26.262691 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:52:21.138308 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ea78008bf10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ea78008be90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ea78008b950>]}


============================== 10:52:21.142824 | 687c8323-ce7f-4c41-9bfa-5e9e15ffc677 ==============================
[0m10:52:21.142824 [info ] [MainThread]: Running with dbt=1.9.0
[0m10:52:21.145164 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/usr/app/dbt/logs', 'version_check': 'True', 'profiles_dir': '/usr/app/dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt run', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m10:52:21.192010 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m10:52:21.195513 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m10:52:21.198636 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m10:52:21.290727 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '687c8323-ce7f-4c41-9bfa-5e9e15ffc677', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ea77f9509d0>]}
[0m10:52:21.325940 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '687c8323-ce7f-4c41-9bfa-5e9e15ffc677', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ea78221b950>]}
[0m10:52:21.328921 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m10:52:21.412338 [debug] [MainThread]: checksum: c99e828bba267739642b5a3ce85f17518764ea526e0e6c4fdc649171c1a66bff, vars: {}, profile: , target: , version: 1.9.0
[0m10:52:24.394296 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 6 files changed.
[0m10:52:24.396686 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_poc://models/gold/gold_mart_uemoa_public_finance.sql
[0m10:52:24.398661 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_poc://models/gold/gold_mart_uemoa_external_trade.sql
[0m10:52:24.400309 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_poc://models/gold/gold_mart_uemoa_monetary_dashboard.sql
[0m10:52:24.402214 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_poc://models/gold/gold_kpi_uemoa_growth_yoy.sql
[0m10:52:24.404386 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_poc://models/silver/dim_uemoa_indicators.sql
[0m10:52:24.406594 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_poc://models/gold/gold_mart_uemoa_external_stability.sql
[0m10:52:24.684135 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- seeds.data_pipeline_poc
- snapshots.data_pipeline_poc
[0m10:52:24.694370 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '687c8323-ce7f-4c41-9bfa-5e9e15ffc677', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ea77f469e90>]}
[0m10:52:24.837286 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/app/dbt/target/manifest.json
[0m10:52:24.849800 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/app/dbt/target/semantic_manifest.json
[0m10:52:24.878239 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '687c8323-ce7f-4c41-9bfa-5e9e15ffc677', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ea77c4c7550>]}
[0m10:52:24.880364 [info ] [MainThread]: Found 9 models, 13 data tests, 3 sources, 583 macros
[0m10:52:24.882120 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '687c8323-ce7f-4c41-9bfa-5e9e15ffc677', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ea78153a450>]}
[0m10:52:24.885538 [info ] [MainThread]: 
[0m10:52:24.887107 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m10:52:24.888321 [info ] [MainThread]: 
[0m10:52:24.889759 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m10:52:24.895459 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m10:52:24.903237 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m10:52:24.904909 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m10:52:24.906635 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:52:24.996364 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m10:52:24.998197 [debug] [ThreadPool]: SQL status: OK in 0.091 seconds
[0m10:52:25.005055 [debug] [ThreadPool]: On list_schemas: Close
[0m10:52:25.012367 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m10:52:25.013769 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m10:52:25.015068 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:52:25.062174 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m10:52:25.064568 [debug] [ThreadPool]: SQL status: OK in 0.049 seconds
[0m10:52:25.069036 [debug] [ThreadPool]: On list_schemas: Close
[0m10:52:25.075055 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m10:52:25.077287 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m10:52:25.079077 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:52:25.122691 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m10:52:25.125553 [debug] [ThreadPool]: SQL status: OK in 0.046 seconds
[0m10:52:25.129954 [debug] [ThreadPool]: On list_schemas: Close
[0m10:52:25.136335 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m10:52:25.138702 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m10:52:25.140730 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:52:25.186625 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m10:52:25.188992 [debug] [ThreadPool]: SQL status: OK in 0.048 seconds
[0m10:52:25.193459 [debug] [ThreadPool]: On list_schemas: Close
[0m10:52:25.199808 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default_gold)
[0m10:52:25.205693 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m10:52:25.209399 [debug] [ThreadPool]: Using spark connection "list_None_default_gold"
[0m10:52:25.212335 [debug] [ThreadPool]: On list_None_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_gold"} */
show table extended in default_gold like '*'
  
[0m10:52:25.215395 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:52:25.249168 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#410, tableName#411, isTemporary#412, information#413]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@45f2b594, [default_gold]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#410, tableName#411, isTemporary#412, information#413]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@45f2b594, [default_gold]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m10:52:25.251470 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m10:52:25.253038 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_gold"} */
show table extended in default_gold like '*'
  
[0m10:52:25.254715 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#410, tableName#411, isTemporary#412, information#413]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@45f2b594, [default_gold]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#410, tableName#411, isTemporary#412, information#413]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@45f2b594, [default_gold]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m10:52:25.256821 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m10:52:25.258570 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#410, tableName#411, isTemporary#412, information#413]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@45f2b594, [default_gold]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#410, tableName#411, isTemporary#412, information#413]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@45f2b594, [default_gold]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m10:52:25.263125 [debug] [ThreadPool]: Using spark connection "list_None_default_gold"
[0m10:52:25.265040 [debug] [ThreadPool]: On list_None_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_gold"} */
show tables in default_gold like '*'
  
[0m10:52:25.287304 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m10:52:25.289228 [debug] [ThreadPool]: SQL status: OK in 0.022 seconds
[0m10:52:25.293002 [debug] [ThreadPool]: On list_None_default_gold: ROLLBACK
[0m10:52:25.294647 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m10:52:25.296204 [debug] [ThreadPool]: On list_None_default_gold: Close
[0m10:52:25.300811 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default_gold, now list_None_default_default_gold)
[0m10:52:25.303617 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m10:52:25.306105 [debug] [ThreadPool]: Using spark connection "list_None_default_default_gold"
[0m10:52:25.308156 [debug] [ThreadPool]: On list_None_default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show table extended in default_default_gold like '*'
  
[0m10:52:25.310015 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:52:25.340829 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#425, tableName#426, isTemporary#427, information#428]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@569a7d95, [default_default_gold]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#425, tableName#426, isTemporary#427, information#428]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@569a7d95, [default_default_gold]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m10:52:25.342927 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m10:52:25.345029 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show table extended in default_default_gold like '*'
  
[0m10:52:25.346830 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#425, tableName#426, isTemporary#427, information#428]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@569a7d95, [default_default_gold]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#425, tableName#426, isTemporary#427, information#428]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@569a7d95, [default_default_gold]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m10:52:25.349025 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m10:52:25.350885 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#425, tableName#426, isTemporary#427, information#428]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@569a7d95, [default_default_gold]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#425, tableName#426, isTemporary#427, information#428]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@569a7d95, [default_default_gold]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m10:52:25.353432 [debug] [ThreadPool]: Using spark connection "list_None_default_default_gold"
[0m10:52:25.355032 [debug] [ThreadPool]: On list_None_default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show tables in default_default_gold like '*'
  
[0m10:52:25.382671 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m10:52:25.384401 [debug] [ThreadPool]: SQL status: OK in 0.027 seconds
[0m10:52:25.390854 [debug] [ThreadPool]: Using spark connection "list_None_default_default_gold"
[0m10:52:25.392643 [debug] [ThreadPool]: On list_None_default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
describe extended default_default_gold.fct_events_enriched
  
[0m10:52:25.459733 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m10:52:25.461978 [debug] [ThreadPool]: SQL status: OK in 0.068 seconds
[0m10:52:25.467727 [debug] [ThreadPool]: On list_None_default_default_gold: ROLLBACK
[0m10:52:25.469687 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m10:52:25.471442 [debug] [ThreadPool]: On list_None_default_default_gold: Close
[0m10:52:25.476078 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default_default_gold, now list_None_default_silver)
[0m10:52:25.479310 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m10:52:25.481213 [debug] [ThreadPool]: Using spark connection "list_None_default_silver"
[0m10:52:25.482668 [debug] [ThreadPool]: On list_None_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_silver"} */
show table extended in default_silver like '*'
  
[0m10:52:25.484581 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:52:25.514533 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#479, tableName#480, isTemporary#481, information#482]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@2cfb6895, [default_silver]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#479, tableName#480, isTemporary#481, information#482]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@2cfb6895, [default_silver]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m10:52:25.516865 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m10:52:25.519039 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_silver"} */
show table extended in default_silver like '*'
  
[0m10:52:25.521037 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#479, tableName#480, isTemporary#481, information#482]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@2cfb6895, [default_silver]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#479, tableName#480, isTemporary#481, information#482]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@2cfb6895, [default_silver]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m10:52:25.523436 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m10:52:25.525528 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#479, tableName#480, isTemporary#481, information#482]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@2cfb6895, [default_silver]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#479, tableName#480, isTemporary#481, information#482]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@2cfb6895, [default_silver]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m10:52:25.528798 [debug] [ThreadPool]: Using spark connection "list_None_default_silver"
[0m10:52:25.530180 [debug] [ThreadPool]: On list_None_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_silver"} */
show tables in default_silver like '*'
  
[0m10:52:25.548908 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m10:52:25.551189 [debug] [ThreadPool]: SQL status: OK in 0.020 seconds
[0m10:52:25.555502 [debug] [ThreadPool]: On list_None_default_silver: ROLLBACK
[0m10:52:25.557858 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m10:52:25.559779 [debug] [ThreadPool]: On list_None_default_silver: Close
[0m10:52:25.564719 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default_silver, now list_None_default_default_silver)
[0m10:52:25.573498 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m10:52:25.576061 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m10:52:25.578513 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show table extended in default_default_silver like '*'
  
[0m10:52:25.580327 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:52:25.609573 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#494, tableName#495, isTemporary#496, information#497]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7624264f, [default_default_silver]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#494, tableName#495, isTemporary#496, information#497]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7624264f, [default_default_silver]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m10:52:25.613379 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m10:52:25.616291 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show table extended in default_default_silver like '*'
  
[0m10:52:25.617955 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#494, tableName#495, isTemporary#496, information#497]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7624264f, [default_default_silver]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#494, tableName#495, isTemporary#496, information#497]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7624264f, [default_default_silver]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m10:52:25.620472 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m10:52:25.622860 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#494, tableName#495, isTemporary#496, information#497]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7624264f, [default_default_silver]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#494, tableName#495, isTemporary#496, information#497]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7624264f, [default_default_silver]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m10:52:25.626389 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m10:52:25.628587 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show tables in default_default_silver like '*'
  
[0m10:52:25.659367 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m10:52:25.666370 [debug] [ThreadPool]: SQL status: OK in 0.036 seconds
[0m10:52:25.672175 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m10:52:25.674064 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
describe extended default_default_silver.stg_events
  
[0m10:52:25.719665 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m10:52:25.721815 [debug] [ThreadPool]: SQL status: OK in 0.045 seconds
[0m10:52:25.727710 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m10:52:25.729319 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
describe extended default_default_silver.stg_users
  
[0m10:52:25.770116 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m10:52:25.772077 [debug] [ThreadPool]: SQL status: OK in 0.041 seconds
[0m10:52:25.776803 [debug] [ThreadPool]: On list_None_default_default_silver: ROLLBACK
[0m10:52:25.778989 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m10:52:25.780911 [debug] [ThreadPool]: On list_None_default_default_silver: Close
[0m10:52:25.786393 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '687c8323-ce7f-4c41-9bfa-5e9e15ffc677', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ea77c532c90>]}
[0m10:52:25.788054 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m10:52:25.789747 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m10:52:25.792262 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.dim_uemoa_indicators
[0m10:52:25.793734 [info ] [Thread-1 (]: 1 of 9 START sql table model default_silver.dim_uemoa_indicators ............... [RUN]
[0m10:52:25.795346 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default_default_silver, now model.data_pipeline_poc.dim_uemoa_indicators)
[0m10:52:25.797200 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_poc.dim_uemoa_indicators
[0m10:52:25.803947 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_poc.dim_uemoa_indicators"
[0m10:52:25.819447 [debug] [Thread-1 (]: Began executing node model.data_pipeline_poc.dim_uemoa_indicators
[0m10:52:25.833470 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.dim_uemoa_indicators"
[0m10:52:25.835830 [debug] [Thread-1 (]: On model.data_pipeline_poc.dim_uemoa_indicators: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.dim_uemoa_indicators"} */
drop table if exists default_silver.dim_uemoa_indicators
[0m10:52:25.837850 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m10:52:25.880503 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m10:52:25.882638 [debug] [Thread-1 (]: SQL status: OK in 0.045 seconds
[0m10:52:25.907503 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_poc.dim_uemoa_indicators"
[0m10:52:25.931237 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m10:52:25.932992 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.dim_uemoa_indicators"
[0m10:52:25.934880 [debug] [Thread-1 (]: On model.data_pipeline_poc.dim_uemoa_indicators: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.dim_uemoa_indicators"} */

  
    
        create or replace table default_silver.dim_uemoa_indicators
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Ce modèle sert de "source de vérité" propre pour tous les modèles Gold.
-- Il sélectionne depuis la source et applique les premières transformations/nettoyages.
SELECT
    date,
    pib_nominal_milliards_fcfa,
    poids_secteur_primaire_pct,
    poids_secteur_secondaire_pct,
    poids_secteur_tertiaire_pct,
    taux_croissance_reel_pib_pct,
    taux_inflation_moyen_annuel_ipc_pct,
    recettes_fiscales,
    recettes_fiscales_pct_pib,
    depenses_totales_et_prets_nets,
    solde_budgetaire_global_avec_dons,
    solde_budgetaire_global_hors_dons,
    encours_de_la_dette,
    encours_de_la_dette_pct_pib,
    exportations_biens_fob,
    importations_biens_fob,
    balance_des_biens,
    compte_transactions_courantes,
    balance_courante_sur_pib_pct,
    agregats_monnaie_masse_monetaire_m2,
    taux_couverture_emission_monetaire
    -- Ajoutez toutes les autres colonnes dont vous avez besoin
FROM
    bronze.indicateurs_economiques_uemoa
WHERE
    pib_nominal_milliards_fcfa IS NOT NULL
  
[0m10:52:25.955850 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='42P01', errorCode=0, errorMessage="org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze`.`indicateurs_economiques_uemoa` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 45 pos 4;\n'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false\n:- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@2b5238d5, default_silver.dim_uemoa_indicators\n+- 'Project ['date, 'pib_nominal_milliards_fcfa, 'poids_secteur_primaire_pct, 'poids_secteur_secondaire_pct, 'poids_secteur_tertiaire_pct, 'taux_croissance_reel_pib_pct, 'taux_inflation_moyen_annuel_ipc_pct, 'recettes_fiscales, 'recettes_fiscales_pct_pib, 'depenses_totales_et_prets_nets, 'solde_budgetaire_global_avec_dons, 'solde_budgetaire_global_hors_dons, 'encours_de_la_dette, 'encours_de_la_dette_pct_pib, 'exportations_biens_fob, 'importations_biens_fob, 'balance_des_biens, 'compte_transactions_courantes, 'balance_courante_sur_pib_pct, 'agregats_monnaie_masse_monetaire_m2, 'taux_couverture_emission_monetaire]\n   +- 'Filter isnotnull('pib_nominal_milliards_fcfa)\n      +- 'UnresolvedRelation [bronze, indicateurs_economiques_uemoa], [], false\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze`.`indicateurs_economiques_uemoa` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 45 pos 4;\n'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false\n:- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@2b5238d5, default_silver.dim_uemoa_indicators\n+- 'Project ['date, 'pib_nominal_milliards_fcfa, 'poids_secteur_primaire_pct, 'poids_secteur_secondaire_pct, 'poids_secteur_tertiaire_pct, 'taux_croissance_reel_pib_pct, 'taux_inflation_moyen_annuel_ipc_pct, 'recettes_fiscales, 'recettes_fiscales_pct_pib, 'depenses_totales_et_prets_nets, 'solde_budgetaire_global_avec_dons, 'solde_budgetaire_global_hors_dons, 'encours_de_la_dette, 'encours_de_la_dette_pct_pib, 'exportations_biens_fob, 'importations_biens_fob, 'balance_des_biens, 'compte_transactions_courantes, 'balance_courante_sur_pib_pct, 'agregats_monnaie_masse_monetaire_m2, 'taux_couverture_emission_monetaire]\n   +- 'Filter isnotnull('pib_nominal_milliards_fcfa)\n      +- 'UnresolvedRelation [bronze, indicateurs_economiques_uemoa], [], false\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:235)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n", taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m10:52:25.958441 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m10:52:25.960638 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.dim_uemoa_indicators"} */

  
    
        create or replace table default_silver.dim_uemoa_indicators
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Ce modèle sert de "source de vérité" propre pour tous les modèles Gold.
-- Il sélectionne depuis la source et applique les premières transformations/nettoyages.
SELECT
    date,
    pib_nominal_milliards_fcfa,
    poids_secteur_primaire_pct,
    poids_secteur_secondaire_pct,
    poids_secteur_tertiaire_pct,
    taux_croissance_reel_pib_pct,
    taux_inflation_moyen_annuel_ipc_pct,
    recettes_fiscales,
    recettes_fiscales_pct_pib,
    depenses_totales_et_prets_nets,
    solde_budgetaire_global_avec_dons,
    solde_budgetaire_global_hors_dons,
    encours_de_la_dette,
    encours_de_la_dette_pct_pib,
    exportations_biens_fob,
    importations_biens_fob,
    balance_des_biens,
    compte_transactions_courantes,
    balance_courante_sur_pib_pct,
    agregats_monnaie_masse_monetaire_m2,
    taux_couverture_emission_monetaire
    -- Ajoutez toutes les autres colonnes dont vous avez besoin
FROM
    bronze.indicateurs_economiques_uemoa
WHERE
    pib_nominal_milliards_fcfa IS NOT NULL
  
[0m10:52:25.963078 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze`.`indicateurs_economiques_uemoa` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 45 pos 4;
  'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false
  :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@2b5238d5, default_silver.dim_uemoa_indicators
  +- 'Project ['date, 'pib_nominal_milliards_fcfa, 'poids_secteur_primaire_pct, 'poids_secteur_secondaire_pct, 'poids_secteur_tertiaire_pct, 'taux_croissance_reel_pib_pct, 'taux_inflation_moyen_annuel_ipc_pct, 'recettes_fiscales, 'recettes_fiscales_pct_pib, 'depenses_totales_et_prets_nets, 'solde_budgetaire_global_avec_dons, 'solde_budgetaire_global_hors_dons, 'encours_de_la_dette, 'encours_de_la_dette_pct_pib, 'exportations_biens_fob, 'importations_biens_fob, 'balance_des_biens, 'compte_transactions_courantes, 'balance_courante_sur_pib_pct, 'agregats_monnaie_masse_monetaire_m2, 'taux_couverture_emission_monetaire]
     +- 'Filter isnotnull('pib_nominal_milliards_fcfa)
        +- 'UnresolvedRelation [bronze, indicateurs_economiques_uemoa], [], false
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze`.`indicateurs_economiques_uemoa` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 45 pos 4;
  'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false
  :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@2b5238d5, default_silver.dim_uemoa_indicators
  +- 'Project ['date, 'pib_nominal_milliards_fcfa, 'poids_secteur_primaire_pct, 'poids_secteur_secondaire_pct, 'poids_secteur_tertiaire_pct, 'taux_croissance_reel_pib_pct, 'taux_inflation_moyen_annuel_ipc_pct, 'recettes_fiscales, 'recettes_fiscales_pct_pib, 'depenses_totales_et_prets_nets, 'solde_budgetaire_global_avec_dons, 'solde_budgetaire_global_hors_dons, 'encours_de_la_dette, 'encours_de_la_dette_pct_pib, 'exportations_biens_fob, 'importations_biens_fob, 'balance_des_biens, 'compte_transactions_courantes, 'balance_courante_sur_pib_pct, 'agregats_monnaie_masse_monetaire_m2, 'taux_couverture_emission_monetaire]
     +- 'Filter isnotnull('pib_nominal_milliards_fcfa)
        +- 'UnresolvedRelation [bronze, indicateurs_economiques_uemoa], [], false
  
  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:87)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:235)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
  	at scala.collection.immutable.List.foreach(List.scala:431)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m10:52:25.965749 [debug] [Thread-1 (]: On model.data_pipeline_poc.dim_uemoa_indicators: ROLLBACK
[0m10:52:25.968397 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m10:52:25.970934 [debug] [Thread-1 (]: On model.data_pipeline_poc.dim_uemoa_indicators: Close
[0m10:52:25.979411 [debug] [Thread-1 (]: Runtime Error in model dim_uemoa_indicators (models/silver/dim_uemoa_indicators.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze`.`indicateurs_economiques_uemoa` cannot be found. Verify the spelling and correctness of the schema and catalog.
    If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
    To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 45 pos 4;
    'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false
    :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@2b5238d5, default_silver.dim_uemoa_indicators
    +- 'Project ['date, 'pib_nominal_milliards_fcfa, 'poids_secteur_primaire_pct, 'poids_secteur_secondaire_pct, 'poids_secteur_tertiaire_pct, 'taux_croissance_reel_pib_pct, 'taux_inflation_moyen_annuel_ipc_pct, 'recettes_fiscales, 'recettes_fiscales_pct_pib, 'depenses_totales_et_prets_nets, 'solde_budgetaire_global_avec_dons, 'solde_budgetaire_global_hors_dons, 'encours_de_la_dette, 'encours_de_la_dette_pct_pib, 'exportations_biens_fob, 'importations_biens_fob, 'balance_des_biens, 'compte_transactions_courantes, 'balance_courante_sur_pib_pct, 'agregats_monnaie_masse_monetaire_m2, 'taux_couverture_emission_monetaire]
       +- 'Filter isnotnull('pib_nominal_milliards_fcfa)
          +- 'UnresolvedRelation [bronze, indicateurs_economiques_uemoa], [], false
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze`.`indicateurs_economiques_uemoa` cannot be found. Verify the spelling and correctness of the schema and catalog.
    If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
    To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 45 pos 4;
    'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false
    :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@2b5238d5, default_silver.dim_uemoa_indicators
    +- 'Project ['date, 'pib_nominal_milliards_fcfa, 'poids_secteur_primaire_pct, 'poids_secteur_secondaire_pct, 'poids_secteur_tertiaire_pct, 'taux_croissance_reel_pib_pct, 'taux_inflation_moyen_annuel_ipc_pct, 'recettes_fiscales, 'recettes_fiscales_pct_pib, 'depenses_totales_et_prets_nets, 'solde_budgetaire_global_avec_dons, 'solde_budgetaire_global_hors_dons, 'encours_de_la_dette, 'encours_de_la_dette_pct_pib, 'exportations_biens_fob, 'importations_biens_fob, 'balance_des_biens, 'compte_transactions_courantes, 'balance_courante_sur_pib_pct, 'agregats_monnaie_masse_monetaire_m2, 'taux_couverture_emission_monetaire]
       +- 'Filter isnotnull('pib_nominal_milliards_fcfa)
          +- 'UnresolvedRelation [bronze, indicateurs_economiques_uemoa], [], false
    
    	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:87)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:235)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m10:52:25.984289 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '687c8323-ce7f-4c41-9bfa-5e9e15ffc677', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ea77f5cde90>]}
[0m10:52:25.988137 [error] [Thread-1 (]: 1 of 9 ERROR creating sql table model default_silver.dim_uemoa_indicators ...... [[31mERROR[0m in 0.19s]
[0m10:52:25.990147 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.dim_uemoa_indicators
[0m10:52:25.992106 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.stg_events
[0m10:52:25.992767 [debug] [Thread-4 (]: Marking all children of 'model.data_pipeline_poc.dim_uemoa_indicators' to be skipped because of status 'error'.  Reason: Runtime Error in model dim_uemoa_indicators (models/silver/dim_uemoa_indicators.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze`.`indicateurs_economiques_uemoa` cannot be found. Verify the spelling and correctness of the schema and catalog.
    If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
    To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 45 pos 4;
    'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false
    :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@2b5238d5, default_silver.dim_uemoa_indicators
    +- 'Project ['date, 'pib_nominal_milliards_fcfa, 'poids_secteur_primaire_pct, 'poids_secteur_secondaire_pct, 'poids_secteur_tertiaire_pct, 'taux_croissance_reel_pib_pct, 'taux_inflation_moyen_annuel_ipc_pct, 'recettes_fiscales, 'recettes_fiscales_pct_pib, 'depenses_totales_et_prets_nets, 'solde_budgetaire_global_avec_dons, 'solde_budgetaire_global_hors_dons, 'encours_de_la_dette, 'encours_de_la_dette_pct_pib, 'exportations_biens_fob, 'importations_biens_fob, 'balance_des_biens, 'compte_transactions_courantes, 'balance_courante_sur_pib_pct, 'agregats_monnaie_masse_monetaire_m2, 'taux_couverture_emission_monetaire]
       +- 'Filter isnotnull('pib_nominal_milliards_fcfa)
          +- 'UnresolvedRelation [bronze, indicateurs_economiques_uemoa], [], false
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze`.`indicateurs_economiques_uemoa` cannot be found. Verify the spelling and correctness of the schema and catalog.
    If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
    To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 45 pos 4;
    'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false
    :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@2b5238d5, default_silver.dim_uemoa_indicators
    +- 'Project ['date, 'pib_nominal_milliards_fcfa, 'poids_secteur_primaire_pct, 'poids_secteur_secondaire_pct, 'poids_secteur_tertiaire_pct, 'taux_croissance_reel_pib_pct, 'taux_inflation_moyen_annuel_ipc_pct, 'recettes_fiscales, 'recettes_fiscales_pct_pib, 'depenses_totales_et_prets_nets, 'solde_budgetaire_global_avec_dons, 'solde_budgetaire_global_hors_dons, 'encours_de_la_dette, 'encours_de_la_dette_pct_pib, 'exportations_biens_fob, 'importations_biens_fob, 'balance_des_biens, 'compte_transactions_courantes, 'balance_courante_sur_pib_pct, 'agregats_monnaie_masse_monetaire_m2, 'taux_couverture_emission_monetaire]
       +- 'Filter isnotnull('pib_nominal_milliards_fcfa)
          +- 'UnresolvedRelation [bronze, indicateurs_economiques_uemoa], [], false
    
    	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:87)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:235)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    .
[0m10:52:25.994319 [info ] [Thread-1 (]: 2 of 9 START sql table model default_default_silver.stg_events ................. [RUN]
[0m10:52:25.997589 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline_poc.dim_uemoa_indicators, now model.data_pipeline_poc.stg_events)
[0m10:52:25.999121 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_poc.stg_events
[0m10:52:26.003095 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_poc.stg_events"
[0m10:52:26.021533 [debug] [Thread-1 (]: Began executing node model.data_pipeline_poc.stg_events
[0m10:52:26.027009 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_poc.stg_events"
[0m10:52:26.051533 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m10:52:26.053705 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.stg_events"
[0m10:52:26.056005 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_events: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.stg_events"} */

  
    
        create or replace table default_default_silver.stg_events
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Staging model for raw events from bronze layer
-- This model cleans and standardizes the raw event data

SELECT
    event_id,
    event_type,
    user_id,
    event_timestamp,
    event_data,
    current_timestamp() as dbt_loaded_at
FROM bronze.raw_events
WHERE event_id IS NOT NULL
  
[0m10:52:26.058103 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m10:52:27.179128 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m10:52:27.181439 [debug] [Thread-1 (]: SQL status: OK in 1.125 seconds
[0m10:52:27.195159 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_events: ROLLBACK
[0m10:52:27.197621 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m10:52:27.200130 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_events: Close
[0m10:52:27.206242 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '687c8323-ce7f-4c41-9bfa-5e9e15ffc677', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ea77f407a90>]}
[0m10:52:27.209298 [info ] [Thread-1 (]: 2 of 9 OK created sql table model default_default_silver.stg_events ............ [[32mOK[0m in 1.21s]
[0m10:52:27.211881 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.stg_events
[0m10:52:27.213903 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.stg_users
[0m10:52:27.215959 [info ] [Thread-1 (]: 3 of 9 START sql table model default_default_silver.stg_users .................. [RUN]
[0m10:52:27.217529 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline_poc.stg_events, now model.data_pipeline_poc.stg_users)
[0m10:52:27.218698 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_poc.stg_users
[0m10:52:27.223086 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_poc.stg_users"
[0m10:52:27.237652 [debug] [Thread-1 (]: Began executing node model.data_pipeline_poc.stg_users
[0m10:52:27.241554 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_poc.stg_users"
[0m10:52:27.262056 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m10:52:27.263731 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.stg_users"
[0m10:52:27.265730 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_users: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.stg_users"} */

  
    
        create or replace table default_default_silver.stg_users
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Staging model for raw users from bronze layer
-- This model cleans and standardizes the raw user data

SELECT
    user_id,
    user_name,
    email as user_email,
    created_at,
    current_timestamp() as dbt_loaded_at
FROM bronze.raw_users
WHERE user_id IS NOT NULL
  
[0m10:52:27.267698 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m10:52:28.059399 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m10:52:28.061640 [debug] [Thread-1 (]: SQL status: OK in 0.794 seconds
[0m10:52:28.064613 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_users: ROLLBACK
[0m10:52:28.066523 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m10:52:28.068165 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_users: Close
[0m10:52:28.074452 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '687c8323-ce7f-4c41-9bfa-5e9e15ffc677', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ea77f8f0090>]}
[0m10:52:28.076907 [info ] [Thread-1 (]: 3 of 9 OK created sql table model default_default_silver.stg_users ............. [[32mOK[0m in 0.86s]
[0m10:52:28.078969 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.stg_users
[0m10:52:28.080652 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.gold_kpi_uemoa_growth_yoy
[0m10:52:28.082618 [info ] [Thread-1 (]: 4 of 9 SKIP relation default_gold.gold_kpi_uemoa_growth_yoy .................... [[33mSKIP[0m]
[0m10:52:28.084586 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.gold_kpi_uemoa_growth_yoy
[0m10:52:28.085866 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.gold_mart_uemoa_external_stability
[0m10:52:28.086756 [info ] [Thread-1 (]: 5 of 9 SKIP relation default_gold.gold_mart_uemoa_external_stability ........... [[33mSKIP[0m]
[0m10:52:28.088331 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.gold_mart_uemoa_external_stability
[0m10:52:28.089794 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.gold_mart_uemoa_external_trade
[0m10:52:28.091768 [info ] [Thread-1 (]: 6 of 9 SKIP relation default_gold.gold_mart_uemoa_external_trade ............... [[33mSKIP[0m]
[0m10:52:28.093484 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.gold_mart_uemoa_external_trade
[0m10:52:28.094668 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.gold_mart_uemoa_monetary_dashboard
[0m10:52:28.096318 [info ] [Thread-1 (]: 7 of 9 SKIP relation default_gold.gold_mart_uemoa_monetary_dashboard ........... [[33mSKIP[0m]
[0m10:52:28.097976 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.gold_mart_uemoa_monetary_dashboard
[0m10:52:28.099180 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.gold_mart_uemoa_public_finance
[0m10:52:28.100548 [info ] [Thread-1 (]: 8 of 9 SKIP relation default_gold.gold_mart_uemoa_public_finance ............... [[33mSKIP[0m]
[0m10:52:28.101657 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.gold_mart_uemoa_public_finance
[0m10:52:28.102924 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.fct_events_enriched
[0m10:52:28.104271 [info ] [Thread-1 (]: 9 of 9 START sql table model default_default_gold.fct_events_enriched .......... [RUN]
[0m10:52:28.105917 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline_poc.stg_users, now model.data_pipeline_poc.fct_events_enriched)
[0m10:52:28.107200 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_poc.fct_events_enriched
[0m10:52:28.110650 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_poc.fct_events_enriched"
[0m10:52:28.122432 [debug] [Thread-1 (]: Began executing node model.data_pipeline_poc.fct_events_enriched
[0m10:52:28.125902 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_poc.fct_events_enriched"
[0m10:52:28.139393 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m10:52:28.140979 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.fct_events_enriched"
[0m10:52:28.142622 [debug] [Thread-1 (]: On model.data_pipeline_poc.fct_events_enriched: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.fct_events_enriched"} */

  
    
        create or replace table default_default_gold.fct_events_enriched
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Mart model: Events enriched with user information
-- This model joins event data with user data for analytics

WITH events AS (
    SELECT * FROM default_default_silver.stg_events
),

users AS (
    SELECT * FROM default_default_silver.stg_users
),

enriched AS (
    SELECT
        e.event_id,
        e.event_type,
        e.event_timestamp,
        e.event_data,
        e.user_id,
        u.user_name,
        u.user_email,
        u.created_at as user_created_at,
        e.dbt_loaded_at
    FROM events e
    LEFT JOIN users u ON e.user_id = u.user_id
)

SELECT * FROM enriched
  
[0m10:52:28.144125 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m10:52:29.063071 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m10:52:29.064752 [debug] [Thread-1 (]: SQL status: OK in 0.921 seconds
[0m10:52:29.067186 [debug] [Thread-1 (]: On model.data_pipeline_poc.fct_events_enriched: ROLLBACK
[0m10:52:29.068387 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m10:52:29.069734 [debug] [Thread-1 (]: On model.data_pipeline_poc.fct_events_enriched: Close
[0m10:52:29.075251 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '687c8323-ce7f-4c41-9bfa-5e9e15ffc677', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ea77c4bbc90>]}
[0m10:52:29.076833 [info ] [Thread-1 (]: 9 of 9 OK created sql table model default_default_gold.fct_events_enriched ..... [[32mOK[0m in 0.97s]
[0m10:52:29.078311 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.fct_events_enriched
[0m10:52:29.080888 [debug] [MainThread]: On master: ROLLBACK
[0m10:52:29.082545 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:52:29.110850 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m10:52:29.112563 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m10:52:29.114015 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m10:52:29.115002 [debug] [MainThread]: On master: ROLLBACK
[0m10:52:29.117029 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m10:52:29.118017 [debug] [MainThread]: On master: Close
[0m10:52:29.121379 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:52:29.123904 [debug] [MainThread]: Connection 'model.data_pipeline_poc.fct_events_enriched' was properly closed.
[0m10:52:29.125539 [info ] [MainThread]: 
[0m10:52:29.126709 [info ] [MainThread]: Finished running 9 table models in 0 hours 0 minutes and 4.24 seconds (4.24s).
[0m10:52:29.129426 [debug] [MainThread]: Command end result
[0m10:52:29.182884 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/app/dbt/target/manifest.json
[0m10:52:29.188328 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/app/dbt/target/semantic_manifest.json
[0m10:52:29.198618 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/app/dbt/target/run_results.json
[0m10:52:29.200499 [info ] [MainThread]: 
[0m10:52:29.202148 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m10:52:29.203370 [info ] [MainThread]: 
[0m10:52:29.206774 [error] [MainThread]:   Runtime Error in model dim_uemoa_indicators (models/silver/dim_uemoa_indicators.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze`.`indicateurs_economiques_uemoa` cannot be found. Verify the spelling and correctness of the schema and catalog.
    If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
    To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 45 pos 4;
    'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false
    :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@2b5238d5, default_silver.dim_uemoa_indicators
    +- 'Project ['date, 'pib_nominal_milliards_fcfa, 'poids_secteur_primaire_pct, 'poids_secteur_secondaire_pct, 'poids_secteur_tertiaire_pct, 'taux_croissance_reel_pib_pct, 'taux_inflation_moyen_annuel_ipc_pct, 'recettes_fiscales, 'recettes_fiscales_pct_pib, 'depenses_totales_et_prets_nets, 'solde_budgetaire_global_avec_dons, 'solde_budgetaire_global_hors_dons, 'encours_de_la_dette, 'encours_de_la_dette_pct_pib, 'exportations_biens_fob, 'importations_biens_fob, 'balance_des_biens, 'compte_transactions_courantes, 'balance_courante_sur_pib_pct, 'agregats_monnaie_masse_monetaire_m2, 'taux_couverture_emission_monetaire]
       +- 'Filter isnotnull('pib_nominal_milliards_fcfa)
          +- 'UnresolvedRelation [bronze, indicateurs_economiques_uemoa], [], false
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze`.`indicateurs_economiques_uemoa` cannot be found. Verify the spelling and correctness of the schema and catalog.
    If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
    To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 45 pos 4;
    'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false
    :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@2b5238d5, default_silver.dim_uemoa_indicators
    +- 'Project ['date, 'pib_nominal_milliards_fcfa, 'poids_secteur_primaire_pct, 'poids_secteur_secondaire_pct, 'poids_secteur_tertiaire_pct, 'taux_croissance_reel_pib_pct, 'taux_inflation_moyen_annuel_ipc_pct, 'recettes_fiscales, 'recettes_fiscales_pct_pib, 'depenses_totales_et_prets_nets, 'solde_budgetaire_global_avec_dons, 'solde_budgetaire_global_hors_dons, 'encours_de_la_dette, 'encours_de_la_dette_pct_pib, 'exportations_biens_fob, 'importations_biens_fob, 'balance_des_biens, 'compte_transactions_courantes, 'balance_courante_sur_pib_pct, 'agregats_monnaie_masse_monetaire_m2, 'taux_couverture_emission_monetaire]
       +- 'Filter isnotnull('pib_nominal_milliards_fcfa)
          +- 'UnresolvedRelation [bronze, indicateurs_economiques_uemoa], [], false
    
    	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:87)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:235)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m10:52:29.208499 [info ] [MainThread]: 
[0m10:52:29.209833 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=1 SKIP=5 TOTAL=9
[0m10:52:29.212191 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 8.1641445, "process_in_blocks": "0", "process_kernel_time": 0.193888, "process_mem_max_rss": "110976", "process_out_blocks": "0", "process_user_time": 1.639971}
[0m10:52:29.215050 [debug] [MainThread]: Command `dbt run` failed at 10:52:29.214837 after 8.17 seconds
[0m10:52:29.217912 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ea7839b12d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ea7839b1150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ea7839b13d0>]}
[0m10:52:29.219619 [debug] [MainThread]: Flushing usage events
[0m10:52:31.636735 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:26:55.565208 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e262f912810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e262fd0b590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e262f9136d0>]}


============================== 11:26:55.569188 | 08c40faa-6fea-423e-bb73-54fa1d382bcd ==============================
[0m11:26:55.569188 [info ] [MainThread]: Running with dbt=1.9.0
[0m11:26:55.571401 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/app/dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/usr/app/dbt/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select dim_uemoa_indicators', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m11:26:55.624290 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:26:55.628273 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:26:55.630922 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:26:55.726247 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '08c40faa-6fea-423e-bb73-54fa1d382bcd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e262f2f8310>]}
[0m11:26:55.762385 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '08c40faa-6fea-423e-bb73-54fa1d382bcd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e2630733e50>]}
[0m11:26:55.765820 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m11:26:55.857314 [debug] [MainThread]: checksum: c99e828bba267739642b5a3ce85f17518764ea526e0e6c4fdc649171c1a66bff, vars: {}, profile: , target: , version: 1.9.0
[0m11:26:59.078034 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m11:26:59.080497 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_poc://models/silver/dim_uemoa_indicators.sql
[0m11:26:59.387715 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- seeds.data_pipeline_poc
- snapshots.data_pipeline_poc
[0m11:26:59.398330 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '08c40faa-6fea-423e-bb73-54fa1d382bcd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e262ecb7510>]}
[0m11:26:59.559416 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/app/dbt/target/manifest.json
[0m11:26:59.573074 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/app/dbt/target/semantic_manifest.json
[0m11:26:59.601262 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '08c40faa-6fea-423e-bb73-54fa1d382bcd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e25dfa97c90>]}
[0m11:26:59.603096 [info ] [MainThread]: Found 9 models, 13 data tests, 3 sources, 583 macros
[0m11:26:59.605026 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '08c40faa-6fea-423e-bb73-54fa1d382bcd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e25dfa781d0>]}
[0m11:26:59.608440 [info ] [MainThread]: 
[0m11:26:59.610371 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:26:59.612463 [info ] [MainThread]: 
[0m11:26:59.614374 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m11:26:59.617148 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m11:26:59.625594 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m11:26:59.627267 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m11:26:59.628883 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:26:59.699780 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:26:59.701944 [debug] [ThreadPool]: SQL status: OK in 0.073 seconds
[0m11:26:59.707798 [debug] [ThreadPool]: On list_schemas: Close
[0m11:26:59.718430 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default_silver)
[0m11:26:59.726140 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:26:59.728244 [debug] [ThreadPool]: Using spark connection "list_None_default_silver"
[0m11:26:59.731188 [debug] [ThreadPool]: On list_None_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_silver"} */
show table extended in default_silver like '*'
  
[0m11:26:59.733293 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:26:59.769389 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#691, tableName#692, isTemporary#693, information#694]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@38f5a4c3, [default_silver]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#691, tableName#692, isTemporary#693, information#694]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@38f5a4c3, [default_silver]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m11:26:59.771730 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m11:26:59.773954 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_silver"} */
show table extended in default_silver like '*'
  
[0m11:26:59.775952 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#691, tableName#692, isTemporary#693, information#694]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@38f5a4c3, [default_silver]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#691, tableName#692, isTemporary#693, information#694]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@38f5a4c3, [default_silver]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m11:26:59.778035 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m11:26:59.780285 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#691, tableName#692, isTemporary#693, information#694]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@38f5a4c3, [default_silver]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#691, tableName#692, isTemporary#693, information#694]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@38f5a4c3, [default_silver]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m11:26:59.786015 [debug] [ThreadPool]: Using spark connection "list_None_default_silver"
[0m11:26:59.788122 [debug] [ThreadPool]: On list_None_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_silver"} */
show tables in default_silver like '*'
  
[0m11:26:59.810899 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:26:59.813649 [debug] [ThreadPool]: SQL status: OK in 0.023 seconds
[0m11:26:59.818788 [debug] [ThreadPool]: On list_None_default_silver: ROLLBACK
[0m11:26:59.820646 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m11:26:59.823168 [debug] [ThreadPool]: On list_None_default_silver: Close
[0m11:26:59.829623 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default_silver, now list_None_default_default_gold)
[0m11:26:59.832709 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:26:59.834859 [debug] [ThreadPool]: Using spark connection "list_None_default_default_gold"
[0m11:26:59.836562 [debug] [ThreadPool]: On list_None_default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show table extended in default_default_gold like '*'
  
[0m11:26:59.838139 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:26:59.877977 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#706, tableName#707, isTemporary#708, information#709]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@2ee408e5, [default_default_gold]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#706, tableName#707, isTemporary#708, information#709]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@2ee408e5, [default_default_gold]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m11:26:59.879974 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m11:26:59.881624 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show table extended in default_default_gold like '*'
  
[0m11:26:59.883601 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#706, tableName#707, isTemporary#708, information#709]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@2ee408e5, [default_default_gold]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#706, tableName#707, isTemporary#708, information#709]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@2ee408e5, [default_default_gold]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m11:26:59.885023 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m11:26:59.886435 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#706, tableName#707, isTemporary#708, information#709]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@2ee408e5, [default_default_gold]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#706, tableName#707, isTemporary#708, information#709]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@2ee408e5, [default_default_gold]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m11:26:59.889121 [debug] [ThreadPool]: Using spark connection "list_None_default_default_gold"
[0m11:26:59.890761 [debug] [ThreadPool]: On list_None_default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show tables in default_default_gold like '*'
  
[0m11:26:59.925268 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:26:59.926720 [debug] [ThreadPool]: SQL status: OK in 0.034 seconds
[0m11:26:59.933671 [debug] [ThreadPool]: Using spark connection "list_None_default_default_gold"
[0m11:26:59.935293 [debug] [ThreadPool]: On list_None_default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
describe extended default_default_gold.fct_events_enriched
  
[0m11:27:00.013363 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:27:00.014901 [debug] [ThreadPool]: SQL status: OK in 0.078 seconds
[0m11:27:00.019904 [debug] [ThreadPool]: On list_None_default_default_gold: ROLLBACK
[0m11:27:00.021397 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m11:27:00.023056 [debug] [ThreadPool]: On list_None_default_default_gold: Close
[0m11:27:00.029060 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default_default_gold, now list_None_default_default_silver)
[0m11:27:00.032964 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:27:00.034303 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m11:27:00.035674 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show table extended in default_default_silver like '*'
  
[0m11:27:00.037103 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:27:00.073880 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#760, tableName#761, isTemporary#762, information#763]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@3e7bb105, [default_default_silver]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#760, tableName#761, isTemporary#762, information#763]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@3e7bb105, [default_default_silver]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m11:27:00.075412 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m11:27:00.077039 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show table extended in default_default_silver like '*'
  
[0m11:27:00.078807 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#760, tableName#761, isTemporary#762, information#763]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@3e7bb105, [default_default_silver]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#760, tableName#761, isTemporary#762, information#763]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@3e7bb105, [default_default_silver]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m11:27:00.080674 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m11:27:00.082633 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#760, tableName#761, isTemporary#762, information#763]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@3e7bb105, [default_default_silver]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#760, tableName#761, isTemporary#762, information#763]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@3e7bb105, [default_default_silver]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m11:27:00.085913 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m11:27:00.087693 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show tables in default_default_silver like '*'
  
[0m11:27:00.117078 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:27:00.119219 [debug] [ThreadPool]: SQL status: OK in 0.029 seconds
[0m11:27:00.125081 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m11:27:00.126865 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
describe extended default_default_silver.stg_events
  
[0m11:27:00.176876 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:27:00.178573 [debug] [ThreadPool]: SQL status: OK in 0.050 seconds
[0m11:27:00.184394 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m11:27:00.186003 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
describe extended default_default_silver.stg_users
  
[0m11:27:00.235180 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:27:00.237694 [debug] [ThreadPool]: SQL status: OK in 0.050 seconds
[0m11:27:00.243227 [debug] [ThreadPool]: On list_None_default_default_silver: ROLLBACK
[0m11:27:00.245285 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m11:27:00.247040 [debug] [ThreadPool]: On list_None_default_default_silver: Close
[0m11:27:00.252691 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default_default_silver, now list_None_default_gold)
[0m11:27:00.255984 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:27:00.257951 [debug] [ThreadPool]: Using spark connection "list_None_default_gold"
[0m11:27:00.259831 [debug] [ThreadPool]: On list_None_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_gold"} */
show table extended in default_gold like '*'
  
[0m11:27:00.261606 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:27:00.298308 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#836, tableName#837, isTemporary#838, information#839]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@126d19ec, [default_gold]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#836, tableName#837, isTemporary#838, information#839]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@126d19ec, [default_gold]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m11:27:00.301363 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m11:27:00.303734 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_gold"} */
show table extended in default_gold like '*'
  
[0m11:27:00.306441 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#836, tableName#837, isTemporary#838, information#839]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@126d19ec, [default_gold]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#836, tableName#837, isTemporary#838, information#839]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@126d19ec, [default_gold]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m11:27:00.309236 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m11:27:00.311399 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#836, tableName#837, isTemporary#838, information#839]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@126d19ec, [default_gold]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#836, tableName#837, isTemporary#838, information#839]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@126d19ec, [default_gold]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m11:27:00.315138 [debug] [ThreadPool]: Using spark connection "list_None_default_gold"
[0m11:27:00.317492 [debug] [ThreadPool]: On list_None_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_gold"} */
show tables in default_gold like '*'
  
[0m11:27:00.336898 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:27:00.339529 [debug] [ThreadPool]: SQL status: OK in 0.020 seconds
[0m11:27:00.343975 [debug] [ThreadPool]: On list_None_default_gold: ROLLBACK
[0m11:27:00.347036 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m11:27:00.349030 [debug] [ThreadPool]: On list_None_default_gold: Close
[0m11:27:00.355600 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '08c40faa-6fea-423e-bb73-54fa1d382bcd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e262eda3290>]}
[0m11:27:00.357570 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:27:00.359653 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:27:00.363038 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.dim_uemoa_indicators
[0m11:27:00.366121 [info ] [Thread-1 (]: 1 of 1 START sql table model default_silver.dim_uemoa_indicators ............... [RUN]
[0m11:27:00.368369 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default_gold, now model.data_pipeline_poc.dim_uemoa_indicators)
[0m11:27:00.370408 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_poc.dim_uemoa_indicators
[0m11:27:00.380106 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_poc.dim_uemoa_indicators"
[0m11:27:00.399585 [debug] [Thread-1 (]: Began executing node model.data_pipeline_poc.dim_uemoa_indicators
[0m11:27:00.414599 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.dim_uemoa_indicators"
[0m11:27:00.416710 [debug] [Thread-1 (]: On model.data_pipeline_poc.dim_uemoa_indicators: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.dim_uemoa_indicators"} */
drop table if exists default_silver.dim_uemoa_indicators
[0m11:27:00.418807 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:27:00.471821 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:27:00.473977 [debug] [Thread-1 (]: SQL status: OK in 0.055 seconds
[0m11:27:00.501093 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_poc.dim_uemoa_indicators"
[0m11:27:00.528850 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:27:00.531526 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.dim_uemoa_indicators"
[0m11:27:00.534771 [debug] [Thread-1 (]: On model.data_pipeline_poc.dim_uemoa_indicators: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.dim_uemoa_indicators"} */

  
    
        create or replace table default_silver.dim_uemoa_indicators
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Ce modèle lit directement les fichiers Parquet d'Airbyte et crée une table Iceberg
-- Les données proviennent de: s3a://lakehouse/bronze/indicateurs_economiques_uemoa/
WITH airbyte_data AS (
    SELECT *
    FROM parquet.`s3a://lakehouse/bronze/indicateurs_economiques_uemoa/*.parquet`
)

SELECT
    date,
    pib_nominal_milliards_fcfa,
    poids_secteur_primaire_pct,
    poids_secteur_secondaire_pct,
    poids_secteur_tertiaire_pct,
    taux_croissance_reel_pib_pct,
    taux_inflation_moyen_annuel_ipc_pct,
    recettes_fiscales,
    recettes_fiscales_pct_pib,
    depenses_totales_et_prets_nets,
    solde_budgetaire_global_avec_dons,
    solde_budgetaire_global_hors_dons,
    encours_de_la_dette,
    encours_de_la_dette_pct_pib,
    exportations_biens_fob,
    importations_biens_fob,
    balance_des_biens,
    compte_transactions_courantes,
    balance_courante_sur_pib_pct,
    agregats_monnaie_masse_monetaire_m2,
    taux_couverture_emission_monetaire
FROM airbyte_data
WHERE pib_nominal_milliards_fcfa IS NOT NULL
  
[0m11:27:00.568958 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] org.apache.spark.sql.AnalysisException: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: parquet; line 23 pos 9\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: parquet; line 23 pos 9\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:66)\n\tat org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:87)\n\tat org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:63)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:1676)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Filter.mapChildren(basicLogicalOperators.scala:316)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)\n\tat scala.collection.immutable.List.map(List.scala:297)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:699)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:63)\n\tat org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:43)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\nCaused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)\n\tat org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:79)\n\t... 115 more\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n\t... 128 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m11:27:00.572934 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m11:27:00.575821 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.dim_uemoa_indicators"} */

  
    
        create or replace table default_silver.dim_uemoa_indicators
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Ce modèle lit directement les fichiers Parquet d'Airbyte et crée une table Iceberg
-- Les données proviennent de: s3a://lakehouse/bronze/indicateurs_economiques_uemoa/
WITH airbyte_data AS (
    SELECT *
    FROM parquet.`s3a://lakehouse/bronze/indicateurs_economiques_uemoa/*.parquet`
)

SELECT
    date,
    pib_nominal_milliards_fcfa,
    poids_secteur_primaire_pct,
    poids_secteur_secondaire_pct,
    poids_secteur_tertiaire_pct,
    taux_croissance_reel_pib_pct,
    taux_inflation_moyen_annuel_ipc_pct,
    recettes_fiscales,
    recettes_fiscales_pct_pib,
    depenses_totales_et_prets_nets,
    solde_budgetaire_global_avec_dons,
    solde_budgetaire_global_hors_dons,
    encours_de_la_dette,
    encours_de_la_dette_pct_pib,
    exportations_biens_fob,
    importations_biens_fob,
    balance_des_biens,
    compte_transactions_courantes,
    balance_courante_sur_pib_pct,
    agregats_monnaie_masse_monetaire_m2,
    taux_couverture_emission_monetaire
FROM airbyte_data
WHERE pib_nominal_milliards_fcfa IS NOT NULL
  
[0m11:27:00.579089 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] org.apache.spark.sql.AnalysisException: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: parquet; line 23 pos 9
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: parquet; line 23 pos 9
  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:66)
  	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:87)
  	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:63)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
  	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
  	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
  	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
  	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
  	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
  	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
  	at org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:1676)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
  	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
  	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
  	at org.apache.spark.sql.catalyst.plans.logical.Filter.mapChildren(basicLogicalOperators.scala:316)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
  	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
  	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
  	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
  	at scala.collection.immutable.List.map(List.scala:297)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:699)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
  	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:63)
  	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:43)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
  	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
  	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
  	at scala.collection.immutable.List.foldLeft(List.scala:91)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
  	at scala.collection.immutable.List.foreach(List.scala:431)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
  	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
  	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)
  	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)
  	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
  	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
  	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
  	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
  	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
  	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
  	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)
  	at scala.collection.immutable.List.map(List.scala:293)
  	at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)
  	at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)
  	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)
  	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:79)
  	... 115 more
  Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
  	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)
  	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)
  	... 128 more
  
[0m11:27:00.582602 [debug] [Thread-1 (]: On model.data_pipeline_poc.dim_uemoa_indicators: ROLLBACK
[0m11:27:00.585195 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m11:27:00.588152 [debug] [Thread-1 (]: On model.data_pipeline_poc.dim_uemoa_indicators: Close
[0m11:27:00.598202 [debug] [Thread-1 (]: Runtime Error in model dim_uemoa_indicators (models/silver/dim_uemoa_indicators.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] org.apache.spark.sql.AnalysisException: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: parquet; line 23 pos 9
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: parquet; line 23 pos 9
    	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:66)
    	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:87)
    	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:63)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
    	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
    	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
    	at org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:1676)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
    	at org.apache.spark.sql.catalyst.plans.logical.Filter.mapChildren(basicLogicalOperators.scala:316)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
    	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
    	at scala.collection.immutable.List.map(List.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:699)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
    	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:63)
    	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:43)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
    	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
    	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
    	at scala.collection.immutable.List.foldLeft(List.scala:91)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
    	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
    	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)
    	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)
    	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
    	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
    	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
    	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
    	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
    	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
    	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)
    	at scala.collection.immutable.List.map(List.scala:293)
    	at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)
    	at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)
    	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)
    	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:79)
    	... 115 more
    Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
    	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)
    	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)
    	... 128 more
    
[0m11:27:00.602272 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '08c40faa-6fea-423e-bb73-54fa1d382bcd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e262ee8bd50>]}
[0m11:27:00.605281 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model default_silver.dim_uemoa_indicators ...... [[31mERROR[0m in 0.23s]
[0m11:27:00.608547 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.dim_uemoa_indicators
[0m11:27:00.611356 [debug] [Thread-4 (]: Marking all children of 'model.data_pipeline_poc.dim_uemoa_indicators' to be skipped because of status 'error'.  Reason: Runtime Error in model dim_uemoa_indicators (models/silver/dim_uemoa_indicators.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] org.apache.spark.sql.AnalysisException: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: parquet; line 23 pos 9
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: parquet; line 23 pos 9
    	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:66)
    	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:87)
    	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:63)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
    	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
    	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
    	at org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:1676)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
    	at org.apache.spark.sql.catalyst.plans.logical.Filter.mapChildren(basicLogicalOperators.scala:316)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
    	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
    	at scala.collection.immutable.List.map(List.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:699)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
    	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:63)
    	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:43)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
    	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
    	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
    	at scala.collection.immutable.List.foldLeft(List.scala:91)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
    	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
    	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)
    	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)
    	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
    	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
    	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
    	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
    	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
    	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
    	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)
    	at scala.collection.immutable.List.map(List.scala:293)
    	at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)
    	at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)
    	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)
    	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:79)
    	... 115 more
    Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
    	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)
    	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)
    	... 128 more
    .
[0m11:27:00.615351 [debug] [MainThread]: On master: ROLLBACK
[0m11:27:00.617547 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:27:00.649222 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m11:27:00.651018 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:27:00.652781 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:27:00.655252 [debug] [MainThread]: On master: ROLLBACK
[0m11:27:00.656670 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m11:27:00.658484 [debug] [MainThread]: On master: Close
[0m11:27:00.662824 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:27:00.666462 [debug] [MainThread]: Connection 'model.data_pipeline_poc.dim_uemoa_indicators' was properly closed.
[0m11:27:00.668294 [info ] [MainThread]: 
[0m11:27:00.670074 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 1.05 seconds (1.05s).
[0m11:27:00.673241 [debug] [MainThread]: Command end result
[0m11:27:00.754421 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/app/dbt/target/manifest.json
[0m11:27:00.769426 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/app/dbt/target/semantic_manifest.json
[0m11:27:00.789773 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/app/dbt/target/run_results.json
[0m11:27:00.792288 [info ] [MainThread]: 
[0m11:27:00.795534 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m11:27:00.797592 [info ] [MainThread]: 
[0m11:27:00.800238 [error] [MainThread]:   Runtime Error in model dim_uemoa_indicators (models/silver/dim_uemoa_indicators.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] org.apache.spark.sql.AnalysisException: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: parquet; line 23 pos 9
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: parquet; line 23 pos 9
    	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:66)
    	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:87)
    	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:63)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
    	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
    	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
    	at org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:1676)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
    	at org.apache.spark.sql.catalyst.plans.logical.Filter.mapChildren(basicLogicalOperators.scala:316)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
    	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
    	at scala.collection.immutable.List.map(List.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:699)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
    	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:63)
    	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:43)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
    	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
    	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
    	at scala.collection.immutable.List.foldLeft(List.scala:91)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
    	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
    	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)
    	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)
    	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
    	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
    	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
    	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
    	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
    	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
    	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)
    	at scala.collection.immutable.List.map(List.scala:293)
    	at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)
    	at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)
    	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)
    	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:79)
    	... 115 more
    Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
    	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)
    	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)
    	... 128 more
    
[0m11:27:00.802819 [info ] [MainThread]: 
[0m11:27:00.804991 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m11:27:00.808297 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 5.343825, "process_in_blocks": "2352", "process_kernel_time": 0.311396, "process_mem_max_rss": "111364", "process_out_blocks": "0", "process_user_time": 1.608881}
[0m11:27:00.810535 [debug] [MainThread]: Command `dbt run` failed at 11:27:00.810406 after 5.35 seconds
[0m11:27:00.812516 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e263329d490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e263329cb50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e263329d810>]}
[0m11:27:00.814431 [debug] [MainThread]: Flushing usage events
[0m11:27:02.505489 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:31:24.821427 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72046e571d50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72046e939b10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72046e5716d0>]}


============================== 11:31:24.825643 | 44b16e63-b60b-45f3-a3dc-cfc59f21ad1a ==============================
[0m11:31:24.825643 [info ] [MainThread]: Running with dbt=1.9.0
[0m11:31:24.828382 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '/usr/app/dbt', 'log_path': '/usr/app/dbt/logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt run-operation create_bronze_table_from_parquet', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m11:31:24.882605 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:31:24.884450 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:31:24.886627 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:31:24.980139 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '44b16e63-b60b-45f3-a3dc-cfc59f21ad1a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72046df62110>]}
[0m11:31:25.015298 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '44b16e63-b60b-45f3-a3dc-cfc59f21ad1a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72046ea7b450>]}
[0m11:31:25.017812 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m11:31:25.099006 [debug] [MainThread]: checksum: c99e828bba267739642b5a3ce85f17518764ea526e0e6c4fdc649171c1a66bff, vars: {}, profile: , target: , version: 1.9.0
[0m11:31:28.420195 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:31:28.422329 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:31:28.429579 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- seeds.data_pipeline_poc
- snapshots.data_pipeline_poc
[0m11:31:28.457529 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '44b16e63-b60b-45f3-a3dc-cfc59f21ad1a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72046e596d50>]}
[0m11:31:28.581087 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/app/dbt/target/manifest.json
[0m11:31:28.594547 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/app/dbt/target/semantic_manifest.json
[0m11:31:28.626740 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '44b16e63-b60b-45f3-a3dc-cfc59f21ad1a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72046de46e10>]}
[0m11:31:28.629104 [info ] [MainThread]: Found 9 models, 13 data tests, 3 sources, 583 macros
[0m11:31:28.630908 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '44b16e63-b60b-45f3-a3dc-cfc59f21ad1a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72046db6b910>]}
[0m11:31:28.632905 [debug] [MainThread]: Acquiring new spark connection 'macro_create_bronze_table_from_parquet'
[0m11:31:28.634464 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:31:28.636217 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:31:28.638629 [error] [MainThread]: Encountered an error while running operation: Runtime Error
  dbt could not find a macro with the name "create_bronze_table_from_parquet" in any package
[0m11:31:28.641453 [debug] [MainThread]: Traceback (most recent call last):
  File "/usr/local/lib/python3.11/site-packages/dbt/task/run_operation.py", line 67, in run
    self._run_unsafe(package_name, macro_name)
  File "/usr/local/lib/python3.11/site-packages/dbt/task/run_operation.py", line 48, in _run_unsafe
    res = adapter.execute_macro(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 1176, in execute_macro
    raise DbtRuntimeError(
dbt_common.exceptions.base.DbtRuntimeError: Runtime Error
  dbt could not find a macro with the name "create_bronze_table_from_parquet" in any package

[0m11:31:28.643557 [error] [MainThread]: Encountered an error:
Internal Error
  dbt could not find a macro with the name 'create_bronze_table_from_parquet' in any package
[0m11:31:28.646179 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": false, "command_wall_clock_time": 3.9112837, "process_in_blocks": "32", "process_kernel_time": 0.191123, "process_mem_max_rss": "104080", "process_out_blocks": "0", "process_user_time": 1.172225}
[0m11:31:28.648519 [debug] [MainThread]: Command `dbt run-operation` failed at 11:31:28.648406 after 3.91 seconds
[0m11:31:28.650443 [debug] [MainThread]: Connection 'macro_create_bronze_table_from_parquet' was properly closed.
[0m11:31:28.652463 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72046f90b710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72046db4e510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72046db4c790>]}
[0m11:31:28.654676 [debug] [MainThread]: Flushing usage events
[0m11:31:29.736608 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:45:35.219840 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7890456b7450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7890456b7990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7890456b7190>]}


============================== 11:45:35.224099 | a0d9a16e-a8ec-498f-91b7-50c2ee7b50bb ==============================
[0m11:45:35.224099 [info ] [MainThread]: Running with dbt=1.9.0
[0m11:45:35.226151 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/usr/app/dbt/logs', 'debug': 'False', 'profiles_dir': '/usr/app/dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select dim_uemoa_indicators', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m11:45:35.279234 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:45:35.281943 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:45:35.284251 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:45:35.379634 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a0d9a16e-a8ec-498f-91b7-50c2ee7b50bb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78904508af10>]}
[0m11:45:35.417063 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a0d9a16e-a8ec-498f-91b7-50c2ee7b50bb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7890478a7a50>]}
[0m11:45:35.422303 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m11:45:35.513935 [debug] [MainThread]: checksum: c99e828bba267739642b5a3ce85f17518764ea526e0e6c4fdc649171c1a66bff, vars: {}, profile: , target: , version: 1.9.0
[0m11:45:36.178664 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading data_pipeline_poc: staging/sources.yml - Runtime Error
    Syntax error near line 43
    ------------------------------
    40 |         meta:
    41 |           file_format: parquet
    42 |               - unique
    43 |           - name: user_name
    44 |             description: User's display name
    45 |           - name: user_email
    46 |             description: User's email address
    
    Raw Error:
    ------------------------------
    while parsing a block mapping
      in "<unicode string>", line 41, column 11
    did not find expected key
      in "<unicode string>", line 43, column 11
[0m11:45:36.181679 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.0394043, "process_in_blocks": "0", "process_kernel_time": 0.191383, "process_mem_max_rss": "103344", "process_out_blocks": "0", "process_user_time": 0.984826}
[0m11:45:36.183903 [debug] [MainThread]: Command `dbt run` failed at 11:45:36.183780 after 1.04 seconds
[0m11:45:36.185801 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78904903d510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78904903d4d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7890456bcb90>]}
[0m11:45:36.187926 [debug] [MainThread]: Flushing usage events
[0m11:45:37.243444 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:46:25.547698 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71f1be1f6f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71f1be1f6e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71f1be1f7310>]}


============================== 11:46:25.552054 | c930926c-ea7b-42e5-b9d3-b2cd0bb499bb ==============================
[0m11:46:25.552054 [info ] [MainThread]: Running with dbt=1.9.0
[0m11:46:25.553874 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '/usr/app/dbt', 'log_path': '/usr/app/dbt/logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt run --select dim_uemoa_indicators', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m11:46:25.607169 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:46:25.609842 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:46:25.613620 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:46:25.709730 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c930926c-ea7b-42e5-b9d3-b2cd0bb499bb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71f1bdbe01d0>]}
[0m11:46:25.747119 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c930926c-ea7b-42e5-b9d3-b2cd0bb499bb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71f1bf0e3450>]}
[0m11:46:25.750301 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m11:46:25.838576 [debug] [MainThread]: checksum: c99e828bba267739642b5a3ce85f17518764ea526e0e6c4fdc649171c1a66bff, vars: {}, profile: , target: , version: 1.9.0
[0m11:46:26.488868 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading data_pipeline_poc: staging/sources.yml - Runtime Error
    Syntax error near line 1
    ------------------------------
    1  | version: 2version: 2
    2  | 
    3  | 
    4  | 
    
    Raw Error:
    ------------------------------
    mapping values are not allowed in this context
      in "<unicode string>", line 1, column 18
[0m11:46:26.491498 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.0277311, "process_in_blocks": "0", "process_kernel_time": 0.151296, "process_mem_max_rss": "103288", "process_out_blocks": "0", "process_user_time": 0.993648}
[0m11:46:26.493666 [debug] [MainThread]: Command `dbt run` failed at 11:46:26.493572 after 1.03 seconds
[0m11:46:26.496813 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71f1be080f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71f1be707e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71f1be1f6c50>]}
[0m11:46:26.498965 [debug] [MainThread]: Flushing usage events
[0m11:46:27.397197 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:47:19.443004 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74d5be1c3090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74d5be211910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74d5be1c3950>]}


============================== 11:47:19.447136 | 7c8e1d34-7a98-4e94-9d3b-e68eca89679a ==============================
[0m11:47:19.447136 [info ] [MainThread]: Running with dbt=1.9.0
[0m11:47:19.449248 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/usr/app/dbt/logs', 'profiles_dir': '/usr/app/dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select dim_uemoa_indicators', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m11:47:19.513745 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:47:19.516383 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:47:19.518412 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:47:19.614689 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7c8e1d34-7a98-4e94-9d3b-e68eca89679a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74d5be22ab90>]}
[0m11:47:19.651104 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7c8e1d34-7a98-4e94-9d3b-e68eca89679a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74d5c03f3ad0>]}
[0m11:47:19.654703 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m11:47:19.753010 [debug] [MainThread]: checksum: c99e828bba267739642b5a3ce85f17518764ea526e0e6c4fdc649171c1a66bff, vars: {}, profile: , target: , version: 1.9.0
[0m11:47:20.351674 [error] [MainThread]: Encountered an error:
'utf-8' codec can't decode byte 0xe9 in position 1294: invalid continuation byte
[0m11:47:20.357010 [error] [MainThread]: Traceback (most recent call last):
  File "/usr/local/lib/python3.11/site-packages/dbt/cli/requires.py", line 153, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/dbt/cli/requires.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/dbt/cli/requires.py", line 235, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/dbt/cli/requires.py", line 264, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/dbt/cli/requires.py", line 311, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/dbt/cli/requires.py", line 327, in wrapper
    setup_manifest(ctx, write=write, write_perf_info=write_perf_info)
  File "/usr/local/lib/python3.11/site-packages/dbt/cli/requires.py", line 351, in setup_manifest
    ctx.obj["manifest"] = parse_manifest(
                          ^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/dbt/parser/manifest.py", line 2037, in parse_manifest
    manifest = ManifestLoader.get_full_manifest(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/dbt/parser/manifest.py", line 310, in get_full_manifest
    manifest = loader.load()
               ^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/dbt/parser/manifest.py", line 357, in load
    file_reader.read_files()
  File "/usr/local/lib/python3.11/site-packages/dbt/parser/read_files.py", line 212, in read_files
    self.read_files_for_project(project, file_types)
  File "/usr/local/lib/python3.11/site-packages/dbt/parser/read_files.py", line 219, in read_files_for_project
    project_files[file_type_info["parser"]] = read_files_for_parser(
                                              ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/dbt/parser/read_files.py", line 167, in read_files_for_parser
    source_files = get_source_files(
                   ^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/dbt/parser/read_files.py", line 156, in get_source_files
    file = load_source_file(fp, parse_file_type, project.project_name, saved_files)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/dbt/parser/read_files.py", line 84, in load_source_file
    file_contents = load_file_contents(path.absolute_path, strip=True)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/dbt_common/clients/system.py", line 178, in load_file_contents
    to_return = handle.read().decode("utf-8")
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe9 in position 1294: invalid continuation byte

[0m11:47:20.360276 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.0077105, "process_in_blocks": "88", "process_kernel_time": 0.148685, "process_mem_max_rss": "102608", "process_out_blocks": "0", "process_user_time": 1.040796}
[0m11:47:20.362395 [debug] [MainThread]: Command `dbt run` failed at 11:47:20.362286 after 1.01 seconds
[0m11:47:20.364269 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74d5c1b89510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74d5c1b89890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74d5c1b89610>]}
[0m11:47:20.366232 [debug] [MainThread]: Flushing usage events
[0m11:47:26.867085 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:48:49.452564 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7df2fd74f710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7df2fd7b1c90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7df2fd7b0e90>]}


============================== 11:48:49.456993 | 932e318d-3f46-4f3b-ac3a-116ec23ff0ae ==============================
[0m11:48:49.456993 [info ] [MainThread]: Running with dbt=1.9.0
[0m11:48:49.459561 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/app/dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': '/usr/app/dbt/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run --select dim_uemoa_indicators', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m11:48:49.514751 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:48:49.517748 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:48:49.520512 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:48:49.622797 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '932e318d-3f46-4f3b-ac3a-116ec23ff0ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7df2fd757050>]}
[0m11:48:49.660038 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '932e318d-3f46-4f3b-ac3a-116ec23ff0ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7df2fe52f110>]}
[0m11:48:49.663306 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m11:48:49.758345 [debug] [MainThread]: checksum: c99e828bba267739642b5a3ce85f17518764ea526e0e6c4fdc649171c1a66bff, vars: {}, profile: , target: , version: 1.9.0
[0m11:48:53.271724 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m11:48:53.274402 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_poc://models/staging/sources.yml
[0m11:48:53.276820 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_poc://models/silver/dim_uemoa_indicators.sql
[0m11:48:53.711143 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- seeds.data_pipeline_poc
- snapshots.data_pipeline_poc
[0m11:48:53.769591 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '932e318d-3f46-4f3b-ac3a-116ec23ff0ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7df2f59cdd50>]}
[0m11:48:53.894799 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/app/dbt/target/manifest.json
[0m11:48:53.909432 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/app/dbt/target/semantic_manifest.json
[0m11:48:53.939559 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '932e318d-3f46-4f3b-ac3a-116ec23ff0ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7df2fd59f910>]}
[0m11:48:53.942403 [info ] [MainThread]: Found 9 models, 11 data tests, 3 sources, 583 macros
[0m11:48:53.944879 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '932e318d-3f46-4f3b-ac3a-116ec23ff0ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7df2fcbafcd0>]}
[0m11:48:53.947842 [info ] [MainThread]: 
[0m11:48:53.949901 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:48:53.951525 [info ] [MainThread]: 
[0m11:48:53.953749 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m11:48:53.956188 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m11:48:53.966064 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m11:48:53.968106 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m11:48:53.970375 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:48:54.883466 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:48:54.885953 [debug] [ThreadPool]: SQL status: OK in 0.916 seconds
[0m11:48:54.899748 [debug] [ThreadPool]: On list_schemas: Close
[0m11:48:54.911919 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default_default_gold)
[0m11:48:54.917244 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:48:54.919534 [debug] [ThreadPool]: Using spark connection "list_None_default_default_gold"
[0m11:48:54.922389 [debug] [ThreadPool]: On list_None_default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show table extended in default_default_gold like '*'
  
[0m11:48:54.924877 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:48:54.987390 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#11, tableName#12, isTemporary#13, information#14]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@3d7c185f, [default_default_gold]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#11, tableName#12, isTemporary#13, information#14]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@3d7c185f, [default_default_gold]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m11:48:54.989570 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m11:48:54.991752 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show table extended in default_default_gold like '*'
  
[0m11:48:54.993919 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#11, tableName#12, isTemporary#13, information#14]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@3d7c185f, [default_default_gold]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#11, tableName#12, isTemporary#13, information#14]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@3d7c185f, [default_default_gold]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m11:48:54.996086 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m11:48:54.998512 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#11, tableName#12, isTemporary#13, information#14]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@3d7c185f, [default_default_gold]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#11, tableName#12, isTemporary#13, information#14]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@3d7c185f, [default_default_gold]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m11:48:55.002977 [debug] [ThreadPool]: Using spark connection "list_None_default_default_gold"
[0m11:48:55.005833 [debug] [ThreadPool]: On list_None_default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show tables in default_default_gold like '*'
  
[0m11:48:55.102285 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:48:55.104109 [debug] [ThreadPool]: SQL status: OK in 0.096 seconds
[0m11:48:55.115076 [debug] [ThreadPool]: Using spark connection "list_None_default_default_gold"
[0m11:48:55.117181 [debug] [ThreadPool]: On list_None_default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
describe extended default_default_gold.fct_events_enriched
  
[0m11:48:55.309454 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:48:55.311596 [debug] [ThreadPool]: SQL status: OK in 0.193 seconds
[0m11:48:55.317656 [debug] [ThreadPool]: On list_None_default_default_gold: ROLLBACK
[0m11:48:55.319625 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m11:48:55.321512 [debug] [ThreadPool]: On list_None_default_default_gold: Close
[0m11:48:55.327530 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default_default_gold, now list_None_default_default_silver)
[0m11:48:55.331030 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:48:55.333278 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m11:48:55.334999 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show table extended in default_default_silver like '*'
  
[0m11:48:55.336879 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:48:55.382858 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#65, tableName#66, isTemporary#67, information#68]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@6b4df6ee, [default_default_silver]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#65, tableName#66, isTemporary#67, information#68]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@6b4df6ee, [default_default_silver]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m11:48:55.384993 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m11:48:55.387876 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show table extended in default_default_silver like '*'
  
[0m11:48:55.390514 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#65, tableName#66, isTemporary#67, information#68]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@6b4df6ee, [default_default_silver]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#65, tableName#66, isTemporary#67, information#68]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@6b4df6ee, [default_default_silver]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m11:48:55.392818 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m11:48:55.395518 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#65, tableName#66, isTemporary#67, information#68]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@6b4df6ee, [default_default_silver]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#65, tableName#66, isTemporary#67, information#68]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@6b4df6ee, [default_default_silver]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m11:48:55.399356 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m11:48:55.401647 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show tables in default_default_silver like '*'
  
[0m11:48:55.447623 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:48:55.450356 [debug] [ThreadPool]: SQL status: OK in 0.047 seconds
[0m11:48:55.457609 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m11:48:55.460485 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
describe extended default_default_silver.stg_events
  
[0m11:48:55.523765 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:48:55.526427 [debug] [ThreadPool]: SQL status: OK in 0.064 seconds
[0m11:48:55.532720 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m11:48:55.534969 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
describe extended default_default_silver.stg_users
  
[0m11:48:55.593382 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:48:55.595444 [debug] [ThreadPool]: SQL status: OK in 0.059 seconds
[0m11:48:55.600210 [debug] [ThreadPool]: On list_None_default_default_silver: ROLLBACK
[0m11:48:55.602045 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m11:48:55.604279 [debug] [ThreadPool]: On list_None_default_default_silver: Close
[0m11:48:55.611039 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default_default_silver, now list_None_default_silver)
[0m11:48:55.614589 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:48:55.616664 [debug] [ThreadPool]: Using spark connection "list_None_default_silver"
[0m11:48:55.618641 [debug] [ThreadPool]: On list_None_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_silver"} */
show table extended in default_silver like '*'
  
[0m11:48:55.621037 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:48:55.660866 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#141, tableName#142, isTemporary#143, information#144]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@22e36e2d, [default_silver]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#141, tableName#142, isTemporary#143, information#144]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@22e36e2d, [default_silver]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m11:48:55.662923 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m11:48:55.664705 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_silver"} */
show table extended in default_silver like '*'
  
[0m11:48:55.667158 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#141, tableName#142, isTemporary#143, information#144]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@22e36e2d, [default_silver]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#141, tableName#142, isTemporary#143, information#144]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@22e36e2d, [default_silver]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m11:48:55.669721 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m11:48:55.672057 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#141, tableName#142, isTemporary#143, information#144]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@22e36e2d, [default_silver]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#141, tableName#142, isTemporary#143, information#144]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@22e36e2d, [default_silver]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m11:48:55.675517 [debug] [ThreadPool]: Using spark connection "list_None_default_silver"
[0m11:48:55.677352 [debug] [ThreadPool]: On list_None_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_silver"} */
show tables in default_silver like '*'
  
[0m11:48:55.699608 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:48:55.701161 [debug] [ThreadPool]: SQL status: OK in 0.022 seconds
[0m11:48:55.704538 [debug] [ThreadPool]: On list_None_default_silver: ROLLBACK
[0m11:48:55.706744 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m11:48:55.708542 [debug] [ThreadPool]: On list_None_default_silver: Close
[0m11:48:55.715272 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default_silver, now list_None_default_gold)
[0m11:48:55.724791 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:48:55.727309 [debug] [ThreadPool]: Using spark connection "list_None_default_gold"
[0m11:48:55.730444 [debug] [ThreadPool]: On list_None_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_gold"} */
show table extended in default_gold like '*'
  
[0m11:48:55.732839 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:48:55.769521 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#156, tableName#157, isTemporary#158, information#159]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@a80051f, [default_gold]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#156, tableName#157, isTemporary#158, information#159]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@a80051f, [default_gold]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m11:48:55.771636 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m11:48:55.773555 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_gold"} */
show table extended in default_gold like '*'
  
[0m11:48:55.775879 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#156, tableName#157, isTemporary#158, information#159]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@a80051f, [default_gold]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#156, tableName#157, isTemporary#158, information#159]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@a80051f, [default_gold]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m11:48:55.778192 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m11:48:55.780137 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#156, tableName#157, isTemporary#158, information#159]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@a80051f, [default_gold]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#156, tableName#157, isTemporary#158, information#159]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@a80051f, [default_gold]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m11:48:55.783327 [debug] [ThreadPool]: Using spark connection "list_None_default_gold"
[0m11:48:55.785174 [debug] [ThreadPool]: On list_None_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_gold"} */
show tables in default_gold like '*'
  
[0m11:48:55.808322 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:48:55.814556 [debug] [ThreadPool]: SQL status: OK in 0.028 seconds
[0m11:48:55.819365 [debug] [ThreadPool]: On list_None_default_gold: ROLLBACK
[0m11:48:55.822093 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m11:48:55.824658 [debug] [ThreadPool]: On list_None_default_gold: Close
[0m11:48:55.831395 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '932e318d-3f46-4f3b-ac3a-116ec23ff0ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7df2f5897ed0>]}
[0m11:48:55.833700 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:48:55.835642 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:48:55.839229 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.dim_uemoa_indicators
[0m11:48:55.841839 [info ] [Thread-1 (]: 1 of 1 START sql table model default_silver.dim_uemoa_indicators ............... [RUN]
[0m11:48:55.844146 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default_gold, now model.data_pipeline_poc.dim_uemoa_indicators)
[0m11:48:55.846439 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_poc.dim_uemoa_indicators
[0m11:48:55.853161 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_poc.dim_uemoa_indicators"
[0m11:48:55.874771 [debug] [Thread-1 (]: Began executing node model.data_pipeline_poc.dim_uemoa_indicators
[0m11:48:55.888049 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:48:55.890847 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.dim_uemoa_indicators"
[0m11:48:55.893625 [debug] [Thread-1 (]: On model.data_pipeline_poc.dim_uemoa_indicators: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.dim_uemoa_indicators"} */

        CREATE TABLE IF NOT EXISTS bronze.indicateurs_economiques_uemoa USING parquet LOCATION 's3a://lakehouse/bronze/indicateurs_economiques_uemoa'
      
[0m11:48:55.896193 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:48:56.126362 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:48:56.128471 [debug] [Thread-1 (]: SQL status: OK in 0.232 seconds
[0m11:48:56.135424 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.dim_uemoa_indicators"
[0m11:48:56.138039 [debug] [Thread-1 (]: On model.data_pipeline_poc.dim_uemoa_indicators: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.dim_uemoa_indicators"} */
drop table if exists default_silver.dim_uemoa_indicators
[0m11:48:56.157562 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:48:56.159900 [debug] [Thread-1 (]: SQL status: OK in 0.020 seconds
[0m11:48:56.185252 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_poc.dim_uemoa_indicators"
[0m11:48:56.214139 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.dim_uemoa_indicators"
[0m11:48:56.217502 [debug] [Thread-1 (]: On model.data_pipeline_poc.dim_uemoa_indicators: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.dim_uemoa_indicators"} */

  
    
        create or replace table default_silver.dim_uemoa_indicators
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Lecture directe des données Parquet d'Airbyte et transformation en table Iceberg Silver
SELECT
    date,
    pib_nominal_milliards_fcfa,
    poids_secteur_primaire_pct,
    poids_secteur_secondaire_pct,
    poids_secteur_tertiaire_pct,
    taux_croissance_reel_pib_pct,
    taux_inflation_moyen_annuel_ipc_pct,
    recettes_fiscales,
    recettes_fiscales_pct_pib,
    depenses_totales_et_prets_nets,
    solde_budgetaire_global_avec_dons,
    solde_budgetaire_global_hors_dons,
    encours_de_la_dette,
    encours_de_la_dette_pct_pib,
    exportations_biens_fob,
    importations_biens_fob,
    balance_des_biens,
    compte_transactions_courantes,
    balance_courante_sur_pib_pct,
    agregats_monnaie_masse_monetaire_m2,
    taux_couverture_emission_monetaire
FROM bronze.indicateurs_economiques_uemoa
WHERE pib_nominal_milliards_fcfa IS NOT NULL
  
[0m11:48:56.283828 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='42703', errorCode=0, errorMessage="org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITHOUT_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITHOUT_SUGGESTION] A column or function parameter with name `pib_nominal_milliards_fcfa` cannot be resolved. ; line 43 pos 6;\n'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false\n:- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@605526d0, default_silver.dim_uemoa_indicators\n+- 'Project ['date, 'pib_nominal_milliards_fcfa, 'poids_secteur_primaire_pct, 'poids_secteur_secondaire_pct, 'poids_secteur_tertiaire_pct, 'taux_croissance_reel_pib_pct, 'taux_inflation_moyen_annuel_ipc_pct, 'recettes_fiscales, 'recettes_fiscales_pct_pib, 'depenses_totales_et_prets_nets, 'solde_budgetaire_global_avec_dons, 'solde_budgetaire_global_hors_dons, 'encours_de_la_dette, 'encours_de_la_dette_pct_pib, 'exportations_biens_fob, 'importations_biens_fob, 'balance_des_biens, 'compte_transactions_courantes, 'balance_courante_sur_pib_pct, 'agregats_monnaie_masse_monetaire_m2, 'taux_couverture_emission_monetaire]\n   +- 'Filter isnotnull('pib_nominal_milliards_fcfa)\n      +- SubqueryAlias spark_catalog.bronze.indicateurs_economiques_uemoa\n         +- RelationV2[] spark_catalog.bronze.indicateurs_economiques_uemoa spark_catalog.bronze.indicateurs_economiques_uemoa\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITHOUT_SUGGESTION] A column or function parameter with name `pib_nominal_milliards_fcfa` cannot be resolved. ; line 43 pos 6;\n'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false\n:- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@605526d0, default_silver.dim_uemoa_indicators\n+- 'Project ['date, 'pib_nominal_milliards_fcfa, 'poids_secteur_primaire_pct, 'poids_secteur_secondaire_pct, 'poids_secteur_tertiaire_pct, 'taux_croissance_reel_pib_pct, 'taux_inflation_moyen_annuel_ipc_pct, 'recettes_fiscales, 'recettes_fiscales_pct_pib, 'depenses_totales_et_prets_nets, 'solde_budgetaire_global_avec_dons, 'solde_budgetaire_global_hors_dons, 'encours_de_la_dette, 'encours_de_la_dette_pct_pib, 'exportations_biens_fob, 'importations_biens_fob, 'balance_des_biens, 'compte_transactions_courantes, 'balance_courante_sur_pib_pct, 'agregats_monnaie_masse_monetaire_m2, 'taux_couverture_emission_monetaire]\n   +- 'Filter isnotnull('pib_nominal_milliards_fcfa)\n      +- SubqueryAlias spark_catalog.bronze.indicateurs_economiques_uemoa\n         +- RelationV2[] spark_catalog.bronze.indicateurs_economiques_uemoa spark_catalog.bronze.indicateurs_economiques_uemoa\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)\n\tat scala.collection.immutable.Stream.foreach(Stream.scala:533)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n", taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m11:48:56.291198 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m11:48:56.293717 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.dim_uemoa_indicators"} */

  
    
        create or replace table default_silver.dim_uemoa_indicators
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Lecture directe des données Parquet d'Airbyte et transformation en table Iceberg Silver
SELECT
    date,
    pib_nominal_milliards_fcfa,
    poids_secteur_primaire_pct,
    poids_secteur_secondaire_pct,
    poids_secteur_tertiaire_pct,
    taux_croissance_reel_pib_pct,
    taux_inflation_moyen_annuel_ipc_pct,
    recettes_fiscales,
    recettes_fiscales_pct_pib,
    depenses_totales_et_prets_nets,
    solde_budgetaire_global_avec_dons,
    solde_budgetaire_global_hors_dons,
    encours_de_la_dette,
    encours_de_la_dette_pct_pib,
    exportations_biens_fob,
    importations_biens_fob,
    balance_des_biens,
    compte_transactions_courantes,
    balance_courante_sur_pib_pct,
    agregats_monnaie_masse_monetaire_m2,
    taux_couverture_emission_monetaire
FROM bronze.indicateurs_economiques_uemoa
WHERE pib_nominal_milliards_fcfa IS NOT NULL
  
[0m11:48:56.295979 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITHOUT_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITHOUT_SUGGESTION] A column or function parameter with name `pib_nominal_milliards_fcfa` cannot be resolved. ; line 43 pos 6;
  'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false
  :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@605526d0, default_silver.dim_uemoa_indicators
  +- 'Project ['date, 'pib_nominal_milliards_fcfa, 'poids_secteur_primaire_pct, 'poids_secteur_secondaire_pct, 'poids_secteur_tertiaire_pct, 'taux_croissance_reel_pib_pct, 'taux_inflation_moyen_annuel_ipc_pct, 'recettes_fiscales, 'recettes_fiscales_pct_pib, 'depenses_totales_et_prets_nets, 'solde_budgetaire_global_avec_dons, 'solde_budgetaire_global_hors_dons, 'encours_de_la_dette, 'encours_de_la_dette_pct_pib, 'exportations_biens_fob, 'importations_biens_fob, 'balance_des_biens, 'compte_transactions_courantes, 'balance_courante_sur_pib_pct, 'agregats_monnaie_masse_monetaire_m2, 'taux_couverture_emission_monetaire]
     +- 'Filter isnotnull('pib_nominal_milliards_fcfa)
        +- SubqueryAlias spark_catalog.bronze.indicateurs_economiques_uemoa
           +- RelationV2[] spark_catalog.bronze.indicateurs_economiques_uemoa spark_catalog.bronze.indicateurs_economiques_uemoa
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITHOUT_SUGGESTION] A column or function parameter with name `pib_nominal_milliards_fcfa` cannot be resolved. ; line 43 pos 6;
  'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false
  :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@605526d0, default_silver.dim_uemoa_indicators
  +- 'Project ['date, 'pib_nominal_milliards_fcfa, 'poids_secteur_primaire_pct, 'poids_secteur_secondaire_pct, 'poids_secteur_tertiaire_pct, 'taux_croissance_reel_pib_pct, 'taux_inflation_moyen_annuel_ipc_pct, 'recettes_fiscales, 'recettes_fiscales_pct_pib, 'depenses_totales_et_prets_nets, 'solde_budgetaire_global_avec_dons, 'solde_budgetaire_global_hors_dons, 'encours_de_la_dette, 'encours_de_la_dette_pct_pib, 'exportations_biens_fob, 'importations_biens_fob, 'balance_des_biens, 'compte_transactions_courantes, 'balance_courante_sur_pib_pct, 'agregats_monnaie_masse_monetaire_m2, 'taux_couverture_emission_monetaire]
     +- 'Filter isnotnull('pib_nominal_milliards_fcfa)
        +- SubqueryAlias spark_catalog.bronze.indicateurs_economiques_uemoa
           +- RelationV2[] spark_catalog.bronze.indicateurs_economiques_uemoa spark_catalog.bronze.indicateurs_economiques_uemoa
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
  	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
  	at scala.collection.immutable.List.foreach(List.scala:431)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m11:48:56.298736 [debug] [Thread-1 (]: On model.data_pipeline_poc.dim_uemoa_indicators: ROLLBACK
[0m11:48:56.300754 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m11:48:56.302700 [debug] [Thread-1 (]: On model.data_pipeline_poc.dim_uemoa_indicators: Close
[0m11:48:56.312961 [debug] [Thread-1 (]: Runtime Error in model dim_uemoa_indicators (models/silver/dim_uemoa_indicators.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITHOUT_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITHOUT_SUGGESTION] A column or function parameter with name `pib_nominal_milliards_fcfa` cannot be resolved. ; line 43 pos 6;
    'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false
    :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@605526d0, default_silver.dim_uemoa_indicators
    +- 'Project ['date, 'pib_nominal_milliards_fcfa, 'poids_secteur_primaire_pct, 'poids_secteur_secondaire_pct, 'poids_secteur_tertiaire_pct, 'taux_croissance_reel_pib_pct, 'taux_inflation_moyen_annuel_ipc_pct, 'recettes_fiscales, 'recettes_fiscales_pct_pib, 'depenses_totales_et_prets_nets, 'solde_budgetaire_global_avec_dons, 'solde_budgetaire_global_hors_dons, 'encours_de_la_dette, 'encours_de_la_dette_pct_pib, 'exportations_biens_fob, 'importations_biens_fob, 'balance_des_biens, 'compte_transactions_courantes, 'balance_courante_sur_pib_pct, 'agregats_monnaie_masse_monetaire_m2, 'taux_couverture_emission_monetaire]
       +- 'Filter isnotnull('pib_nominal_milliards_fcfa)
          +- SubqueryAlias spark_catalog.bronze.indicateurs_economiques_uemoa
             +- RelationV2[] spark_catalog.bronze.indicateurs_economiques_uemoa spark_catalog.bronze.indicateurs_economiques_uemoa
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITHOUT_SUGGESTION] A column or function parameter with name `pib_nominal_milliards_fcfa` cannot be resolved. ; line 43 pos 6;
    'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false
    :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@605526d0, default_silver.dim_uemoa_indicators
    +- 'Project ['date, 'pib_nominal_milliards_fcfa, 'poids_secteur_primaire_pct, 'poids_secteur_secondaire_pct, 'poids_secteur_tertiaire_pct, 'taux_croissance_reel_pib_pct, 'taux_inflation_moyen_annuel_ipc_pct, 'recettes_fiscales, 'recettes_fiscales_pct_pib, 'depenses_totales_et_prets_nets, 'solde_budgetaire_global_avec_dons, 'solde_budgetaire_global_hors_dons, 'encours_de_la_dette, 'encours_de_la_dette_pct_pib, 'exportations_biens_fob, 'importations_biens_fob, 'balance_des_biens, 'compte_transactions_courantes, 'balance_courante_sur_pib_pct, 'agregats_monnaie_masse_monetaire_m2, 'taux_couverture_emission_monetaire]
       +- 'Filter isnotnull('pib_nominal_milliards_fcfa)
          +- SubqueryAlias spark_catalog.bronze.indicateurs_economiques_uemoa
             +- RelationV2[] spark_catalog.bronze.indicateurs_economiques_uemoa spark_catalog.bronze.indicateurs_economiques_uemoa
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m11:48:56.316583 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '932e318d-3f46-4f3b-ac3a-116ec23ff0ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7df2fe6a4850>]}
[0m11:48:56.319072 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model default_silver.dim_uemoa_indicators ...... [[31mERROR[0m in 0.47s]
[0m11:48:56.322161 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.dim_uemoa_indicators
[0m11:48:56.324526 [debug] [Thread-4 (]: Marking all children of 'model.data_pipeline_poc.dim_uemoa_indicators' to be skipped because of status 'error'.  Reason: Runtime Error in model dim_uemoa_indicators (models/silver/dim_uemoa_indicators.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITHOUT_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITHOUT_SUGGESTION] A column or function parameter with name `pib_nominal_milliards_fcfa` cannot be resolved. ; line 43 pos 6;
    'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false
    :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@605526d0, default_silver.dim_uemoa_indicators
    +- 'Project ['date, 'pib_nominal_milliards_fcfa, 'poids_secteur_primaire_pct, 'poids_secteur_secondaire_pct, 'poids_secteur_tertiaire_pct, 'taux_croissance_reel_pib_pct, 'taux_inflation_moyen_annuel_ipc_pct, 'recettes_fiscales, 'recettes_fiscales_pct_pib, 'depenses_totales_et_prets_nets, 'solde_budgetaire_global_avec_dons, 'solde_budgetaire_global_hors_dons, 'encours_de_la_dette, 'encours_de_la_dette_pct_pib, 'exportations_biens_fob, 'importations_biens_fob, 'balance_des_biens, 'compte_transactions_courantes, 'balance_courante_sur_pib_pct, 'agregats_monnaie_masse_monetaire_m2, 'taux_couverture_emission_monetaire]
       +- 'Filter isnotnull('pib_nominal_milliards_fcfa)
          +- SubqueryAlias spark_catalog.bronze.indicateurs_economiques_uemoa
             +- RelationV2[] spark_catalog.bronze.indicateurs_economiques_uemoa spark_catalog.bronze.indicateurs_economiques_uemoa
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITHOUT_SUGGESTION] A column or function parameter with name `pib_nominal_milliards_fcfa` cannot be resolved. ; line 43 pos 6;
    'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false
    :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@605526d0, default_silver.dim_uemoa_indicators
    +- 'Project ['date, 'pib_nominal_milliards_fcfa, 'poids_secteur_primaire_pct, 'poids_secteur_secondaire_pct, 'poids_secteur_tertiaire_pct, 'taux_croissance_reel_pib_pct, 'taux_inflation_moyen_annuel_ipc_pct, 'recettes_fiscales, 'recettes_fiscales_pct_pib, 'depenses_totales_et_prets_nets, 'solde_budgetaire_global_avec_dons, 'solde_budgetaire_global_hors_dons, 'encours_de_la_dette, 'encours_de_la_dette_pct_pib, 'exportations_biens_fob, 'importations_biens_fob, 'balance_des_biens, 'compte_transactions_courantes, 'balance_courante_sur_pib_pct, 'agregats_monnaie_masse_monetaire_m2, 'taux_couverture_emission_monetaire]
       +- 'Filter isnotnull('pib_nominal_milliards_fcfa)
          +- SubqueryAlias spark_catalog.bronze.indicateurs_economiques_uemoa
             +- RelationV2[] spark_catalog.bronze.indicateurs_economiques_uemoa spark_catalog.bronze.indicateurs_economiques_uemoa
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    .
[0m11:48:56.328210 [debug] [MainThread]: On master: ROLLBACK
[0m11:48:56.330068 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:48:56.364783 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m11:48:56.366706 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:48:56.368463 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:48:56.370030 [debug] [MainThread]: On master: ROLLBACK
[0m11:48:56.371672 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m11:48:56.373466 [debug] [MainThread]: On master: Close
[0m11:48:56.378081 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:48:56.380051 [debug] [MainThread]: Connection 'model.data_pipeline_poc.dim_uemoa_indicators' was properly closed.
[0m11:48:56.383130 [info ] [MainThread]: 
[0m11:48:56.384967 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 2.43 seconds (2.43s).
[0m11:48:56.387268 [debug] [MainThread]: Command end result
[0m11:48:56.467114 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/app/dbt/target/manifest.json
[0m11:48:56.478638 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/app/dbt/target/semantic_manifest.json
[0m11:48:56.494203 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/app/dbt/target/run_results.json
[0m11:48:56.496089 [info ] [MainThread]: 
[0m11:48:56.497944 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m11:48:56.499937 [info ] [MainThread]: 
[0m11:48:56.502335 [error] [MainThread]:   Runtime Error in model dim_uemoa_indicators (models/silver/dim_uemoa_indicators.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITHOUT_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITHOUT_SUGGESTION] A column or function parameter with name `pib_nominal_milliards_fcfa` cannot be resolved. ; line 43 pos 6;
    'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false
    :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@605526d0, default_silver.dim_uemoa_indicators
    +- 'Project ['date, 'pib_nominal_milliards_fcfa, 'poids_secteur_primaire_pct, 'poids_secteur_secondaire_pct, 'poids_secteur_tertiaire_pct, 'taux_croissance_reel_pib_pct, 'taux_inflation_moyen_annuel_ipc_pct, 'recettes_fiscales, 'recettes_fiscales_pct_pib, 'depenses_totales_et_prets_nets, 'solde_budgetaire_global_avec_dons, 'solde_budgetaire_global_hors_dons, 'encours_de_la_dette, 'encours_de_la_dette_pct_pib, 'exportations_biens_fob, 'importations_biens_fob, 'balance_des_biens, 'compte_transactions_courantes, 'balance_courante_sur_pib_pct, 'agregats_monnaie_masse_monetaire_m2, 'taux_couverture_emission_monetaire]
       +- 'Filter isnotnull('pib_nominal_milliards_fcfa)
          +- SubqueryAlias spark_catalog.bronze.indicateurs_economiques_uemoa
             +- RelationV2[] spark_catalog.bronze.indicateurs_economiques_uemoa spark_catalog.bronze.indicateurs_economiques_uemoa
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITHOUT_SUGGESTION] A column or function parameter with name `pib_nominal_milliards_fcfa` cannot be resolved. ; line 43 pos 6;
    'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false
    :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@605526d0, default_silver.dim_uemoa_indicators
    +- 'Project ['date, 'pib_nominal_milliards_fcfa, 'poids_secteur_primaire_pct, 'poids_secteur_secondaire_pct, 'poids_secteur_tertiaire_pct, 'taux_croissance_reel_pib_pct, 'taux_inflation_moyen_annuel_ipc_pct, 'recettes_fiscales, 'recettes_fiscales_pct_pib, 'depenses_totales_et_prets_nets, 'solde_budgetaire_global_avec_dons, 'solde_budgetaire_global_hors_dons, 'encours_de_la_dette, 'encours_de_la_dette_pct_pib, 'exportations_biens_fob, 'importations_biens_fob, 'balance_des_biens, 'compte_transactions_courantes, 'balance_courante_sur_pib_pct, 'agregats_monnaie_masse_monetaire_m2, 'taux_couverture_emission_monetaire]
       +- 'Filter isnotnull('pib_nominal_milliards_fcfa)
          +- SubqueryAlias spark_catalog.bronze.indicateurs_economiques_uemoa
             +- RelationV2[] spark_catalog.bronze.indicateurs_economiques_uemoa spark_catalog.bronze.indicateurs_economiques_uemoa
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m11:48:56.505063 [info ] [MainThread]: 
[0m11:48:56.507184 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m11:48:56.510439 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 7.13902, "process_in_blocks": "0", "process_kernel_time": 0.297071, "process_mem_max_rss": "112652", "process_out_blocks": "0", "process_user_time": 1.770385}
[0m11:48:56.512539 [debug] [MainThread]: Command `dbt run` failed at 11:48:56.512436 after 7.14 seconds
[0m11:48:56.514414 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7df2fd2cfd10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7df301111510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7df3011114d0>]}
[0m11:48:56.516285 [debug] [MainThread]: Flushing usage events
[0m11:49:00.444496 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:09:43.588486 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76b6ec3b3850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76b6ec3b30d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76b6ec3b3250>]}


============================== 12:09:43.592771 | c4a6abdb-b8b1-46f9-bc25-22e37981f1e1 ==============================
[0m12:09:43.592771 [info ] [MainThread]: Running with dbt=1.9.0
[0m12:09:43.594958 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/usr/app/dbt', 'log_path': '/usr/app/dbt/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'invocation_command': 'dbt run', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m12:09:43.645775 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:09:43.648573 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:09:43.650962 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:09:43.741435 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c4a6abdb-b8b1-46f9-bc25-22e37981f1e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76b6ec3b36d0>]}
[0m12:09:43.778798 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c4a6abdb-b8b1-46f9-bc25-22e37981f1e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76b6ed1cbc10>]}
[0m12:09:43.781753 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m12:09:43.868689 [debug] [MainThread]: checksum: c99e828bba267739642b5a3ce85f17518764ea526e0e6c4fdc649171c1a66bff, vars: {}, profile: , target: , version: 1.9.0
[0m12:09:47.029505 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m12:09:47.031366 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_poc://models/staging/sources.yml
[0m12:09:47.032942 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_poc://models/silver/dim_uemoa_indicators.sql
[0m12:09:47.364902 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- seeds.data_pipeline_poc
- snapshots.data_pipeline_poc
[0m12:09:47.422300 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c4a6abdb-b8b1-46f9-bc25-22e37981f1e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76b6e861c690>]}
[0m12:09:47.518437 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/app/dbt/target/manifest.json
[0m12:09:47.526804 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/app/dbt/target/semantic_manifest.json
[0m12:09:47.550653 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c4a6abdb-b8b1-46f9-bc25-22e37981f1e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76b6ebd891d0>]}
[0m12:09:47.552529 [info ] [MainThread]: Found 9 models, 13 data tests, 3 sources, 583 macros
[0m12:09:47.554013 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c4a6abdb-b8b1-46f9-bc25-22e37981f1e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76b6e84e1310>]}
[0m12:09:47.556608 [info ] [MainThread]: 
[0m12:09:47.557890 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:09:47.559110 [info ] [MainThread]: 
[0m12:09:47.560454 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m12:09:47.566444 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m12:09:47.574211 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m12:09:47.576082 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m12:09:47.577883 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:09:47.638104 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m12:09:47.639765 [debug] [ThreadPool]: SQL status: OK in 0.062 seconds
[0m12:09:47.644053 [debug] [ThreadPool]: On list_schemas: Close
[0m12:09:47.650189 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m12:09:47.651488 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m12:09:47.653075 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:09:47.700355 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m12:09:47.702488 [debug] [ThreadPool]: SQL status: OK in 0.049 seconds
[0m12:09:47.707414 [debug] [ThreadPool]: On list_schemas: Close
[0m12:09:47.713353 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m12:09:47.715382 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m12:09:47.716822 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:09:47.766029 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m12:09:47.768117 [debug] [ThreadPool]: SQL status: OK in 0.051 seconds
[0m12:09:47.772879 [debug] [ThreadPool]: On list_schemas: Close
[0m12:09:47.780144 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m12:09:47.782152 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m12:09:47.783360 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:09:47.839837 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m12:09:47.842426 [debug] [ThreadPool]: SQL status: OK in 0.059 seconds
[0m12:09:47.849901 [debug] [ThreadPool]: On list_schemas: Close
[0m12:09:47.858217 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default_silver)
[0m12:09:47.863795 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:09:47.865859 [debug] [ThreadPool]: Using spark connection "list_None_default_silver"
[0m12:09:47.867919 [debug] [ThreadPool]: On list_None_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_silver"} */
show table extended in default_silver like '*'
  
[0m12:09:47.869781 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:09:47.905782 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#217, tableName#218, isTemporary#219, information#220]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@591c8753, [default_silver]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#217, tableName#218, isTemporary#219, information#220]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@591c8753, [default_silver]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m12:09:47.907503 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m12:09:47.908949 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_silver"} */
show table extended in default_silver like '*'
  
[0m12:09:47.910759 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#217, tableName#218, isTemporary#219, information#220]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@591c8753, [default_silver]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#217, tableName#218, isTemporary#219, information#220]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@591c8753, [default_silver]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m12:09:47.912682 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m12:09:47.914516 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#217, tableName#218, isTemporary#219, information#220]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@591c8753, [default_silver]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#217, tableName#218, isTemporary#219, information#220]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@591c8753, [default_silver]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m12:09:47.918611 [debug] [ThreadPool]: Using spark connection "list_None_default_silver"
[0m12:09:47.919805 [debug] [ThreadPool]: On list_None_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_silver"} */
show tables in default_silver like '*'
  
[0m12:09:47.939966 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m12:09:47.941835 [debug] [ThreadPool]: SQL status: OK in 0.021 seconds
[0m12:09:47.945754 [debug] [ThreadPool]: On list_None_default_silver: ROLLBACK
[0m12:09:47.947871 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m12:09:47.949428 [debug] [ThreadPool]: On list_None_default_silver: Close
[0m12:09:47.954711 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default_silver, now list_None_default_default_silver)
[0m12:09:47.957798 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:09:47.959246 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m12:09:47.961086 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show table extended in default_default_silver like '*'
  
[0m12:09:47.962638 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:09:47.993044 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#232, tableName#233, isTemporary#234, information#235]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@59708af6, [default_default_silver]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#232, tableName#233, isTemporary#234, information#235]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@59708af6, [default_default_silver]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m12:09:47.995081 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m12:09:47.996875 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show table extended in default_default_silver like '*'
  
[0m12:09:47.998914 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#232, tableName#233, isTemporary#234, information#235]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@59708af6, [default_default_silver]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#232, tableName#233, isTemporary#234, information#235]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@59708af6, [default_default_silver]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m12:09:48.000711 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m12:09:48.002749 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#232, tableName#233, isTemporary#234, information#235]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@59708af6, [default_default_silver]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#232, tableName#233, isTemporary#234, information#235]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@59708af6, [default_default_silver]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m12:09:48.005960 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m12:09:48.007628 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show tables in default_default_silver like '*'
  
[0m12:09:48.033305 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m12:09:48.035551 [debug] [ThreadPool]: SQL status: OK in 0.026 seconds
[0m12:09:48.042910 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m12:09:48.045019 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
describe extended default_default_silver.stg_events
  
[0m12:09:48.099729 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m12:09:48.101477 [debug] [ThreadPool]: SQL status: OK in 0.055 seconds
[0m12:09:48.107548 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m12:09:48.109278 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
describe extended default_default_silver.stg_users
  
[0m12:09:48.151495 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m12:09:48.153709 [debug] [ThreadPool]: SQL status: OK in 0.043 seconds
[0m12:09:48.158752 [debug] [ThreadPool]: On list_None_default_default_silver: ROLLBACK
[0m12:09:48.160284 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m12:09:48.161684 [debug] [ThreadPool]: On list_None_default_default_silver: Close
[0m12:09:48.167911 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default_default_silver, now list_None_default_gold)
[0m12:09:48.170754 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:09:48.173172 [debug] [ThreadPool]: Using spark connection "list_None_default_gold"
[0m12:09:48.174775 [debug] [ThreadPool]: On list_None_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_gold"} */
show table extended in default_gold like '*'
  
[0m12:09:48.176142 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:09:48.207600 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#308, tableName#309, isTemporary#310, information#311]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@696b5df8, [default_gold]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#308, tableName#309, isTemporary#310, information#311]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@696b5df8, [default_gold]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m12:09:48.209639 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m12:09:48.211517 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_gold"} */
show table extended in default_gold like '*'
  
[0m12:09:48.213575 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#308, tableName#309, isTemporary#310, information#311]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@696b5df8, [default_gold]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#308, tableName#309, isTemporary#310, information#311]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@696b5df8, [default_gold]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m12:09:48.215670 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m12:09:48.217539 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#308, tableName#309, isTemporary#310, information#311]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@696b5df8, [default_gold]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#308, tableName#309, isTemporary#310, information#311]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@696b5df8, [default_gold]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m12:09:48.220349 [debug] [ThreadPool]: Using spark connection "list_None_default_gold"
[0m12:09:48.222145 [debug] [ThreadPool]: On list_None_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_gold"} */
show tables in default_gold like '*'
  
[0m12:09:48.242699 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m12:09:48.244610 [debug] [ThreadPool]: SQL status: OK in 0.021 seconds
[0m12:09:48.248735 [debug] [ThreadPool]: On list_None_default_gold: ROLLBACK
[0m12:09:48.250618 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m12:09:48.252378 [debug] [ThreadPool]: On list_None_default_gold: Close
[0m12:09:48.257477 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default_gold, now list_None_default_default_gold)
[0m12:09:48.261176 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:09:48.263383 [debug] [ThreadPool]: Using spark connection "list_None_default_default_gold"
[0m12:09:48.265449 [debug] [ThreadPool]: On list_None_default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show table extended in default_default_gold like '*'
  
[0m12:09:48.267308 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:09:48.308101 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#323, tableName#324, isTemporary#325, information#326]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@280bf958, [default_default_gold]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#323, tableName#324, isTemporary#325, information#326]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@280bf958, [default_default_gold]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m12:09:48.309634 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m12:09:48.310721 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show table extended in default_default_gold like '*'
  
[0m12:09:48.312709 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#323, tableName#324, isTemporary#325, information#326]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@280bf958, [default_default_gold]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#323, tableName#324, isTemporary#325, information#326]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@280bf958, [default_default_gold]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m12:09:48.314665 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m12:09:48.316036 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#323, tableName#324, isTemporary#325, information#326]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@280bf958, [default_default_gold]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#323, tableName#324, isTemporary#325, information#326]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@280bf958, [default_default_gold]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m12:09:48.325246 [debug] [ThreadPool]: Using spark connection "list_None_default_default_gold"
[0m12:09:48.327010 [debug] [ThreadPool]: On list_None_default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show tables in default_default_gold like '*'
  
[0m12:09:48.364669 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m12:09:48.366653 [debug] [ThreadPool]: SQL status: OK in 0.038 seconds
[0m12:09:48.372779 [debug] [ThreadPool]: Using spark connection "list_None_default_default_gold"
[0m12:09:48.374558 [debug] [ThreadPool]: On list_None_default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
describe extended default_default_gold.fct_events_enriched
  
[0m12:09:48.415715 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m12:09:48.417314 [debug] [ThreadPool]: SQL status: OK in 0.042 seconds
[0m12:09:48.421900 [debug] [ThreadPool]: On list_None_default_default_gold: ROLLBACK
[0m12:09:48.423613 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m12:09:48.425029 [debug] [ThreadPool]: On list_None_default_default_gold: Close
[0m12:09:48.431105 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c4a6abdb-b8b1-46f9-bc25-22e37981f1e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76b6eb851c90>]}
[0m12:09:48.432854 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:09:48.434429 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:09:48.437518 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.dim_uemoa_indicators
[0m12:09:48.439425 [info ] [Thread-1 (]: 1 of 9 START sql table model default_silver.dim_uemoa_indicators ............... [RUN]
[0m12:09:48.441495 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default_default_gold, now model.data_pipeline_poc.dim_uemoa_indicators)
[0m12:09:48.443421 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_poc.dim_uemoa_indicators
[0m12:09:48.450924 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_poc.dim_uemoa_indicators"
[0m12:09:48.463585 [debug] [Thread-1 (]: Began executing node model.data_pipeline_poc.dim_uemoa_indicators
[0m12:09:48.477536 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.dim_uemoa_indicators"
[0m12:09:48.479294 [debug] [Thread-1 (]: On model.data_pipeline_poc.dim_uemoa_indicators: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.dim_uemoa_indicators"} */
drop table if exists default_silver.dim_uemoa_indicators
[0m12:09:48.480944 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:09:48.521342 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m12:09:48.523512 [debug] [Thread-1 (]: SQL status: OK in 0.043 seconds
[0m12:09:48.549011 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_poc.dim_uemoa_indicators"
[0m12:09:48.566629 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:09:48.568248 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.dim_uemoa_indicators"
[0m12:09:48.570107 [debug] [Thread-1 (]: On model.data_pipeline_poc.dim_uemoa_indicators: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.dim_uemoa_indicators"} */

  
    
        create or replace table default_silver.dim_uemoa_indicators
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Ce modèle sert de "source de vérité" propre pour tous les modèles Gold.
-- Il sélectionne depuis la source et applique les premières transformations/nettoyages.
SELECT
    date,
    pib_nominal_milliards_fcfa,
    poids_secteur_primaire_pct,
    poids_secteur_secondaire_pct,
    poids_secteur_tertiaire_pct,
    taux_croissance_reel_pib_pct,
    taux_inflation_moyen_annuel_ipc_pct,
    recettes_fiscales,
    recettes_fiscales_pct_pib,
    depenses_totales_et_prets_nets,
    solde_budgetaire_global_avec_dons,
    solde_budgetaire_global_hors_dons,
    encours_de_la_dette,
    encours_de_la_dette_pct_pib,
    exportations_biens_fob,
    importations_biens_fob,
    balance_des_biens,
    compte_transactions_courantes,
    balance_courante_sur_pib_pct,
    agregats_monnaie_masse_monetaire_m2,
    taux_couverture_emission_monetaire
    -- Ajoutez toutes les autres colonnes dont vous avez besoin
FROM
    bronze.indicateurs_economiques_uemoa
WHERE
    pib_nominal_milliards_fcfa IS NOT NULL
  
[0m12:09:48.593084 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='42P01', errorCode=0, errorMessage="org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze`.`indicateurs_economiques_uemoa` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 45 pos 4;\n'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false\n:- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@88b5be3, default_silver.dim_uemoa_indicators\n+- 'Project ['date, 'pib_nominal_milliards_fcfa, 'poids_secteur_primaire_pct, 'poids_secteur_secondaire_pct, 'poids_secteur_tertiaire_pct, 'taux_croissance_reel_pib_pct, 'taux_inflation_moyen_annuel_ipc_pct, 'recettes_fiscales, 'recettes_fiscales_pct_pib, 'depenses_totales_et_prets_nets, 'solde_budgetaire_global_avec_dons, 'solde_budgetaire_global_hors_dons, 'encours_de_la_dette, 'encours_de_la_dette_pct_pib, 'exportations_biens_fob, 'importations_biens_fob, 'balance_des_biens, 'compte_transactions_courantes, 'balance_courante_sur_pib_pct, 'agregats_monnaie_masse_monetaire_m2, 'taux_couverture_emission_monetaire]\n   +- 'Filter isnotnull('pib_nominal_milliards_fcfa)\n      +- 'UnresolvedRelation [bronze, indicateurs_economiques_uemoa], [], false\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze`.`indicateurs_economiques_uemoa` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 45 pos 4;\n'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false\n:- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@88b5be3, default_silver.dim_uemoa_indicators\n+- 'Project ['date, 'pib_nominal_milliards_fcfa, 'poids_secteur_primaire_pct, 'poids_secteur_secondaire_pct, 'poids_secteur_tertiaire_pct, 'taux_croissance_reel_pib_pct, 'taux_inflation_moyen_annuel_ipc_pct, 'recettes_fiscales, 'recettes_fiscales_pct_pib, 'depenses_totales_et_prets_nets, 'solde_budgetaire_global_avec_dons, 'solde_budgetaire_global_hors_dons, 'encours_de_la_dette, 'encours_de_la_dette_pct_pib, 'exportations_biens_fob, 'importations_biens_fob, 'balance_des_biens, 'compte_transactions_courantes, 'balance_courante_sur_pib_pct, 'agregats_monnaie_masse_monetaire_m2, 'taux_couverture_emission_monetaire]\n   +- 'Filter isnotnull('pib_nominal_milliards_fcfa)\n      +- 'UnresolvedRelation [bronze, indicateurs_economiques_uemoa], [], false\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:235)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n", taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m12:09:48.594870 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m12:09:48.596344 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.dim_uemoa_indicators"} */

  
    
        create or replace table default_silver.dim_uemoa_indicators
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Ce modèle sert de "source de vérité" propre pour tous les modèles Gold.
-- Il sélectionne depuis la source et applique les premières transformations/nettoyages.
SELECT
    date,
    pib_nominal_milliards_fcfa,
    poids_secteur_primaire_pct,
    poids_secteur_secondaire_pct,
    poids_secteur_tertiaire_pct,
    taux_croissance_reel_pib_pct,
    taux_inflation_moyen_annuel_ipc_pct,
    recettes_fiscales,
    recettes_fiscales_pct_pib,
    depenses_totales_et_prets_nets,
    solde_budgetaire_global_avec_dons,
    solde_budgetaire_global_hors_dons,
    encours_de_la_dette,
    encours_de_la_dette_pct_pib,
    exportations_biens_fob,
    importations_biens_fob,
    balance_des_biens,
    compte_transactions_courantes,
    balance_courante_sur_pib_pct,
    agregats_monnaie_masse_monetaire_m2,
    taux_couverture_emission_monetaire
    -- Ajoutez toutes les autres colonnes dont vous avez besoin
FROM
    bronze.indicateurs_economiques_uemoa
WHERE
    pib_nominal_milliards_fcfa IS NOT NULL
  
[0m12:09:48.598013 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze`.`indicateurs_economiques_uemoa` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 45 pos 4;
  'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false
  :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@88b5be3, default_silver.dim_uemoa_indicators
  +- 'Project ['date, 'pib_nominal_milliards_fcfa, 'poids_secteur_primaire_pct, 'poids_secteur_secondaire_pct, 'poids_secteur_tertiaire_pct, 'taux_croissance_reel_pib_pct, 'taux_inflation_moyen_annuel_ipc_pct, 'recettes_fiscales, 'recettes_fiscales_pct_pib, 'depenses_totales_et_prets_nets, 'solde_budgetaire_global_avec_dons, 'solde_budgetaire_global_hors_dons, 'encours_de_la_dette, 'encours_de_la_dette_pct_pib, 'exportations_biens_fob, 'importations_biens_fob, 'balance_des_biens, 'compte_transactions_courantes, 'balance_courante_sur_pib_pct, 'agregats_monnaie_masse_monetaire_m2, 'taux_couverture_emission_monetaire]
     +- 'Filter isnotnull('pib_nominal_milliards_fcfa)
        +- 'UnresolvedRelation [bronze, indicateurs_economiques_uemoa], [], false
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze`.`indicateurs_economiques_uemoa` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 45 pos 4;
  'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false
  :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@88b5be3, default_silver.dim_uemoa_indicators
  +- 'Project ['date, 'pib_nominal_milliards_fcfa, 'poids_secteur_primaire_pct, 'poids_secteur_secondaire_pct, 'poids_secteur_tertiaire_pct, 'taux_croissance_reel_pib_pct, 'taux_inflation_moyen_annuel_ipc_pct, 'recettes_fiscales, 'recettes_fiscales_pct_pib, 'depenses_totales_et_prets_nets, 'solde_budgetaire_global_avec_dons, 'solde_budgetaire_global_hors_dons, 'encours_de_la_dette, 'encours_de_la_dette_pct_pib, 'exportations_biens_fob, 'importations_biens_fob, 'balance_des_biens, 'compte_transactions_courantes, 'balance_courante_sur_pib_pct, 'agregats_monnaie_masse_monetaire_m2, 'taux_couverture_emission_monetaire]
     +- 'Filter isnotnull('pib_nominal_milliards_fcfa)
        +- 'UnresolvedRelation [bronze, indicateurs_economiques_uemoa], [], false
  
  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:87)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:235)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
  	at scala.collection.immutable.List.foreach(List.scala:431)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m12:09:48.599798 [debug] [Thread-1 (]: On model.data_pipeline_poc.dim_uemoa_indicators: ROLLBACK
[0m12:09:48.601032 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m12:09:48.602246 [debug] [Thread-1 (]: On model.data_pipeline_poc.dim_uemoa_indicators: Close
[0m12:09:48.610106 [debug] [Thread-1 (]: Runtime Error in model dim_uemoa_indicators (models/silver/dim_uemoa_indicators.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze`.`indicateurs_economiques_uemoa` cannot be found. Verify the spelling and correctness of the schema and catalog.
    If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
    To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 45 pos 4;
    'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false
    :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@88b5be3, default_silver.dim_uemoa_indicators
    +- 'Project ['date, 'pib_nominal_milliards_fcfa, 'poids_secteur_primaire_pct, 'poids_secteur_secondaire_pct, 'poids_secteur_tertiaire_pct, 'taux_croissance_reel_pib_pct, 'taux_inflation_moyen_annuel_ipc_pct, 'recettes_fiscales, 'recettes_fiscales_pct_pib, 'depenses_totales_et_prets_nets, 'solde_budgetaire_global_avec_dons, 'solde_budgetaire_global_hors_dons, 'encours_de_la_dette, 'encours_de_la_dette_pct_pib, 'exportations_biens_fob, 'importations_biens_fob, 'balance_des_biens, 'compte_transactions_courantes, 'balance_courante_sur_pib_pct, 'agregats_monnaie_masse_monetaire_m2, 'taux_couverture_emission_monetaire]
       +- 'Filter isnotnull('pib_nominal_milliards_fcfa)
          +- 'UnresolvedRelation [bronze, indicateurs_economiques_uemoa], [], false
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze`.`indicateurs_economiques_uemoa` cannot be found. Verify the spelling and correctness of the schema and catalog.
    If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
    To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 45 pos 4;
    'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false
    :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@88b5be3, default_silver.dim_uemoa_indicators
    +- 'Project ['date, 'pib_nominal_milliards_fcfa, 'poids_secteur_primaire_pct, 'poids_secteur_secondaire_pct, 'poids_secteur_tertiaire_pct, 'taux_croissance_reel_pib_pct, 'taux_inflation_moyen_annuel_ipc_pct, 'recettes_fiscales, 'recettes_fiscales_pct_pib, 'depenses_totales_et_prets_nets, 'solde_budgetaire_global_avec_dons, 'solde_budgetaire_global_hors_dons, 'encours_de_la_dette, 'encours_de_la_dette_pct_pib, 'exportations_biens_fob, 'importations_biens_fob, 'balance_des_biens, 'compte_transactions_courantes, 'balance_courante_sur_pib_pct, 'agregats_monnaie_masse_monetaire_m2, 'taux_couverture_emission_monetaire]
       +- 'Filter isnotnull('pib_nominal_milliards_fcfa)
          +- 'UnresolvedRelation [bronze, indicateurs_economiques_uemoa], [], false
    
    	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:87)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:235)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m12:09:48.613054 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c4a6abdb-b8b1-46f9-bc25-22e37981f1e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76b6ebcbf790>]}
[0m12:09:48.614530 [error] [Thread-1 (]: 1 of 9 ERROR creating sql table model default_silver.dim_uemoa_indicators ...... [[31mERROR[0m in 0.17s]
[0m12:09:48.616264 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.dim_uemoa_indicators
[0m12:09:48.617600 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.stg_events
[0m12:09:48.618346 [debug] [Thread-4 (]: Marking all children of 'model.data_pipeline_poc.dim_uemoa_indicators' to be skipped because of status 'error'.  Reason: Runtime Error in model dim_uemoa_indicators (models/silver/dim_uemoa_indicators.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze`.`indicateurs_economiques_uemoa` cannot be found. Verify the spelling and correctness of the schema and catalog.
    If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
    To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 45 pos 4;
    'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false
    :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@88b5be3, default_silver.dim_uemoa_indicators
    +- 'Project ['date, 'pib_nominal_milliards_fcfa, 'poids_secteur_primaire_pct, 'poids_secteur_secondaire_pct, 'poids_secteur_tertiaire_pct, 'taux_croissance_reel_pib_pct, 'taux_inflation_moyen_annuel_ipc_pct, 'recettes_fiscales, 'recettes_fiscales_pct_pib, 'depenses_totales_et_prets_nets, 'solde_budgetaire_global_avec_dons, 'solde_budgetaire_global_hors_dons, 'encours_de_la_dette, 'encours_de_la_dette_pct_pib, 'exportations_biens_fob, 'importations_biens_fob, 'balance_des_biens, 'compte_transactions_courantes, 'balance_courante_sur_pib_pct, 'agregats_monnaie_masse_monetaire_m2, 'taux_couverture_emission_monetaire]
       +- 'Filter isnotnull('pib_nominal_milliards_fcfa)
          +- 'UnresolvedRelation [bronze, indicateurs_economiques_uemoa], [], false
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze`.`indicateurs_economiques_uemoa` cannot be found. Verify the spelling and correctness of the schema and catalog.
    If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
    To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 45 pos 4;
    'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false
    :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@88b5be3, default_silver.dim_uemoa_indicators
    +- 'Project ['date, 'pib_nominal_milliards_fcfa, 'poids_secteur_primaire_pct, 'poids_secteur_secondaire_pct, 'poids_secteur_tertiaire_pct, 'taux_croissance_reel_pib_pct, 'taux_inflation_moyen_annuel_ipc_pct, 'recettes_fiscales, 'recettes_fiscales_pct_pib, 'depenses_totales_et_prets_nets, 'solde_budgetaire_global_avec_dons, 'solde_budgetaire_global_hors_dons, 'encours_de_la_dette, 'encours_de_la_dette_pct_pib, 'exportations_biens_fob, 'importations_biens_fob, 'balance_des_biens, 'compte_transactions_courantes, 'balance_courante_sur_pib_pct, 'agregats_monnaie_masse_monetaire_m2, 'taux_couverture_emission_monetaire]
       +- 'Filter isnotnull('pib_nominal_milliards_fcfa)
          +- 'UnresolvedRelation [bronze, indicateurs_economiques_uemoa], [], false
    
    	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:87)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:235)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    .
[0m12:09:48.619609 [info ] [Thread-1 (]: 2 of 9 START sql table model default_default_silver.stg_events ................. [RUN]
[0m12:09:48.623013 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline_poc.dim_uemoa_indicators, now model.data_pipeline_poc.stg_events)
[0m12:09:48.624257 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_poc.stg_events
[0m12:09:48.627704 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_poc.stg_events"
[0m12:09:48.642652 [debug] [Thread-1 (]: Began executing node model.data_pipeline_poc.stg_events
[0m12:09:48.646556 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_poc.stg_events"
[0m12:09:48.662404 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:09:48.663924 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.stg_events"
[0m12:09:48.665316 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_events: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.stg_events"} */

  
    
        create or replace table default_default_silver.stg_events
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Staging model for raw events from bronze layer
-- This model cleans and standardizes the raw event data

SELECT
    event_id,
    event_type,
    user_id,
    event_timestamp,
    event_data,
    current_timestamp() as dbt_loaded_at
FROM bronze.raw_events
WHERE event_id IS NOT NULL
  
[0m12:09:48.666545 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:09:51.887425 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m12:09:51.890089 [debug] [Thread-1 (]: SQL status: OK in 3.224 seconds
[0m12:09:51.904354 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_events: ROLLBACK
[0m12:09:51.906767 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m12:09:51.909050 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_events: Close
[0m12:09:51.915379 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c4a6abdb-b8b1-46f9-bc25-22e37981f1e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76b6e8532a50>]}
[0m12:09:51.918125 [info ] [Thread-1 (]: 2 of 9 OK created sql table model default_default_silver.stg_events ............ [[32mOK[0m in 3.29s]
[0m12:09:51.920248 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.stg_events
[0m12:09:51.922220 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.stg_users
[0m12:09:51.924727 [info ] [Thread-1 (]: 3 of 9 START sql table model default_default_silver.stg_users .................. [RUN]
[0m12:09:51.927098 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline_poc.stg_events, now model.data_pipeline_poc.stg_users)
[0m12:09:51.928901 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_poc.stg_users
[0m12:09:51.934343 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_poc.stg_users"
[0m12:09:51.960843 [debug] [Thread-1 (]: Began executing node model.data_pipeline_poc.stg_users
[0m12:09:51.965778 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_poc.stg_users"
[0m12:09:51.990899 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:09:51.993053 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.stg_users"
[0m12:09:51.994684 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_users: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.stg_users"} */

  
    
        create or replace table default_default_silver.stg_users
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Staging model for raw users from bronze layer
-- This model cleans and standardizes the raw user data

SELECT
    user_id,
    user_name,
    email as user_email,
    created_at,
    current_timestamp() as dbt_loaded_at
FROM bronze.raw_users
WHERE user_id IS NOT NULL
  
[0m12:09:51.996849 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:09:53.034424 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m12:09:53.036597 [debug] [Thread-1 (]: SQL status: OK in 1.040 seconds
[0m12:09:53.039468 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_users: ROLLBACK
[0m12:09:53.041975 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m12:09:53.043804 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_users: Close
[0m12:09:53.048264 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c4a6abdb-b8b1-46f9-bc25-22e37981f1e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76b6e86f2210>]}
[0m12:09:53.050638 [info ] [Thread-1 (]: 3 of 9 OK created sql table model default_default_silver.stg_users ............. [[32mOK[0m in 1.12s]
[0m12:09:53.052569 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.stg_users
[0m12:09:53.054504 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.gold_kpi_uemoa_growth_yoy
[0m12:09:53.057090 [info ] [Thread-1 (]: 4 of 9 SKIP relation default_gold.gold_kpi_uemoa_growth_yoy .................... [[33mSKIP[0m]
[0m12:09:53.058703 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.gold_kpi_uemoa_growth_yoy
[0m12:09:53.060556 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.gold_mart_uemoa_external_stability
[0m12:09:53.062604 [info ] [Thread-1 (]: 5 of 9 SKIP relation default_gold.gold_mart_uemoa_external_stability ........... [[33mSKIP[0m]
[0m12:09:53.064174 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.gold_mart_uemoa_external_stability
[0m12:09:53.065559 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.gold_mart_uemoa_external_trade
[0m12:09:53.066936 [info ] [Thread-1 (]: 6 of 9 SKIP relation default_gold.gold_mart_uemoa_external_trade ............... [[33mSKIP[0m]
[0m12:09:53.068028 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.gold_mart_uemoa_external_trade
[0m12:09:53.069365 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.gold_mart_uemoa_monetary_dashboard
[0m12:09:53.070571 [info ] [Thread-1 (]: 7 of 9 SKIP relation default_gold.gold_mart_uemoa_monetary_dashboard ........... [[33mSKIP[0m]
[0m12:09:53.071847 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.gold_mart_uemoa_monetary_dashboard
[0m12:09:53.073652 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.gold_mart_uemoa_public_finance
[0m12:09:53.075291 [info ] [Thread-1 (]: 8 of 9 SKIP relation default_gold.gold_mart_uemoa_public_finance ............... [[33mSKIP[0m]
[0m12:09:53.077455 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.gold_mart_uemoa_public_finance
[0m12:09:53.079703 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.fct_events_enriched
[0m12:09:53.081409 [info ] [Thread-1 (]: 9 of 9 START sql table model default_default_gold.fct_events_enriched .......... [RUN]
[0m12:09:53.083109 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline_poc.stg_users, now model.data_pipeline_poc.fct_events_enriched)
[0m12:09:53.084219 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_poc.fct_events_enriched
[0m12:09:53.087457 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_poc.fct_events_enriched"
[0m12:09:53.110942 [debug] [Thread-1 (]: Began executing node model.data_pipeline_poc.fct_events_enriched
[0m12:09:53.114913 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_poc.fct_events_enriched"
[0m12:09:53.141336 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:09:53.143150 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.fct_events_enriched"
[0m12:09:53.146119 [debug] [Thread-1 (]: On model.data_pipeline_poc.fct_events_enriched: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.fct_events_enriched"} */

  
    
        create or replace table default_default_gold.fct_events_enriched
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Mart model: Events enriched with user information
-- This model joins event data with user data for analytics

WITH events AS (
    SELECT * FROM default_default_silver.stg_events
),

users AS (
    SELECT * FROM default_default_silver.stg_users
),

enriched AS (
    SELECT
        e.event_id,
        e.event_type,
        e.event_timestamp,
        e.event_data,
        e.user_id,
        u.user_name,
        u.user_email,
        u.created_at as user_created_at,
        e.dbt_loaded_at
    FROM events e
    LEFT JOIN users u ON e.user_id = u.user_id
)

SELECT * FROM enriched
  
[0m12:09:53.147925 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:09:54.452484 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m12:09:54.454797 [debug] [Thread-1 (]: SQL status: OK in 1.307 seconds
[0m12:09:54.457561 [debug] [Thread-1 (]: On model.data_pipeline_poc.fct_events_enriched: ROLLBACK
[0m12:09:54.459405 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m12:09:54.461148 [debug] [Thread-1 (]: On model.data_pipeline_poc.fct_events_enriched: Close
[0m12:09:54.465372 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c4a6abdb-b8b1-46f9-bc25-22e37981f1e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76b6ebd14250>]}
[0m12:09:54.467507 [info ] [Thread-1 (]: 9 of 9 OK created sql table model default_default_gold.fct_events_enriched ..... [[32mOK[0m in 1.38s]
[0m12:09:54.469456 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.fct_events_enriched
[0m12:09:54.473610 [debug] [MainThread]: On master: ROLLBACK
[0m12:09:54.475337 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:09:54.500051 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m12:09:54.501619 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:09:54.502468 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:09:54.503246 [debug] [MainThread]: On master: ROLLBACK
[0m12:09:54.504265 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m12:09:54.505823 [debug] [MainThread]: On master: Close
[0m12:09:54.509239 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:09:54.510303 [debug] [MainThread]: Connection 'model.data_pipeline_poc.fct_events_enriched' was properly closed.
[0m12:09:54.511291 [info ] [MainThread]: 
[0m12:09:54.512286 [info ] [MainThread]: Finished running 9 table models in 0 hours 0 minutes and 6.95 seconds (6.95s).
[0m12:09:54.514013 [debug] [MainThread]: Command end result
[0m12:09:54.577415 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/app/dbt/target/manifest.json
[0m12:09:54.586806 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/app/dbt/target/semantic_manifest.json
[0m12:09:54.600260 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/app/dbt/target/run_results.json
[0m12:09:54.601431 [info ] [MainThread]: 
[0m12:09:54.603054 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m12:09:54.604660 [info ] [MainThread]: 
[0m12:09:54.606577 [error] [MainThread]:   Runtime Error in model dim_uemoa_indicators (models/silver/dim_uemoa_indicators.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze`.`indicateurs_economiques_uemoa` cannot be found. Verify the spelling and correctness of the schema and catalog.
    If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
    To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 45 pos 4;
    'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false
    :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@88b5be3, default_silver.dim_uemoa_indicators
    +- 'Project ['date, 'pib_nominal_milliards_fcfa, 'poids_secteur_primaire_pct, 'poids_secteur_secondaire_pct, 'poids_secteur_tertiaire_pct, 'taux_croissance_reel_pib_pct, 'taux_inflation_moyen_annuel_ipc_pct, 'recettes_fiscales, 'recettes_fiscales_pct_pib, 'depenses_totales_et_prets_nets, 'solde_budgetaire_global_avec_dons, 'solde_budgetaire_global_hors_dons, 'encours_de_la_dette, 'encours_de_la_dette_pct_pib, 'exportations_biens_fob, 'importations_biens_fob, 'balance_des_biens, 'compte_transactions_courantes, 'balance_courante_sur_pib_pct, 'agregats_monnaie_masse_monetaire_m2, 'taux_couverture_emission_monetaire]
       +- 'Filter isnotnull('pib_nominal_milliards_fcfa)
          +- 'UnresolvedRelation [bronze, indicateurs_economiques_uemoa], [], false
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze`.`indicateurs_economiques_uemoa` cannot be found. Verify the spelling and correctness of the schema and catalog.
    If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
    To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 45 pos 4;
    'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false
    :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@88b5be3, default_silver.dim_uemoa_indicators
    +- 'Project ['date, 'pib_nominal_milliards_fcfa, 'poids_secteur_primaire_pct, 'poids_secteur_secondaire_pct, 'poids_secteur_tertiaire_pct, 'taux_croissance_reel_pib_pct, 'taux_inflation_moyen_annuel_ipc_pct, 'recettes_fiscales, 'recettes_fiscales_pct_pib, 'depenses_totales_et_prets_nets, 'solde_budgetaire_global_avec_dons, 'solde_budgetaire_global_hors_dons, 'encours_de_la_dette, 'encours_de_la_dette_pct_pib, 'exportations_biens_fob, 'importations_biens_fob, 'balance_des_biens, 'compte_transactions_courantes, 'balance_courante_sur_pib_pct, 'agregats_monnaie_masse_monetaire_m2, 'taux_couverture_emission_monetaire]
       +- 'Filter isnotnull('pib_nominal_milliards_fcfa)
          +- 'UnresolvedRelation [bronze, indicateurs_economiques_uemoa], [], false
    
    	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:87)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:235)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m12:09:54.608247 [info ] [MainThread]: 
[0m12:09:54.609819 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=1 SKIP=5 TOTAL=9
[0m12:09:54.611686 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 11.099927, "process_in_blocks": "0", "process_kernel_time": 0.237125, "process_mem_max_rss": "112400", "process_out_blocks": "0", "process_user_time": 1.688995}
[0m12:09:54.613018 [debug] [MainThread]: Command `dbt run` failed at 12:09:54.612929 after 11.10 seconds
[0m12:09:54.614309 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76b6efe7d750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76b6efd0d490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76b6efd0d590>]}
[0m12:09:54.615875 [debug] [MainThread]: Flushing usage events
[0m12:09:58.031532 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:36:31.480988 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x716dd3e3f610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x716dd4353e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x716dd3e3ee90>]}


============================== 12:36:31.485220 | ec832284-7b2c-43ae-9283-604f170f7b12 ==============================
[0m12:36:31.485220 [info ] [MainThread]: Running with dbt=1.9.0
[0m12:36:31.487479 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/usr/app/dbt', 'log_path': '/usr/app/dbt/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt compile --select dim_uemoa_indicators', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m12:36:31.542290 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:36:31.544666 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:36:31.546735 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:36:31.644327 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ec832284-7b2c-43ae-9283-604f170f7b12', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x716dd4598b90>]}
[0m12:36:31.682102 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ec832284-7b2c-43ae-9283-604f170f7b12', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x716dd602fa10>]}
[0m12:36:31.685799 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m12:36:31.783268 [debug] [MainThread]: checksum: c99e828bba267739642b5a3ce85f17518764ea526e0e6c4fdc649171c1a66bff, vars: {}, profile: , target: , version: 1.9.0
[0m12:36:35.021306 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m12:36:35.024669 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_poc://models/silver/dim_uemoa_indicators.sql
[0m12:36:35.361486 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- seeds.data_pipeline_poc
- snapshots.data_pipeline_poc
[0m12:36:35.371762 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ec832284-7b2c-43ae-9283-604f170f7b12', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x716dd01c21d0>]}
[0m12:36:35.533225 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/app/dbt/target/manifest.json
[0m12:36:35.547759 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/app/dbt/target/semantic_manifest.json
[0m12:36:35.574716 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ec832284-7b2c-43ae-9283-604f170f7b12', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x716dd023c210>]}
[0m12:36:35.576920 [info ] [MainThread]: Found 9 models, 13 data tests, 3 sources, 583 macros
[0m12:36:35.578840 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ec832284-7b2c-43ae-9283-604f170f7b12', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x716dd01f5dd0>]}
[0m12:36:35.581663 [info ] [MainThread]: 
[0m12:36:35.583564 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:36:35.585532 [info ] [MainThread]: 
[0m12:36:35.587046 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m12:36:35.592765 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_default_default_silver'
[0m12:36:35.600499 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:36:35.602366 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m12:36:35.604560 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show table extended in default_default_silver like '*'
  
[0m12:36:35.606683 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:36:35.643468 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#562, tableName#563, isTemporary#564, information#565]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@1a5e0c8, [default_default_silver]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#562, tableName#563, isTemporary#564, information#565]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@1a5e0c8, [default_default_silver]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m12:36:35.645818 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m12:36:35.647522 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show table extended in default_default_silver like '*'
  
[0m12:36:35.649604 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#562, tableName#563, isTemporary#564, information#565]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@1a5e0c8, [default_default_silver]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#562, tableName#563, isTemporary#564, information#565]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@1a5e0c8, [default_default_silver]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m12:36:35.651367 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m12:36:35.653924 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#562, tableName#563, isTemporary#564, information#565]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@1a5e0c8, [default_default_silver]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#562, tableName#563, isTemporary#564, information#565]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@1a5e0c8, [default_default_silver]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m12:36:35.660039 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m12:36:35.662054 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show tables in default_default_silver like '*'
  
[0m12:36:35.682895 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m12:36:35.685195 [debug] [ThreadPool]: SQL status: OK in 0.021 seconds
[0m12:36:35.693608 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m12:36:35.695652 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
describe extended default_default_silver.stg_events
  
[0m12:36:35.744076 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m12:36:35.748172 [debug] [ThreadPool]: SQL status: OK in 0.050 seconds
[0m12:36:35.755575 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m12:36:35.759295 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
describe extended default_default_silver.stg_users
  
[0m12:36:35.799949 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m12:36:35.802114 [debug] [ThreadPool]: SQL status: OK in 0.041 seconds
[0m12:36:35.808787 [debug] [ThreadPool]: On list_None_default_default_silver: ROLLBACK
[0m12:36:35.811332 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m12:36:35.813353 [debug] [ThreadPool]: On list_None_default_default_silver: Close
[0m12:36:35.818586 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default_default_silver, now list_None_default_gold)
[0m12:36:35.822637 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:36:35.825556 [debug] [ThreadPool]: Using spark connection "list_None_default_gold"
[0m12:36:35.827709 [debug] [ThreadPool]: On list_None_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_gold"} */
show table extended in default_gold like '*'
  
[0m12:36:35.830163 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:36:35.862169 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#638, tableName#639, isTemporary#640, information#641]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@20e5fb9d, [default_gold]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#638, tableName#639, isTemporary#640, information#641]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@20e5fb9d, [default_gold]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m12:36:35.864420 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m12:36:35.866230 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_gold"} */
show table extended in default_gold like '*'
  
[0m12:36:35.868044 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#638, tableName#639, isTemporary#640, information#641]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@20e5fb9d, [default_gold]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#638, tableName#639, isTemporary#640, information#641]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@20e5fb9d, [default_gold]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m12:36:35.871412 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m12:36:35.873395 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#638, tableName#639, isTemporary#640, information#641]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@20e5fb9d, [default_gold]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#638, tableName#639, isTemporary#640, information#641]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@20e5fb9d, [default_gold]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m12:36:35.877835 [debug] [ThreadPool]: Using spark connection "list_None_default_gold"
[0m12:36:35.879963 [debug] [ThreadPool]: On list_None_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_gold"} */
show tables in default_gold like '*'
  
[0m12:36:35.896715 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m12:36:35.900415 [debug] [ThreadPool]: SQL status: OK in 0.018 seconds
[0m12:36:35.905190 [debug] [ThreadPool]: On list_None_default_gold: ROLLBACK
[0m12:36:35.907732 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m12:36:35.908962 [debug] [ThreadPool]: On list_None_default_gold: Close
[0m12:36:35.912969 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default_gold, now list_None_default_default_gold)
[0m12:36:35.916019 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:36:35.917678 [debug] [ThreadPool]: Using spark connection "list_None_default_default_gold"
[0m12:36:35.919365 [debug] [ThreadPool]: On list_None_default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show table extended in default_default_gold like '*'
  
[0m12:36:35.921304 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:36:35.947917 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#653, tableName#654, isTemporary#655, information#656]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@53351b8, [default_default_gold]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#653, tableName#654, isTemporary#655, information#656]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@53351b8, [default_default_gold]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m12:36:35.949975 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m12:36:35.951930 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show table extended in default_default_gold like '*'
  
[0m12:36:35.954085 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#653, tableName#654, isTemporary#655, information#656]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@53351b8, [default_default_gold]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#653, tableName#654, isTemporary#655, information#656]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@53351b8, [default_default_gold]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m12:36:35.956031 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m12:36:35.957800 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#653, tableName#654, isTemporary#655, information#656]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@53351b8, [default_default_gold]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#653, tableName#654, isTemporary#655, information#656]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@53351b8, [default_default_gold]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m12:36:35.960964 [debug] [ThreadPool]: Using spark connection "list_None_default_default_gold"
[0m12:36:35.962845 [debug] [ThreadPool]: On list_None_default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show tables in default_default_gold like '*'
  
[0m12:36:35.983246 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m12:36:35.985347 [debug] [ThreadPool]: SQL status: OK in 0.021 seconds
[0m12:36:35.992184 [debug] [ThreadPool]: Using spark connection "list_None_default_default_gold"
[0m12:36:35.994323 [debug] [ThreadPool]: On list_None_default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
describe extended default_default_gold.fct_events_enriched
  
[0m12:36:36.032547 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m12:36:36.034850 [debug] [ThreadPool]: SQL status: OK in 0.039 seconds
[0m12:36:36.041731 [debug] [ThreadPool]: On list_None_default_default_gold: ROLLBACK
[0m12:36:36.044058 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m12:36:36.046160 [debug] [ThreadPool]: On list_None_default_default_gold: Close
[0m12:36:36.051288 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default_default_gold, now list_None_default_silver)
[0m12:36:36.054446 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:36:36.056230 [debug] [ThreadPool]: Using spark connection "list_None_default_silver"
[0m12:36:36.058449 [debug] [ThreadPool]: On list_None_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_silver"} */
show table extended in default_silver like '*'
  
[0m12:36:36.060527 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:36:36.090446 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#707, tableName#708, isTemporary#709, information#710]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@4810f61f, [default_silver]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#707, tableName#708, isTemporary#709, information#710]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@4810f61f, [default_silver]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m12:36:36.092939 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m12:36:36.094951 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_silver"} */
show table extended in default_silver like '*'
  
[0m12:36:36.097305 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#707, tableName#708, isTemporary#709, information#710]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@4810f61f, [default_silver]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#707, tableName#708, isTemporary#709, information#710]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@4810f61f, [default_silver]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m12:36:36.099519 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m12:36:36.101748 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#707, tableName#708, isTemporary#709, information#710]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@4810f61f, [default_silver]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#707, tableName#708, isTemporary#709, information#710]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@4810f61f, [default_silver]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m12:36:36.106042 [debug] [ThreadPool]: Using spark connection "list_None_default_silver"
[0m12:36:36.109476 [debug] [ThreadPool]: On list_None_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_silver"} */
show tables in default_silver like '*'
  
[0m12:36:36.130149 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m12:36:36.132419 [debug] [ThreadPool]: SQL status: OK in 0.021 seconds
[0m12:36:36.136037 [debug] [ThreadPool]: On list_None_default_silver: ROLLBACK
[0m12:36:36.138180 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m12:36:36.140008 [debug] [ThreadPool]: On list_None_default_silver: Close
[0m12:36:36.145708 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ec832284-7b2c-43ae-9283-604f170f7b12', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x716dd03202d0>]}
[0m12:36:36.149922 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.dim_uemoa_indicators
[0m12:36:36.152098 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default_silver, now model.data_pipeline_poc.dim_uemoa_indicators)
[0m12:36:36.154792 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_poc.dim_uemoa_indicators
[0m12:36:36.161017 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_poc.dim_uemoa_indicators"
[0m12:36:36.185638 [debug] [Thread-1 (]: Began executing node model.data_pipeline_poc.dim_uemoa_indicators
[0m12:36:36.188863 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.dim_uemoa_indicators
[0m12:36:36.191782 [debug] [Thread-1 (]: Began running node test.data_pipeline_poc.dbt_utils_expression_is_true_dim_uemoa_indicators_ABS_poids_secteur_primaire_pct_poids_secteur_secondaire_pct_poids_secteur_tertiaire_pct_100_0_1.566c4d7b3d
[0m12:36:36.193873 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline_poc.dim_uemoa_indicators, now test.data_pipeline_poc.dbt_utils_expression_is_true_dim_uemoa_indicators_ABS_poids_secteur_primaire_pct_poids_secteur_secondaire_pct_poids_secteur_tertiaire_pct_100_0_1.566c4d7b3d)
[0m12:36:36.195825 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_poc.dbt_utils_expression_is_true_dim_uemoa_indicators_ABS_poids_secteur_primaire_pct_poids_secteur_secondaire_pct_poids_secteur_tertiaire_pct_100_0_1.566c4d7b3d
[0m12:36:36.202231 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_poc.dbt_utils_expression_is_true_dim_uemoa_indicators_ABS_poids_secteur_primaire_pct_poids_secteur_secondaire_pct_poids_secteur_tertiaire_pct_100_0_1.566c4d7b3d"
[0m12:36:36.241293 [debug] [Thread-1 (]: Began executing node test.data_pipeline_poc.dbt_utils_expression_is_true_dim_uemoa_indicators_ABS_poids_secteur_primaire_pct_poids_secteur_secondaire_pct_poids_secteur_tertiaire_pct_100_0_1.566c4d7b3d
[0m12:36:36.243502 [debug] [Thread-1 (]: Finished running node test.data_pipeline_poc.dbt_utils_expression_is_true_dim_uemoa_indicators_ABS_poids_secteur_primaire_pct_poids_secteur_secondaire_pct_poids_secteur_tertiaire_pct_100_0_1.566c4d7b3d
[0m12:36:36.245449 [debug] [Thread-1 (]: Began running node test.data_pipeline_poc.dbt_utils_expression_is_true_dim_uemoa_indicators_encours_de_la_dette_pct_pib___0.9b25d583bc
[0m12:36:36.247986 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_poc.dbt_utils_expression_is_true_dim_uemoa_indicators_ABS_poids_secteur_primaire_pct_poids_secteur_secondaire_pct_poids_secteur_tertiaire_pct_100_0_1.566c4d7b3d, now test.data_pipeline_poc.dbt_utils_expression_is_true_dim_uemoa_indicators_encours_de_la_dette_pct_pib___0.9b25d583bc)
[0m12:36:36.250177 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_poc.dbt_utils_expression_is_true_dim_uemoa_indicators_encours_de_la_dette_pct_pib___0.9b25d583bc
[0m12:36:36.255066 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_poc.dbt_utils_expression_is_true_dim_uemoa_indicators_encours_de_la_dette_pct_pib___0.9b25d583bc"
[0m12:36:36.284478 [debug] [Thread-1 (]: Began executing node test.data_pipeline_poc.dbt_utils_expression_is_true_dim_uemoa_indicators_encours_de_la_dette_pct_pib___0.9b25d583bc
[0m12:36:36.286844 [debug] [Thread-1 (]: Finished running node test.data_pipeline_poc.dbt_utils_expression_is_true_dim_uemoa_indicators_encours_de_la_dette_pct_pib___0.9b25d583bc
[0m12:36:36.288397 [debug] [Thread-1 (]: Began running node test.data_pipeline_poc.dbt_utils_expression_is_true_dim_uemoa_indicators_pib_nominal_milliards_fcfa___0.6bf7624fbb
[0m12:36:36.290832 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_poc.dbt_utils_expression_is_true_dim_uemoa_indicators_encours_de_la_dette_pct_pib___0.9b25d583bc, now test.data_pipeline_poc.dbt_utils_expression_is_true_dim_uemoa_indicators_pib_nominal_milliards_fcfa___0.6bf7624fbb)
[0m12:36:36.292926 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_poc.dbt_utils_expression_is_true_dim_uemoa_indicators_pib_nominal_milliards_fcfa___0.6bf7624fbb
[0m12:36:36.297736 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_poc.dbt_utils_expression_is_true_dim_uemoa_indicators_pib_nominal_milliards_fcfa___0.6bf7624fbb"
[0m12:36:36.320657 [debug] [Thread-1 (]: Began executing node test.data_pipeline_poc.dbt_utils_expression_is_true_dim_uemoa_indicators_pib_nominal_milliards_fcfa___0.6bf7624fbb
[0m12:36:36.324533 [debug] [Thread-1 (]: Finished running node test.data_pipeline_poc.dbt_utils_expression_is_true_dim_uemoa_indicators_pib_nominal_milliards_fcfa___0.6bf7624fbb
[0m12:36:36.326694 [debug] [Thread-1 (]: Began running node test.data_pipeline_poc.dbt_utils_expression_is_true_dim_uemoa_indicators_taux_croissance_reel_pib_pct__BETWEEN_20_AND_20.20fa8bcb37
[0m12:36:36.329086 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_poc.dbt_utils_expression_is_true_dim_uemoa_indicators_pib_nominal_milliards_fcfa___0.6bf7624fbb, now test.data_pipeline_poc.dbt_utils_expression_is_true_dim_uemoa_indicators_taux_croissance_reel_pib_pct__BETWEEN_20_AND_20.20fa8bcb37)
[0m12:36:36.331100 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_poc.dbt_utils_expression_is_true_dim_uemoa_indicators_taux_croissance_reel_pib_pct__BETWEEN_20_AND_20.20fa8bcb37
[0m12:36:36.335487 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_poc.dbt_utils_expression_is_true_dim_uemoa_indicators_taux_croissance_reel_pib_pct__BETWEEN_20_AND_20.20fa8bcb37"
[0m12:36:36.366972 [debug] [Thread-1 (]: Began executing node test.data_pipeline_poc.dbt_utils_expression_is_true_dim_uemoa_indicators_taux_croissance_reel_pib_pct__BETWEEN_20_AND_20.20fa8bcb37
[0m12:36:36.369792 [debug] [Thread-1 (]: Finished running node test.data_pipeline_poc.dbt_utils_expression_is_true_dim_uemoa_indicators_taux_croissance_reel_pib_pct__BETWEEN_20_AND_20.20fa8bcb37
[0m12:36:36.373012 [debug] [Thread-1 (]: Began running node test.data_pipeline_poc.not_null_dim_uemoa_indicators_date.1d58bfc31b
[0m12:36:36.375536 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_poc.dbt_utils_expression_is_true_dim_uemoa_indicators_taux_croissance_reel_pib_pct__BETWEEN_20_AND_20.20fa8bcb37, now test.data_pipeline_poc.not_null_dim_uemoa_indicators_date.1d58bfc31b)
[0m12:36:36.378076 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_poc.not_null_dim_uemoa_indicators_date.1d58bfc31b
[0m12:36:36.384511 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_poc.not_null_dim_uemoa_indicators_date.1d58bfc31b"
[0m12:36:36.410962 [debug] [Thread-1 (]: Began executing node test.data_pipeline_poc.not_null_dim_uemoa_indicators_date.1d58bfc31b
[0m12:36:36.413573 [debug] [Thread-1 (]: Finished running node test.data_pipeline_poc.not_null_dim_uemoa_indicators_date.1d58bfc31b
[0m12:36:36.415837 [debug] [Thread-1 (]: Began running node test.data_pipeline_poc.not_null_dim_uemoa_indicators_pib_nominal_milliards_fcfa.7b3591dc71
[0m12:36:36.418226 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_poc.not_null_dim_uemoa_indicators_date.1d58bfc31b, now test.data_pipeline_poc.not_null_dim_uemoa_indicators_pib_nominal_milliards_fcfa.7b3591dc71)
[0m12:36:36.420354 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_poc.not_null_dim_uemoa_indicators_pib_nominal_milliards_fcfa.7b3591dc71
[0m12:36:36.426195 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_poc.not_null_dim_uemoa_indicators_pib_nominal_milliards_fcfa.7b3591dc71"
[0m12:36:36.451118 [debug] [Thread-1 (]: Began executing node test.data_pipeline_poc.not_null_dim_uemoa_indicators_pib_nominal_milliards_fcfa.7b3591dc71
[0m12:36:36.453707 [debug] [Thread-1 (]: Finished running node test.data_pipeline_poc.not_null_dim_uemoa_indicators_pib_nominal_milliards_fcfa.7b3591dc71
[0m12:36:36.455870 [debug] [Thread-1 (]: Began running node test.data_pipeline_poc.unique_dim_uemoa_indicators_date.3ae1bde656
[0m12:36:36.457816 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_poc.not_null_dim_uemoa_indicators_pib_nominal_milliards_fcfa.7b3591dc71, now test.data_pipeline_poc.unique_dim_uemoa_indicators_date.3ae1bde656)
[0m12:36:36.459875 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_poc.unique_dim_uemoa_indicators_date.3ae1bde656
[0m12:36:36.465384 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_poc.unique_dim_uemoa_indicators_date.3ae1bde656"
[0m12:36:36.492167 [debug] [Thread-1 (]: Began executing node test.data_pipeline_poc.unique_dim_uemoa_indicators_date.3ae1bde656
[0m12:36:36.494516 [debug] [Thread-1 (]: Finished running node test.data_pipeline_poc.unique_dim_uemoa_indicators_date.3ae1bde656
[0m12:36:36.497457 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:36:36.499571 [debug] [MainThread]: Connection 'test.data_pipeline_poc.unique_dim_uemoa_indicators_date.3ae1bde656' was properly closed.
[0m12:36:36.501762 [debug] [MainThread]: Command end result
[0m12:36:36.583864 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/app/dbt/target/manifest.json
[0m12:36:36.597793 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/app/dbt/target/semantic_manifest.json
[0m12:36:36.613525 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/app/dbt/target/run_results.json
[0m12:36:36.615361 [debug] [MainThread]: Excluded node 'dbt_utils_expression_is_true_dim_uemoa_indicators_ABS_poids_secteur_primaire_pct_poids_secteur_secondaire_pct_poids_secteur_tertiaire_pct_100_0_1' from results
[0m12:36:36.616894 [debug] [MainThread]: Excluded node 'dbt_utils_expression_is_true_dim_uemoa_indicators_encours_de_la_dette_pct_pib___0' from results
[0m12:36:36.618530 [debug] [MainThread]: Excluded node 'dbt_utils_expression_is_true_dim_uemoa_indicators_pib_nominal_milliards_fcfa___0' from results
[0m12:36:36.621134 [debug] [MainThread]: Excluded node 'dbt_utils_expression_is_true_dim_uemoa_indicators_taux_croissance_reel_pib_pct__BETWEEN_20_AND_20' from results
[0m12:36:36.622749 [debug] [MainThread]: Excluded node 'not_null_dim_uemoa_indicators_date' from results
[0m12:36:36.624615 [debug] [MainThread]: Excluded node 'not_null_dim_uemoa_indicators_pib_nominal_milliards_fcfa' from results
[0m12:36:36.626459 [debug] [MainThread]: Excluded node 'unique_dim_uemoa_indicators_date' from results
[0m12:36:36.628465 [info ] [MainThread]: Compiled node 'dim_uemoa_indicators' is:


-- Ce modèle lit directement le fichier Parquet d'Airbyte et le transforme en table Iceberg Silver
-- Le fichier Parquet est généré par Airbyte dans lakehouse/bronze/indicateurs_economiques_uemoa/
WITH source_data AS (
    SELECT *
    FROM parquet.`s3a://lakehouse/bronze/indicateurs_economiques_uemoa/2025_10_28_1761647715112_0.parquet`
)

SELECT
    date,
    pib_nominal_milliards_fcfa,
    poids_secteur_primaire_pct,
    poids_secteur_secondaire_pct,
    poids_secteur_tertiaire_pct,
    taux_croissance_reel_pib_pct,
    taux_inflation_moyen_annuel_ipc_pct,
    recettes_fiscales,
    recettes_fiscales_pct_pib,
    depenses_totales_et_prets_nets,
    solde_budgetaire_global_avec_dons,
    solde_budgetaire_global_hors_dons,
    encours_de_la_dette,
    encours_de_la_dette_pct_pib,
    exportations_biens_fob,
    importations_biens_fob,
    balance_des_biens,
    compte_transactions_courantes,
    balance_courante_sur_pib_pct,
    agregats_monnaie_masse_monetaire_m2,
    taux_couverture_emission_monetaire
FROM source_data
WHERE
    pib_nominal_milliards_fcfa IS NOT NULL
[0m12:36:36.631193 [debug] [MainThread]: Resource report: {"command_name": "compile", "command_success": true, "command_wall_clock_time": 5.2337923, "process_in_blocks": "0", "process_kernel_time": 0.320564, "process_mem_max_rss": "110896", "process_out_blocks": "0", "process_user_time": 1.647456}
[0m12:36:36.633230 [debug] [MainThread]: Command `dbt compile` succeeded at 12:36:36.633145 after 5.24 seconds
[0m12:36:36.635302 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x716dd7935810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x716dd7668e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x716dd77c4b90>]}
[0m12:36:36.637374 [debug] [MainThread]: Flushing usage events
[0m12:36:37.739280 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:38:43.650815 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d70e2283cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d70e22e3dd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d70e22e3ed0>]}


============================== 12:38:43.656580 | ea3764f0-608c-4bed-8b9c-c504ea938f1e ==============================
[0m12:38:43.656580 [info ] [MainThread]: Running with dbt=1.9.0
[0m12:38:43.658975 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/app/dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': '/usr/app/dbt/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt run --select dim_uemoa_indicators', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m12:38:43.712458 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:38:43.715611 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:38:43.719033 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:38:43.817080 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ea3764f0-608c-4bed-8b9c-c504ea938f1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d70e1c6ac50>]}
[0m12:38:43.856029 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ea3764f0-608c-4bed-8b9c-c504ea938f1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d70e309fe50>]}
[0m12:38:43.858584 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m12:38:43.949854 [debug] [MainThread]: checksum: c99e828bba267739642b5a3ce85f17518764ea526e0e6c4fdc649171c1a66bff, vars: {}, profile: , target: , version: 1.9.0
[0m12:38:47.237244 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m12:38:47.238981 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m12:38:47.244956 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- seeds.data_pipeline_poc
- snapshots.data_pipeline_poc
[0m12:38:47.270080 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ea3764f0-608c-4bed-8b9c-c504ea938f1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d70e18b7b10>]}
[0m12:38:47.384806 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/app/dbt/target/manifest.json
[0m12:38:47.395583 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/app/dbt/target/semantic_manifest.json
[0m12:38:47.421198 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ea3764f0-608c-4bed-8b9c-c504ea938f1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d70e1674310>]}
[0m12:38:47.423038 [info ] [MainThread]: Found 9 models, 13 data tests, 3 sources, 583 macros
[0m12:38:47.424749 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ea3764f0-608c-4bed-8b9c-c504ea938f1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d70e1b96d90>]}
[0m12:38:47.427841 [info ] [MainThread]: 
[0m12:38:47.429715 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:38:47.431622 [info ] [MainThread]: 
[0m12:38:47.433391 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m12:38:47.435371 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m12:38:47.444467 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m12:38:47.447229 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m12:38:47.449248 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:38:47.493574 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m12:38:47.496754 [debug] [ThreadPool]: SQL status: OK in 0.047 seconds
[0m12:38:47.501600 [debug] [ThreadPool]: On list_schemas: Close
[0m12:38:47.510989 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default_default_gold)
[0m12:38:47.516687 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:38:47.518379 [debug] [ThreadPool]: Using spark connection "list_None_default_default_gold"
[0m12:38:47.520286 [debug] [ThreadPool]: On list_None_default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show table extended in default_default_gold like '*'
  
[0m12:38:47.521972 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:38:47.550986 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#731, tableName#732, isTemporary#733, information#734]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@1ad0cb6b, [default_default_gold]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#731, tableName#732, isTemporary#733, information#734]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@1ad0cb6b, [default_default_gold]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m12:38:47.553834 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m12:38:47.555947 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show table extended in default_default_gold like '*'
  
[0m12:38:47.558412 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#731, tableName#732, isTemporary#733, information#734]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@1ad0cb6b, [default_default_gold]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#731, tableName#732, isTemporary#733, information#734]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@1ad0cb6b, [default_default_gold]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m12:38:47.560980 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m12:38:47.563575 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#731, tableName#732, isTemporary#733, information#734]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@1ad0cb6b, [default_default_gold]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#731, tableName#732, isTemporary#733, information#734]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@1ad0cb6b, [default_default_gold]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m12:38:47.568580 [debug] [ThreadPool]: Using spark connection "list_None_default_default_gold"
[0m12:38:47.570345 [debug] [ThreadPool]: On list_None_default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show tables in default_default_gold like '*'
  
[0m12:38:47.589454 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m12:38:47.592059 [debug] [ThreadPool]: SQL status: OK in 0.020 seconds
[0m12:38:47.599023 [debug] [ThreadPool]: Using spark connection "list_None_default_default_gold"
[0m12:38:47.602245 [debug] [ThreadPool]: On list_None_default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
describe extended default_default_gold.fct_events_enriched
  
[0m12:38:47.644645 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m12:38:47.647531 [debug] [ThreadPool]: SQL status: OK in 0.043 seconds
[0m12:38:47.652310 [debug] [ThreadPool]: On list_None_default_default_gold: ROLLBACK
[0m12:38:47.654699 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m12:38:47.656752 [debug] [ThreadPool]: On list_None_default_default_gold: Close
[0m12:38:47.662733 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default_default_gold, now list_None_default_gold)
[0m12:38:47.666611 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:38:47.668626 [debug] [ThreadPool]: Using spark connection "list_None_default_gold"
[0m12:38:47.670585 [debug] [ThreadPool]: On list_None_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_gold"} */
show table extended in default_gold like '*'
  
[0m12:38:47.672552 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:38:47.707483 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#785, tableName#786, isTemporary#787, information#788]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@5929f50b, [default_gold]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#785, tableName#786, isTemporary#787, information#788]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@5929f50b, [default_gold]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m12:38:47.710342 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m12:38:47.712650 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_gold"} */
show table extended in default_gold like '*'
  
[0m12:38:47.714962 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#785, tableName#786, isTemporary#787, information#788]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@5929f50b, [default_gold]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#785, tableName#786, isTemporary#787, information#788]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@5929f50b, [default_gold]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m12:38:47.716910 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m12:38:47.718841 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#785, tableName#786, isTemporary#787, information#788]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@5929f50b, [default_gold]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#785, tableName#786, isTemporary#787, information#788]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@5929f50b, [default_gold]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m12:38:47.721973 [debug] [ThreadPool]: Using spark connection "list_None_default_gold"
[0m12:38:47.723710 [debug] [ThreadPool]: On list_None_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_gold"} */
show tables in default_gold like '*'
  
[0m12:38:47.739903 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m12:38:47.742030 [debug] [ThreadPool]: SQL status: OK in 0.016 seconds
[0m12:38:47.745667 [debug] [ThreadPool]: On list_None_default_gold: ROLLBACK
[0m12:38:47.747626 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m12:38:47.749669 [debug] [ThreadPool]: On list_None_default_gold: Close
[0m12:38:47.754330 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default_gold, now list_None_default_silver)
[0m12:38:47.758577 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:38:47.760655 [debug] [ThreadPool]: Using spark connection "list_None_default_silver"
[0m12:38:47.763371 [debug] [ThreadPool]: On list_None_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_silver"} */
show table extended in default_silver like '*'
  
[0m12:38:47.765363 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:38:47.792293 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#800, tableName#801, isTemporary#802, information#803]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7b670c85, [default_silver]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#800, tableName#801, isTemporary#802, information#803]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7b670c85, [default_silver]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m12:38:47.796165 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m12:38:47.798191 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_silver"} */
show table extended in default_silver like '*'
  
[0m12:38:47.800665 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#800, tableName#801, isTemporary#802, information#803]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7b670c85, [default_silver]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#800, tableName#801, isTemporary#802, information#803]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7b670c85, [default_silver]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m12:38:47.802798 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m12:38:47.805196 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#800, tableName#801, isTemporary#802, information#803]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7b670c85, [default_silver]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#800, tableName#801, isTemporary#802, information#803]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7b670c85, [default_silver]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m12:38:47.808389 [debug] [ThreadPool]: Using spark connection "list_None_default_silver"
[0m12:38:47.810045 [debug] [ThreadPool]: On list_None_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_silver"} */
show tables in default_silver like '*'
  
[0m12:38:47.828193 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m12:38:47.830918 [debug] [ThreadPool]: SQL status: OK in 0.018 seconds
[0m12:38:47.835257 [debug] [ThreadPool]: On list_None_default_silver: ROLLBACK
[0m12:38:47.837232 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m12:38:47.839166 [debug] [ThreadPool]: On list_None_default_silver: Close
[0m12:38:47.843319 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default_silver, now list_None_default_default_silver)
[0m12:38:47.847442 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:38:47.849093 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m12:38:47.851134 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show table extended in default_default_silver like '*'
  
[0m12:38:47.852933 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:38:47.880392 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#815, tableName#816, isTemporary#817, information#818]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@30218a03, [default_default_silver]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#815, tableName#816, isTemporary#817, information#818]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@30218a03, [default_default_silver]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m12:38:47.882833 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m12:38:47.884616 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show table extended in default_default_silver like '*'
  
[0m12:38:47.886777 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#815, tableName#816, isTemporary#817, information#818]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@30218a03, [default_default_silver]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#815, tableName#816, isTemporary#817, information#818]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@30218a03, [default_default_silver]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m12:38:47.889190 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m12:38:47.891450 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#815, tableName#816, isTemporary#817, information#818]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@30218a03, [default_default_silver]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#815, tableName#816, isTemporary#817, information#818]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@30218a03, [default_default_silver]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m12:38:47.895040 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m12:38:47.898063 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show tables in default_default_silver like '*'
  
[0m12:38:47.918473 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m12:38:47.920589 [debug] [ThreadPool]: SQL status: OK in 0.020 seconds
[0m12:38:47.926171 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m12:38:47.929875 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
describe extended default_default_silver.stg_events
  
[0m12:38:47.968446 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m12:38:47.970896 [debug] [ThreadPool]: SQL status: OK in 0.039 seconds
[0m12:38:47.979016 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m12:38:47.981390 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
describe extended default_default_silver.stg_users
  
[0m12:38:48.021393 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m12:38:48.024278 [debug] [ThreadPool]: SQL status: OK in 0.041 seconds
[0m12:38:48.028484 [debug] [ThreadPool]: On list_None_default_default_silver: ROLLBACK
[0m12:38:48.030615 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m12:38:48.032442 [debug] [ThreadPool]: On list_None_default_default_silver: Close
[0m12:38:48.037751 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ea3764f0-608c-4bed-8b9c-c504ea938f1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d70e27e2a90>]}
[0m12:38:48.039924 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:38:48.042132 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:38:48.046028 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.dim_uemoa_indicators
[0m12:38:48.048115 [info ] [Thread-1 (]: 1 of 1 START sql table model default_silver.dim_uemoa_indicators ............... [RUN]
[0m12:38:48.050353 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default_default_silver, now model.data_pipeline_poc.dim_uemoa_indicators)
[0m12:38:48.052291 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_poc.dim_uemoa_indicators
[0m12:38:48.058742 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_poc.dim_uemoa_indicators"
[0m12:38:48.083039 [debug] [Thread-1 (]: Began executing node model.data_pipeline_poc.dim_uemoa_indicators
[0m12:38:48.096640 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.dim_uemoa_indicators"
[0m12:38:48.099511 [debug] [Thread-1 (]: On model.data_pipeline_poc.dim_uemoa_indicators: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.dim_uemoa_indicators"} */
drop table if exists default_silver.dim_uemoa_indicators
[0m12:38:48.101558 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:38:48.136685 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m12:38:48.139287 [debug] [Thread-1 (]: SQL status: OK in 0.037 seconds
[0m12:38:48.165177 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_poc.dim_uemoa_indicators"
[0m12:38:48.187492 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:38:48.189614 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.dim_uemoa_indicators"
[0m12:38:48.191845 [debug] [Thread-1 (]: On model.data_pipeline_poc.dim_uemoa_indicators: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.dim_uemoa_indicators"} */

  
    
        create or replace table default_silver.dim_uemoa_indicators
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Ce modèle lit directement le fichier Parquet d'Airbyte et le transforme en table Iceberg Silver
-- Le fichier Parquet est généré par Airbyte dans lakehouse/bronze/indicateurs_economiques_uemoa/
WITH source_data AS (
    SELECT *
    FROM parquet.`s3a://lakehouse/bronze/indicateurs_economiques_uemoa/2025_10_28_1761647715112_0.parquet`
)

SELECT
    date,
    pib_nominal_milliards_fcfa,
    poids_secteur_primaire_pct,
    poids_secteur_secondaire_pct,
    poids_secteur_tertiaire_pct,
    taux_croissance_reel_pib_pct,
    taux_inflation_moyen_annuel_ipc_pct,
    recettes_fiscales,
    recettes_fiscales_pct_pib,
    depenses_totales_et_prets_nets,
    solde_budgetaire_global_avec_dons,
    solde_budgetaire_global_hors_dons,
    encours_de_la_dette,
    encours_de_la_dette_pct_pib,
    exportations_biens_fob,
    importations_biens_fob,
    balance_des_biens,
    compte_transactions_courantes,
    balance_courante_sur_pib_pct,
    agregats_monnaie_masse_monetaire_m2,
    taux_couverture_emission_monetaire
FROM source_data
WHERE
    pib_nominal_milliards_fcfa IS NOT NULL
  
[0m12:38:48.210819 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] org.apache.spark.sql.AnalysisException: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: parquet; line 23 pos 9\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: parquet; line 23 pos 9\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:66)\n\tat org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:87)\n\tat org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:63)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:1676)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Filter.mapChildren(basicLogicalOperators.scala:316)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)\n\tat scala.collection.immutable.List.map(List.scala:297)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:699)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:63)\n\tat org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:43)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\nCaused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)\n\tat org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:79)\n\t... 115 more\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n\t... 128 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m12:38:48.214875 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m12:38:48.217541 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.dim_uemoa_indicators"} */

  
    
        create or replace table default_silver.dim_uemoa_indicators
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Ce modèle lit directement le fichier Parquet d'Airbyte et le transforme en table Iceberg Silver
-- Le fichier Parquet est généré par Airbyte dans lakehouse/bronze/indicateurs_economiques_uemoa/
WITH source_data AS (
    SELECT *
    FROM parquet.`s3a://lakehouse/bronze/indicateurs_economiques_uemoa/2025_10_28_1761647715112_0.parquet`
)

SELECT
    date,
    pib_nominal_milliards_fcfa,
    poids_secteur_primaire_pct,
    poids_secteur_secondaire_pct,
    poids_secteur_tertiaire_pct,
    taux_croissance_reel_pib_pct,
    taux_inflation_moyen_annuel_ipc_pct,
    recettes_fiscales,
    recettes_fiscales_pct_pib,
    depenses_totales_et_prets_nets,
    solde_budgetaire_global_avec_dons,
    solde_budgetaire_global_hors_dons,
    encours_de_la_dette,
    encours_de_la_dette_pct_pib,
    exportations_biens_fob,
    importations_biens_fob,
    balance_des_biens,
    compte_transactions_courantes,
    balance_courante_sur_pib_pct,
    agregats_monnaie_masse_monetaire_m2,
    taux_couverture_emission_monetaire
FROM source_data
WHERE
    pib_nominal_milliards_fcfa IS NOT NULL
  
[0m12:38:48.220653 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] org.apache.spark.sql.AnalysisException: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: parquet; line 23 pos 9
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: parquet; line 23 pos 9
  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:66)
  	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:87)
  	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:63)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
  	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
  	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
  	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
  	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
  	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
  	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
  	at org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:1676)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
  	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
  	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
  	at org.apache.spark.sql.catalyst.plans.logical.Filter.mapChildren(basicLogicalOperators.scala:316)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
  	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
  	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
  	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
  	at scala.collection.immutable.List.map(List.scala:297)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:699)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
  	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:63)
  	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:43)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
  	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
  	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
  	at scala.collection.immutable.List.foldLeft(List.scala:91)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
  	at scala.collection.immutable.List.foreach(List.scala:431)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
  	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
  	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)
  	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)
  	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
  	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
  	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
  	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
  	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
  	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
  	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)
  	at scala.collection.immutable.List.map(List.scala:293)
  	at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)
  	at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)
  	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)
  	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:79)
  	... 115 more
  Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
  	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)
  	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)
  	... 128 more
  
[0m12:38:48.223444 [debug] [Thread-1 (]: On model.data_pipeline_poc.dim_uemoa_indicators: ROLLBACK
[0m12:38:48.225838 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m12:38:48.228343 [debug] [Thread-1 (]: On model.data_pipeline_poc.dim_uemoa_indicators: Close
[0m12:38:48.238523 [debug] [Thread-1 (]: Runtime Error in model dim_uemoa_indicators (models/silver/dim_uemoa_indicators.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] org.apache.spark.sql.AnalysisException: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: parquet; line 23 pos 9
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: parquet; line 23 pos 9
    	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:66)
    	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:87)
    	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:63)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
    	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
    	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
    	at org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:1676)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
    	at org.apache.spark.sql.catalyst.plans.logical.Filter.mapChildren(basicLogicalOperators.scala:316)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
    	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
    	at scala.collection.immutable.List.map(List.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:699)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
    	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:63)
    	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:43)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
    	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
    	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
    	at scala.collection.immutable.List.foldLeft(List.scala:91)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
    	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
    	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)
    	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)
    	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
    	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
    	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
    	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
    	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
    	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
    	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)
    	at scala.collection.immutable.List.map(List.scala:293)
    	at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)
    	at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)
    	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)
    	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:79)
    	... 115 more
    Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
    	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)
    	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)
    	... 128 more
    
[0m12:38:48.243032 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ea3764f0-608c-4bed-8b9c-c504ea938f1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d70e0d1bf10>]}
[0m12:38:48.246026 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model default_silver.dim_uemoa_indicators ...... [[31mERROR[0m in 0.19s]
[0m12:38:48.249390 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.dim_uemoa_indicators
[0m12:38:48.252508 [debug] [Thread-4 (]: Marking all children of 'model.data_pipeline_poc.dim_uemoa_indicators' to be skipped because of status 'error'.  Reason: Runtime Error in model dim_uemoa_indicators (models/silver/dim_uemoa_indicators.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] org.apache.spark.sql.AnalysisException: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: parquet; line 23 pos 9
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: parquet; line 23 pos 9
    	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:66)
    	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:87)
    	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:63)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
    	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
    	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
    	at org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:1676)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
    	at org.apache.spark.sql.catalyst.plans.logical.Filter.mapChildren(basicLogicalOperators.scala:316)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
    	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
    	at scala.collection.immutable.List.map(List.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:699)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
    	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:63)
    	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:43)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
    	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
    	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
    	at scala.collection.immutable.List.foldLeft(List.scala:91)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
    	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
    	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)
    	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)
    	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
    	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
    	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
    	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
    	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
    	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
    	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)
    	at scala.collection.immutable.List.map(List.scala:293)
    	at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)
    	at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)
    	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)
    	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:79)
    	... 115 more
    Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
    	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)
    	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)
    	... 128 more
    .
[0m12:38:48.257586 [debug] [MainThread]: On master: ROLLBACK
[0m12:38:48.260479 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:38:48.285202 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m12:38:48.287071 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:38:48.289618 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:38:48.291631 [debug] [MainThread]: On master: ROLLBACK
[0m12:38:48.294173 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m12:38:48.297452 [debug] [MainThread]: On master: Close
[0m12:38:48.301525 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:38:48.302942 [debug] [MainThread]: Connection 'model.data_pipeline_poc.dim_uemoa_indicators' was properly closed.
[0m12:38:48.304170 [info ] [MainThread]: 
[0m12:38:48.305717 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.87 seconds (0.87s).
[0m12:38:48.307224 [debug] [MainThread]: Command end result
[0m12:38:48.388695 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/app/dbt/target/manifest.json
[0m12:38:48.402112 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/app/dbt/target/semantic_manifest.json
[0m12:38:48.418685 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/app/dbt/target/run_results.json
[0m12:38:48.421019 [info ] [MainThread]: 
[0m12:38:48.423273 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m12:38:48.425222 [info ] [MainThread]: 
[0m12:38:48.429463 [error] [MainThread]:   Runtime Error in model dim_uemoa_indicators (models/silver/dim_uemoa_indicators.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] org.apache.spark.sql.AnalysisException: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: parquet; line 23 pos 9
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: parquet; line 23 pos 9
    	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:66)
    	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:87)
    	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:63)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
    	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
    	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
    	at org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:1676)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
    	at org.apache.spark.sql.catalyst.plans.logical.Filter.mapChildren(basicLogicalOperators.scala:316)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
    	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
    	at scala.collection.immutable.List.map(List.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:699)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
    	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:63)
    	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:43)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
    	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
    	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
    	at scala.collection.immutable.List.foldLeft(List.scala:91)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
    	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
    	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)
    	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)
    	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
    	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
    	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
    	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
    	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
    	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
    	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)
    	at scala.collection.immutable.List.map(List.scala:293)
    	at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)
    	at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)
    	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)
    	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:79)
    	... 115 more
    Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
    	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)
    	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)
    	... 128 more
    
[0m12:38:48.432506 [info ] [MainThread]: 
[0m12:38:48.434553 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m12:38:48.436853 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 4.869326, "process_in_blocks": "0", "process_kernel_time": 0.255866, "process_mem_max_rss": "106776", "process_out_blocks": "0", "process_user_time": 1.403267}
[0m12:38:48.438704 [debug] [MainThread]: Command `dbt run` failed at 12:38:48.438613 after 4.87 seconds
[0m12:38:48.440627 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d70e5be1510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d70e5be14d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d70e16ad610>]}
[0m12:38:48.442718 [debug] [MainThread]: Flushing usage events
[0m12:38:51.593264 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:39:11.330176 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x767926b62ed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x767926b62c50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x767926b63390>]}


============================== 12:39:11.334320 | 683666f8-1340-43cc-aab6-cd18632f3466 ==============================
[0m12:39:11.334320 [info ] [MainThread]: Running with dbt=1.9.0
[0m12:39:11.336770 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/app/dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/usr/app/dbt/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select dim_uemoa_indicators', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m12:39:11.392499 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:39:11.395148 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:39:11.397717 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:39:11.491019 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '683666f8-1340-43cc-aab6-cd18632f3466', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7679269c5f50>]}
[0m12:39:11.527326 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '683666f8-1340-43cc-aab6-cd18632f3466', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x767927983b90>]}
[0m12:39:11.531246 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m12:39:11.633904 [debug] [MainThread]: checksum: c99e828bba267739642b5a3ce85f17518764ea526e0e6c4fdc649171c1a66bff, vars: {}, profile: , target: , version: 1.9.0
[0m12:39:14.738118 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m12:39:14.739301 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m12:39:14.745475 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- seeds.data_pipeline_poc
- snapshots.data_pipeline_poc
[0m12:39:14.773860 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '683666f8-1340-43cc-aab6-cd18632f3466', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x767926b63fd0>]}
[0m12:39:14.881848 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/app/dbt/target/manifest.json
[0m12:39:14.893029 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/app/dbt/target/semantic_manifest.json
[0m12:39:14.924619 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '683666f8-1340-43cc-aab6-cd18632f3466', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7679262a5d90>]}
[0m12:39:14.928056 [info ] [MainThread]: Found 9 models, 13 data tests, 3 sources, 583 macros
[0m12:39:14.930388 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '683666f8-1340-43cc-aab6-cd18632f3466', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76792617c290>]}
[0m12:39:14.933652 [info ] [MainThread]: 
[0m12:39:14.935717 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:39:14.937462 [info ] [MainThread]: 
[0m12:39:14.939646 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m12:39:14.942251 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m12:39:14.951814 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m12:39:14.955044 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m12:39:14.956958 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:39:14.999533 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m12:39:15.001717 [debug] [ThreadPool]: SQL status: OK in 0.045 seconds
[0m12:39:15.006331 [debug] [ThreadPool]: On list_schemas: Close
[0m12:39:15.016109 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default_default_silver)
[0m12:39:15.021858 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:39:15.023947 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m12:39:15.025892 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show table extended in default_default_silver like '*'
  
[0m12:39:15.027790 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:39:15.055810 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#903, tableName#904, isTemporary#905, information#906]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7b530c86, [default_default_silver]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#903, tableName#904, isTemporary#905, information#906]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7b530c86, [default_default_silver]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m12:39:15.057976 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m12:39:15.059778 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show table extended in default_default_silver like '*'
  
[0m12:39:15.061877 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#903, tableName#904, isTemporary#905, information#906]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7b530c86, [default_default_silver]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#903, tableName#904, isTemporary#905, information#906]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7b530c86, [default_default_silver]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m12:39:15.064287 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m12:39:15.066310 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#903, tableName#904, isTemporary#905, information#906]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7b530c86, [default_default_silver]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#903, tableName#904, isTemporary#905, information#906]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7b530c86, [default_default_silver]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m12:39:15.071418 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m12:39:15.073419 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show tables in default_default_silver like '*'
  
[0m12:39:15.095465 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m12:39:15.097923 [debug] [ThreadPool]: SQL status: OK in 0.022 seconds
[0m12:39:15.105404 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m12:39:15.107410 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
describe extended default_default_silver.stg_events
  
[0m12:39:15.151743 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m12:39:15.154080 [debug] [ThreadPool]: SQL status: OK in 0.045 seconds
[0m12:39:15.160057 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m12:39:15.163196 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
describe extended default_default_silver.stg_users
  
[0m12:39:15.205884 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m12:39:15.207540 [debug] [ThreadPool]: SQL status: OK in 0.042 seconds
[0m12:39:15.212760 [debug] [ThreadPool]: On list_None_default_default_silver: ROLLBACK
[0m12:39:15.214876 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m12:39:15.215961 [debug] [ThreadPool]: On list_None_default_default_silver: Close
[0m12:39:15.220212 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default_default_silver, now list_None_default_gold)
[0m12:39:15.223413 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:39:15.224912 [debug] [ThreadPool]: Using spark connection "list_None_default_gold"
[0m12:39:15.226566 [debug] [ThreadPool]: On list_None_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_gold"} */
show table extended in default_gold like '*'
  
[0m12:39:15.228180 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:39:15.257493 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#979, tableName#980, isTemporary#981, information#982]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@275e8d10, [default_gold]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#979, tableName#980, isTemporary#981, information#982]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@275e8d10, [default_gold]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m12:39:15.260273 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m12:39:15.263117 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_gold"} */
show table extended in default_gold like '*'
  
[0m12:39:15.265585 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#979, tableName#980, isTemporary#981, information#982]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@275e8d10, [default_gold]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#979, tableName#980, isTemporary#981, information#982]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@275e8d10, [default_gold]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m12:39:15.268556 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m12:39:15.270915 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#979, tableName#980, isTemporary#981, information#982]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@275e8d10, [default_gold]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#979, tableName#980, isTemporary#981, information#982]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@275e8d10, [default_gold]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m12:39:15.275349 [debug] [ThreadPool]: Using spark connection "list_None_default_gold"
[0m12:39:15.277235 [debug] [ThreadPool]: On list_None_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_gold"} */
show tables in default_gold like '*'
  
[0m12:39:15.294840 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m12:39:15.297054 [debug] [ThreadPool]: SQL status: OK in 0.017 seconds
[0m12:39:15.301501 [debug] [ThreadPool]: On list_None_default_gold: ROLLBACK
[0m12:39:15.303369 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m12:39:15.305267 [debug] [ThreadPool]: On list_None_default_gold: Close
[0m12:39:15.309836 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default_gold, now list_None_default_default_gold)
[0m12:39:15.313415 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:39:15.315245 [debug] [ThreadPool]: Using spark connection "list_None_default_default_gold"
[0m12:39:15.317064 [debug] [ThreadPool]: On list_None_default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show table extended in default_default_gold like '*'
  
[0m12:39:15.318673 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:39:15.348263 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#994, tableName#995, isTemporary#996, information#997]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@780d99c4, [default_default_gold]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#994, tableName#995, isTemporary#996, information#997]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@780d99c4, [default_default_gold]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m12:39:15.350477 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m12:39:15.352469 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show table extended in default_default_gold like '*'
  
[0m12:39:15.354517 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#994, tableName#995, isTemporary#996, information#997]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@780d99c4, [default_default_gold]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#994, tableName#995, isTemporary#996, information#997]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@780d99c4, [default_default_gold]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m12:39:15.356748 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m12:39:15.358596 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#994, tableName#995, isTemporary#996, information#997]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@780d99c4, [default_default_gold]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#994, tableName#995, isTemporary#996, information#997]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@780d99c4, [default_default_gold]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m12:39:15.362216 [debug] [ThreadPool]: Using spark connection "list_None_default_default_gold"
[0m12:39:15.364419 [debug] [ThreadPool]: On list_None_default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show tables in default_default_gold like '*'
  
[0m12:39:15.386384 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m12:39:15.388877 [debug] [ThreadPool]: SQL status: OK in 0.022 seconds
[0m12:39:15.394624 [debug] [ThreadPool]: Using spark connection "list_None_default_default_gold"
[0m12:39:15.397292 [debug] [ThreadPool]: On list_None_default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
describe extended default_default_gold.fct_events_enriched
  
[0m12:39:15.434950 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m12:39:15.437583 [debug] [ThreadPool]: SQL status: OK in 0.039 seconds
[0m12:39:15.442809 [debug] [ThreadPool]: On list_None_default_default_gold: ROLLBACK
[0m12:39:15.445125 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m12:39:15.446998 [debug] [ThreadPool]: On list_None_default_default_gold: Close
[0m12:39:15.451833 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default_default_gold, now list_None_default_silver)
[0m12:39:15.455146 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:39:15.457129 [debug] [ThreadPool]: Using spark connection "list_None_default_silver"
[0m12:39:15.458859 [debug] [ThreadPool]: On list_None_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_silver"} */
show table extended in default_silver like '*'
  
[0m12:39:15.460649 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:39:15.489889 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#1048, tableName#1049, isTemporary#1050, information#1051]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@79906080, [default_silver]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#1048, tableName#1049, isTemporary#1050, information#1051]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@79906080, [default_silver]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m12:39:15.492089 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m12:39:15.494103 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_silver"} */
show table extended in default_silver like '*'
  
[0m12:39:15.496717 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#1048, tableName#1049, isTemporary#1050, information#1051]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@79906080, [default_silver]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#1048, tableName#1049, isTemporary#1050, information#1051]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@79906080, [default_silver]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m12:39:15.499106 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m12:39:15.501470 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#1048, tableName#1049, isTemporary#1050, information#1051]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@79906080, [default_silver]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#1048, tableName#1049, isTemporary#1050, information#1051]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@79906080, [default_silver]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m12:39:15.505080 [debug] [ThreadPool]: Using spark connection "list_None_default_silver"
[0m12:39:15.507164 [debug] [ThreadPool]: On list_None_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_silver"} */
show tables in default_silver like '*'
  
[0m12:39:15.524930 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m12:39:15.527187 [debug] [ThreadPool]: SQL status: OK in 0.018 seconds
[0m12:39:15.531520 [debug] [ThreadPool]: On list_None_default_silver: ROLLBACK
[0m12:39:15.533420 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m12:39:15.535362 [debug] [ThreadPool]: On list_None_default_silver: Close
[0m12:39:15.540405 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '683666f8-1340-43cc-aab6-cd18632f3466', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76792647b1d0>]}
[0m12:39:15.542603 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:39:15.544112 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:39:15.547359 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.dim_uemoa_indicators
[0m12:39:15.549976 [info ] [Thread-1 (]: 1 of 1 START sql table model default_silver.dim_uemoa_indicators ............... [RUN]
[0m12:39:15.552304 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default_silver, now model.data_pipeline_poc.dim_uemoa_indicators)
[0m12:39:15.554362 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_poc.dim_uemoa_indicators
[0m12:39:15.561345 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_poc.dim_uemoa_indicators"
[0m12:39:15.584843 [debug] [Thread-1 (]: Began executing node model.data_pipeline_poc.dim_uemoa_indicators
[0m12:39:15.601406 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.dim_uemoa_indicators"
[0m12:39:15.603665 [debug] [Thread-1 (]: On model.data_pipeline_poc.dim_uemoa_indicators: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.dim_uemoa_indicators"} */
drop table if exists default_silver.dim_uemoa_indicators
[0m12:39:15.605975 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:39:15.646183 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m12:39:15.648486 [debug] [Thread-1 (]: SQL status: OK in 0.042 seconds
[0m12:39:15.676165 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_poc.dim_uemoa_indicators"
[0m12:39:15.695502 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:39:15.697615 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.dim_uemoa_indicators"
[0m12:39:15.699490 [debug] [Thread-1 (]: On model.data_pipeline_poc.dim_uemoa_indicators: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.dim_uemoa_indicators"} */

  
    
        create or replace table default_silver.dim_uemoa_indicators
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Ce modèle lit directement le fichier Parquet d'Airbyte et le transforme en table Iceberg Silver
-- Le fichier Parquet est généré par Airbyte dans lakehouse/bronze/indicateurs_economiques_uemoa/
WITH source_data AS (
    SELECT *
    FROM parquet.`s3a://lakehouse/bronze/indicateurs_economiques_uemoa/2025_10_28_1761647715112_0.parquet`
)

SELECT
    date,
    pib_nominal_milliards_fcfa,
    poids_secteur_primaire_pct,
    poids_secteur_secondaire_pct,
    poids_secteur_tertiaire_pct,
    taux_croissance_reel_pib_pct,
    taux_inflation_moyen_annuel_ipc_pct,
    recettes_fiscales,
    recettes_fiscales_pct_pib,
    depenses_totales_et_prets_nets,
    solde_budgetaire_global_avec_dons,
    solde_budgetaire_global_hors_dons,
    encours_de_la_dette,
    encours_de_la_dette_pct_pib,
    exportations_biens_fob,
    importations_biens_fob,
    balance_des_biens,
    compte_transactions_courantes,
    balance_courante_sur_pib_pct,
    agregats_monnaie_masse_monetaire_m2,
    taux_couverture_emission_monetaire
FROM source_data
WHERE
    pib_nominal_milliards_fcfa IS NOT NULL
  
[0m12:39:15.713464 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] org.apache.spark.sql.AnalysisException: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: parquet; line 23 pos 9\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: parquet; line 23 pos 9\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:66)\n\tat org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:87)\n\tat org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:63)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:1676)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Filter.mapChildren(basicLogicalOperators.scala:316)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)\n\tat scala.collection.immutable.List.map(List.scala:297)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:699)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:63)\n\tat org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:43)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\nCaused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)\n\tat org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:79)\n\t... 115 more\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n\t... 128 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m12:39:15.716804 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m12:39:15.718551 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.dim_uemoa_indicators"} */

  
    
        create or replace table default_silver.dim_uemoa_indicators
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Ce modèle lit directement le fichier Parquet d'Airbyte et le transforme en table Iceberg Silver
-- Le fichier Parquet est généré par Airbyte dans lakehouse/bronze/indicateurs_economiques_uemoa/
WITH source_data AS (
    SELECT *
    FROM parquet.`s3a://lakehouse/bronze/indicateurs_economiques_uemoa/2025_10_28_1761647715112_0.parquet`
)

SELECT
    date,
    pib_nominal_milliards_fcfa,
    poids_secteur_primaire_pct,
    poids_secteur_secondaire_pct,
    poids_secteur_tertiaire_pct,
    taux_croissance_reel_pib_pct,
    taux_inflation_moyen_annuel_ipc_pct,
    recettes_fiscales,
    recettes_fiscales_pct_pib,
    depenses_totales_et_prets_nets,
    solde_budgetaire_global_avec_dons,
    solde_budgetaire_global_hors_dons,
    encours_de_la_dette,
    encours_de_la_dette_pct_pib,
    exportations_biens_fob,
    importations_biens_fob,
    balance_des_biens,
    compte_transactions_courantes,
    balance_courante_sur_pib_pct,
    agregats_monnaie_masse_monetaire_m2,
    taux_couverture_emission_monetaire
FROM source_data
WHERE
    pib_nominal_milliards_fcfa IS NOT NULL
  
[0m12:39:15.720896 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] org.apache.spark.sql.AnalysisException: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: parquet; line 23 pos 9
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: parquet; line 23 pos 9
  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:66)
  	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:87)
  	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:63)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
  	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
  	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
  	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
  	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
  	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
  	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
  	at org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:1676)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
  	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
  	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
  	at org.apache.spark.sql.catalyst.plans.logical.Filter.mapChildren(basicLogicalOperators.scala:316)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
  	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
  	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
  	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
  	at scala.collection.immutable.List.map(List.scala:297)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:699)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
  	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:63)
  	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:43)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
  	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
  	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
  	at scala.collection.immutable.List.foldLeft(List.scala:91)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
  	at scala.collection.immutable.List.foreach(List.scala:431)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
  	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
  	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)
  	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)
  	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
  	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
  	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
  	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
  	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
  	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
  	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)
  	at scala.collection.immutable.List.map(List.scala:293)
  	at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)
  	at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)
  	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)
  	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:79)
  	... 115 more
  Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
  	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)
  	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)
  	... 128 more
  
[0m12:39:15.723398 [debug] [Thread-1 (]: On model.data_pipeline_poc.dim_uemoa_indicators: ROLLBACK
[0m12:39:15.725459 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m12:39:15.727526 [debug] [Thread-1 (]: On model.data_pipeline_poc.dim_uemoa_indicators: Close
[0m12:39:15.736868 [debug] [Thread-1 (]: Runtime Error in model dim_uemoa_indicators (models/silver/dim_uemoa_indicators.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] org.apache.spark.sql.AnalysisException: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: parquet; line 23 pos 9
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: parquet; line 23 pos 9
    	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:66)
    	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:87)
    	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:63)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
    	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
    	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
    	at org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:1676)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
    	at org.apache.spark.sql.catalyst.plans.logical.Filter.mapChildren(basicLogicalOperators.scala:316)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
    	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
    	at scala.collection.immutable.List.map(List.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:699)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
    	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:63)
    	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:43)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
    	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
    	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
    	at scala.collection.immutable.List.foldLeft(List.scala:91)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
    	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
    	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)
    	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)
    	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
    	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
    	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
    	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
    	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
    	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
    	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)
    	at scala.collection.immutable.List.map(List.scala:293)
    	at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)
    	at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)
    	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)
    	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:79)
    	... 115 more
    Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
    	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)
    	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)
    	... 128 more
    
[0m12:39:15.741120 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '683666f8-1340-43cc-aab6-cd18632f3466', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x767925fdf190>]}
[0m12:39:15.744016 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model default_silver.dim_uemoa_indicators ...... [[31mERROR[0m in 0.19s]
[0m12:39:15.746981 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.dim_uemoa_indicators
[0m12:39:15.749981 [debug] [Thread-4 (]: Marking all children of 'model.data_pipeline_poc.dim_uemoa_indicators' to be skipped because of status 'error'.  Reason: Runtime Error in model dim_uemoa_indicators (models/silver/dim_uemoa_indicators.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] org.apache.spark.sql.AnalysisException: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: parquet; line 23 pos 9
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: parquet; line 23 pos 9
    	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:66)
    	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:87)
    	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:63)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
    	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
    	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
    	at org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:1676)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
    	at org.apache.spark.sql.catalyst.plans.logical.Filter.mapChildren(basicLogicalOperators.scala:316)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
    	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
    	at scala.collection.immutable.List.map(List.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:699)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
    	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:63)
    	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:43)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
    	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
    	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
    	at scala.collection.immutable.List.foldLeft(List.scala:91)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
    	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
    	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)
    	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)
    	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
    	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
    	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
    	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
    	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
    	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
    	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)
    	at scala.collection.immutable.List.map(List.scala:293)
    	at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)
    	at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)
    	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)
    	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:79)
    	... 115 more
    Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
    	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)
    	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)
    	... 128 more
    .
[0m12:39:15.754639 [debug] [MainThread]: On master: ROLLBACK
[0m12:39:15.757239 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:39:15.780863 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m12:39:15.783269 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:39:15.785500 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:39:15.787501 [debug] [MainThread]: On master: ROLLBACK
[0m12:39:15.789765 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m12:39:15.791778 [debug] [MainThread]: On master: Close
[0m12:39:15.796099 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:39:15.798453 [debug] [MainThread]: Connection 'model.data_pipeline_poc.dim_uemoa_indicators' was properly closed.
[0m12:39:15.800778 [info ] [MainThread]: 
[0m12:39:15.802938 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.86 seconds (0.86s).
[0m12:39:15.805913 [debug] [MainThread]: Command end result
[0m12:39:15.887607 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/app/dbt/target/manifest.json
[0m12:39:15.902328 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/app/dbt/target/semantic_manifest.json
[0m12:39:15.919268 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/app/dbt/target/run_results.json
[0m12:39:15.921379 [info ] [MainThread]: 
[0m12:39:15.923726 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m12:39:15.925769 [info ] [MainThread]: 
[0m12:39:15.929141 [error] [MainThread]:   Runtime Error in model dim_uemoa_indicators (models/silver/dim_uemoa_indicators.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] org.apache.spark.sql.AnalysisException: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: parquet; line 23 pos 9
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: parquet; line 23 pos 9
    	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:66)
    	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:87)
    	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:63)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
    	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
    	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
    	at org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:1676)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
    	at org.apache.spark.sql.catalyst.plans.logical.Filter.mapChildren(basicLogicalOperators.scala:316)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
    	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
    	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
    	at scala.collection.immutable.List.map(List.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:699)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
    	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:63)
    	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:43)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
    	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
    	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
    	at scala.collection.immutable.List.foldLeft(List.scala:91)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
    	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
    	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)
    	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)
    	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
    	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
    	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
    	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
    	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
    	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
    	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)
    	at scala.collection.immutable.List.map(List.scala:293)
    	at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)
    	at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)
    	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)
    	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:79)
    	... 115 more
    Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
    	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)
    	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)
    	... 128 more
    
[0m12:39:15.931765 [info ] [MainThread]: 
[0m12:39:15.933793 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m12:39:15.936811 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 4.6938014, "process_in_blocks": "0", "process_kernel_time": 0.286744, "process_mem_max_rss": "107176", "process_out_blocks": "0", "process_user_time": 1.298541}
[0m12:39:15.938781 [debug] [MainThread]: Command `dbt run` failed at 12:39:15.938666 after 4.70 seconds
[0m12:39:15.940567 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7679263cf810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76792a4ed610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76792a4ed510>]}
[0m12:39:15.942296 [debug] [MainThread]: Flushing usage events
[0m12:39:17.416027 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:02:56.171760 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x744acae43790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x744acae43e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x744acae43250>]}


============================== 17:02:56.176534 | 2576bf25-1922-42fd-adb5-4fff76c36a65 ==============================
[0m17:02:56.176534 [info ] [MainThread]: Running with dbt=1.9.0
[0m17:02:56.179283 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/app/dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/usr/app/dbt/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select dim_uemoa_indicators', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m17:02:56.227899 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m17:02:56.231306 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m17:02:56.234458 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m17:02:56.330497 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2576bf25-1922-42fd-adb5-4fff76c36a65', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x744acaca4d90>]}
[0m17:02:56.367295 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2576bf25-1922-42fd-adb5-4fff76c36a65', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x744acbc5fd10>]}
[0m17:02:56.370022 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m17:02:56.463654 [debug] [MainThread]: checksum: c99e828bba267739642b5a3ce85f17518764ea526e0e6c4fdc649171c1a66bff, vars: {}, profile: , target: , version: 1.9.0
[0m17:02:59.934295 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m17:02:59.937377 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_poc://models/silver/dim_uemoa_indicators.sql
[0m17:03:00.225807 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- seeds.data_pipeline_poc
- snapshots.data_pipeline_poc
[0m17:03:00.236761 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2576bf25-1922-42fd-adb5-4fff76c36a65', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x744ac81b5e90>]}
[0m17:03:00.412137 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/app/dbt/target/manifest.json
[0m17:03:00.425703 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/app/dbt/target/semantic_manifest.json
[0m17:03:00.467453 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2576bf25-1922-42fd-adb5-4fff76c36a65', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x744aa2f235d0>]}
[0m17:03:00.470106 [info ] [MainThread]: Found 9 models, 13 data tests, 3 sources, 583 macros
[0m17:03:00.472223 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2576bf25-1922-42fd-adb5-4fff76c36a65', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x744ac8125dd0>]}
[0m17:03:00.475400 [info ] [MainThread]: 
[0m17:03:00.477767 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:03:00.480217 [info ] [MainThread]: 
[0m17:03:00.482960 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m17:03:00.486824 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m17:03:00.495038 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m17:03:00.498550 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m17:03:00.501159 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:03:02.770972 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m17:03:02.773448 [debug] [ThreadPool]: SQL status: OK in 2.272 seconds
[0m17:03:02.836288 [debug] [ThreadPool]: On list_schemas: Close
[0m17:03:02.867537 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default_gold)
[0m17:03:02.873002 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:03:02.875169 [debug] [ThreadPool]: Using spark connection "list_None_default_gold"
[0m17:03:02.876985 [debug] [ThreadPool]: On list_None_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_gold"} */
show table extended in default_gold like '*'
  
[0m17:03:02.878912 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:03:02.951014 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#10, tableName#11, isTemporary#12, information#13]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@2e2aa8c7, [default_gold]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#10, tableName#11, isTemporary#12, information#13]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@2e2aa8c7, [default_gold]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m17:03:02.954253 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m17:03:02.956636 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_gold"} */
show table extended in default_gold like '*'
  
[0m17:03:02.958924 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#10, tableName#11, isTemporary#12, information#13]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@2e2aa8c7, [default_gold]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#10, tableName#11, isTemporary#12, information#13]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@2e2aa8c7, [default_gold]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m17:03:02.961442 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m17:03:02.963454 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#10, tableName#11, isTemporary#12, information#13]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@2e2aa8c7, [default_gold]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#10, tableName#11, isTemporary#12, information#13]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@2e2aa8c7, [default_gold]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m17:03:02.969377 [debug] [ThreadPool]: Using spark connection "list_None_default_gold"
[0m17:03:02.971763 [debug] [ThreadPool]: On list_None_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_gold"} */
show tables in default_gold like '*'
  
[0m17:03:03.013333 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m17:03:03.015692 [debug] [ThreadPool]: SQL status: OK in 0.042 seconds
[0m17:03:03.024631 [debug] [ThreadPool]: On list_None_default_gold: ROLLBACK
[0m17:03:03.026926 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m17:03:03.029737 [debug] [ThreadPool]: On list_None_default_gold: Close
[0m17:03:03.037218 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default_gold, now list_None_default_default_gold)
[0m17:03:03.041119 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:03:03.043088 [debug] [ThreadPool]: Using spark connection "list_None_default_default_gold"
[0m17:03:03.045096 [debug] [ThreadPool]: On list_None_default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show table extended in default_default_gold like '*'
  
[0m17:03:03.046822 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:03:03.094379 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#25, tableName#26, isTemporary#27, information#28]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@400b564d, [default_default_gold]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#25, tableName#26, isTemporary#27, information#28]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@400b564d, [default_default_gold]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m17:03:03.097207 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m17:03:03.099426 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show table extended in default_default_gold like '*'
  
[0m17:03:03.102028 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#25, tableName#26, isTemporary#27, information#28]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@400b564d, [default_default_gold]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#25, tableName#26, isTemporary#27, information#28]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@400b564d, [default_default_gold]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m17:03:03.104569 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m17:03:03.106623 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#25, tableName#26, isTemporary#27, information#28]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@400b564d, [default_default_gold]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#25, tableName#26, isTemporary#27, information#28]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@400b564d, [default_default_gold]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m17:03:03.109989 [debug] [ThreadPool]: Using spark connection "list_None_default_default_gold"
[0m17:03:03.112285 [debug] [ThreadPool]: On list_None_default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show tables in default_default_gold like '*'
  
[0m17:03:03.135319 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.iceberg.exceptions.NoSuchNamespaceException: Namespace does not exist: default_default_gold\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:46)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.iceberg.exceptions.NoSuchNamespaceException: Namespace does not exist: default_default_gold\n\tat org.apache.iceberg.rest.ErrorHandlers$NamespaceErrorHandler.accept(ErrorHandlers.java:173)\n\tat org.apache.iceberg.rest.ErrorHandlers$NamespaceErrorHandler.accept(ErrorHandlers.java:166)\n\tat org.apache.iceberg.rest.HTTPClient.throwFailure(HTTPClient.java:224)\n\tat org.apache.iceberg.rest.HTTPClient.execute(HTTPClient.java:308)\n\tat org.apache.iceberg.rest.BaseHTTPClient.get(BaseHTTPClient.java:77)\n\tat org.apache.iceberg.rest.RESTClient.get(RESTClient.java:97)\n\tat org.apache.iceberg.rest.RESTSessionCatalog.listTables(RESTSessionCatalog.java:388)\n\tat org.apache.iceberg.catalog.BaseSessionCatalog$AsCatalog.listTables(BaseSessionCatalog.java:79)\n\tat org.apache.iceberg.rest.RESTCatalog.listTables(RESTCatalog.java:92)\n\tat org.apache.iceberg.CachingCatalog.listTables(CachingCatalog.java:136)\n\tat org.apache.iceberg.spark.SparkCatalog.listTables(SparkCatalog.java:416)\n\tat org.apache.spark.sql.execution.datasources.v2.ShowTablesExec.run(ShowTablesExec.scala:40)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m17:03:03.137757 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m17:03:03.140039 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show tables in default_default_gold like '*'
  
[0m17:03:03.142742 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.iceberg.exceptions.NoSuchNamespaceException: Namespace does not exist: default_default_gold
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:46)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.iceberg.exceptions.NoSuchNamespaceException: Namespace does not exist: default_default_gold
  	at org.apache.iceberg.rest.ErrorHandlers$NamespaceErrorHandler.accept(ErrorHandlers.java:173)
  	at org.apache.iceberg.rest.ErrorHandlers$NamespaceErrorHandler.accept(ErrorHandlers.java:166)
  	at org.apache.iceberg.rest.HTTPClient.throwFailure(HTTPClient.java:224)
  	at org.apache.iceberg.rest.HTTPClient.execute(HTTPClient.java:308)
  	at org.apache.iceberg.rest.BaseHTTPClient.get(BaseHTTPClient.java:77)
  	at org.apache.iceberg.rest.RESTClient.get(RESTClient.java:97)
  	at org.apache.iceberg.rest.RESTSessionCatalog.listTables(RESTSessionCatalog.java:388)
  	at org.apache.iceberg.catalog.BaseSessionCatalog$AsCatalog.listTables(BaseSessionCatalog.java:79)
  	at org.apache.iceberg.rest.RESTCatalog.listTables(RESTCatalog.java:92)
  	at org.apache.iceberg.CachingCatalog.listTables(CachingCatalog.java:136)
  	at org.apache.iceberg.spark.SparkCatalog.listTables(SparkCatalog.java:416)
  	at org.apache.spark.sql.execution.datasources.v2.ShowTablesExec.run(ShowTablesExec.scala:40)
  	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
  	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
  	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
  	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
  	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
  	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
  	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
  	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
  	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
  	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
  	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
  	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m17:03:03.145090 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_show_tables_without_caching
[0m17:03:03.147446 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.iceberg.exceptions.NoSuchNamespaceException: Namespace does not exist: default_default_gold
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:46)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.iceberg.exceptions.NoSuchNamespaceException: Namespace does not exist: default_default_gold
    	at org.apache.iceberg.rest.ErrorHandlers$NamespaceErrorHandler.accept(ErrorHandlers.java:173)
    	at org.apache.iceberg.rest.ErrorHandlers$NamespaceErrorHandler.accept(ErrorHandlers.java:166)
    	at org.apache.iceberg.rest.HTTPClient.throwFailure(HTTPClient.java:224)
    	at org.apache.iceberg.rest.HTTPClient.execute(HTTPClient.java:308)
    	at org.apache.iceberg.rest.BaseHTTPClient.get(BaseHTTPClient.java:77)
    	at org.apache.iceberg.rest.RESTClient.get(RESTClient.java:97)
    	at org.apache.iceberg.rest.RESTSessionCatalog.listTables(RESTSessionCatalog.java:388)
    	at org.apache.iceberg.catalog.BaseSessionCatalog$AsCatalog.listTables(BaseSessionCatalog.java:79)
    	at org.apache.iceberg.rest.RESTCatalog.listTables(RESTCatalog.java:92)
    	at org.apache.iceberg.CachingCatalog.listTables(CachingCatalog.java:136)
    	at org.apache.iceberg.spark.SparkCatalog.listTables(SparkCatalog.java:416)
    	at org.apache.spark.sql.execution.datasources.v2.ShowTablesExec.run(ShowTablesExec.scala:40)
    	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
    	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
    	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
    	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
    	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
    	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
    	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
    	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
    	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
    	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
    	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
    	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
    	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
    	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m17:03:03.149946 [debug] [ThreadPool]: Spark adapter: Error while retrieving information about default_default_gold: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.iceberg.exceptions.NoSuchNamespaceException: Namespace does not exist: default_default_gold
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:46)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.iceberg.exceptions.NoSuchNamespaceException: Namespace does not exist: default_default_gold
    	at org.apache.iceberg.rest.ErrorHandlers$NamespaceErrorHandler.accept(ErrorHandlers.java:173)
    	at org.apache.iceberg.rest.ErrorHandlers$NamespaceErrorHandler.accept(ErrorHandlers.java:166)
    	at org.apache.iceberg.rest.HTTPClient.throwFailure(HTTPClient.java:224)
    	at org.apache.iceberg.rest.HTTPClient.execute(HTTPClient.java:308)
    	at org.apache.iceberg.rest.BaseHTTPClient.get(BaseHTTPClient.java:77)
    	at org.apache.iceberg.rest.RESTClient.get(RESTClient.java:97)
    	at org.apache.iceberg.rest.RESTSessionCatalog.listTables(RESTSessionCatalog.java:388)
    	at org.apache.iceberg.catalog.BaseSessionCatalog$AsCatalog.listTables(BaseSessionCatalog.java:79)
    	at org.apache.iceberg.rest.RESTCatalog.listTables(RESTCatalog.java:92)
    	at org.apache.iceberg.CachingCatalog.listTables(CachingCatalog.java:136)
    	at org.apache.iceberg.spark.SparkCatalog.listTables(SparkCatalog.java:416)
    	at org.apache.spark.sql.execution.datasources.v2.ShowTablesExec.run(ShowTablesExec.scala:40)
    	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
    	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
    	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
    	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
    	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
    	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
    	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
    	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
    	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
    	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
    	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
    	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
    	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
    	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m17:03:03.152342 [debug] [ThreadPool]: On list_None_default_default_gold: ROLLBACK
[0m17:03:03.154416 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m17:03:03.156271 [debug] [ThreadPool]: On list_None_default_default_gold: Close
[0m17:03:03.162050 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default_default_gold, now list_None_default_silver)
[0m17:03:03.166019 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:03:03.168366 [debug] [ThreadPool]: Using spark connection "list_None_default_silver"
[0m17:03:03.170243 [debug] [ThreadPool]: On list_None_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_silver"} */
show table extended in default_silver like '*'
  
[0m17:03:03.172278 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:03:03.216740 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#36, tableName#37, isTemporary#38, information#39]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@5d262238, [default_silver]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#36, tableName#37, isTemporary#38, information#39]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@5d262238, [default_silver]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m17:03:03.219536 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m17:03:03.221260 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_silver"} */
show table extended in default_silver like '*'
  
[0m17:03:03.223513 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#36, tableName#37, isTemporary#38, information#39]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@5d262238, [default_silver]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#36, tableName#37, isTemporary#38, information#39]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@5d262238, [default_silver]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m17:03:03.225489 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m17:03:03.227326 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#36, tableName#37, isTemporary#38, information#39]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@5d262238, [default_silver]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#36, tableName#37, isTemporary#38, information#39]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@5d262238, [default_silver]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m17:03:03.231242 [debug] [ThreadPool]: Using spark connection "list_None_default_silver"
[0m17:03:03.234210 [debug] [ThreadPool]: On list_None_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_silver"} */
show tables in default_silver like '*'
  
[0m17:03:03.264539 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m17:03:03.266811 [debug] [ThreadPool]: SQL status: OK in 0.030 seconds
[0m17:03:03.271625 [debug] [ThreadPool]: On list_None_default_silver: ROLLBACK
[0m17:03:03.273640 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m17:03:03.275565 [debug] [ThreadPool]: On list_None_default_silver: Close
[0m17:03:03.280129 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default_silver, now list_None_default_default_silver)
[0m17:03:03.283634 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:03:03.286524 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m17:03:03.288462 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show table extended in default_default_silver like '*'
  
[0m17:03:03.290159 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:03:03.333734 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#51, tableName#52, isTemporary#53, information#54]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@345cc534, [default_default_silver]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#51, tableName#52, isTemporary#53, information#54]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@345cc534, [default_default_silver]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m17:03:03.335860 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m17:03:03.337238 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show table extended in default_default_silver like '*'
  
[0m17:03:03.338652 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#51, tableName#52, isTemporary#53, information#54]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@345cc534, [default_default_silver]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#51, tableName#52, isTemporary#53, information#54]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@345cc534, [default_default_silver]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m17:03:03.340567 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m17:03:03.342081 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#51, tableName#52, isTemporary#53, information#54]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@345cc534, [default_default_silver]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#51, tableName#52, isTemporary#53, information#54]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@345cc534, [default_default_silver]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m17:03:03.345028 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m17:03:03.346946 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show tables in default_default_silver like '*'
  
[0m17:03:03.364884 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.iceberg.exceptions.NoSuchNamespaceException: Namespace does not exist: default_default_silver\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:46)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.iceberg.exceptions.NoSuchNamespaceException: Namespace does not exist: default_default_silver\n\tat org.apache.iceberg.rest.ErrorHandlers$NamespaceErrorHandler.accept(ErrorHandlers.java:173)\n\tat org.apache.iceberg.rest.ErrorHandlers$NamespaceErrorHandler.accept(ErrorHandlers.java:166)\n\tat org.apache.iceberg.rest.HTTPClient.throwFailure(HTTPClient.java:224)\n\tat org.apache.iceberg.rest.HTTPClient.execute(HTTPClient.java:308)\n\tat org.apache.iceberg.rest.BaseHTTPClient.get(BaseHTTPClient.java:77)\n\tat org.apache.iceberg.rest.RESTClient.get(RESTClient.java:97)\n\tat org.apache.iceberg.rest.RESTSessionCatalog.listTables(RESTSessionCatalog.java:388)\n\tat org.apache.iceberg.catalog.BaseSessionCatalog$AsCatalog.listTables(BaseSessionCatalog.java:79)\n\tat org.apache.iceberg.rest.RESTCatalog.listTables(RESTCatalog.java:92)\n\tat org.apache.iceberg.CachingCatalog.listTables(CachingCatalog.java:136)\n\tat org.apache.iceberg.spark.SparkCatalog.listTables(SparkCatalog.java:416)\n\tat org.apache.spark.sql.execution.datasources.v2.ShowTablesExec.run(ShowTablesExec.scala:40)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m17:03:03.367568 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m17:03:03.369964 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show tables in default_default_silver like '*'
  
[0m17:03:03.372236 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.iceberg.exceptions.NoSuchNamespaceException: Namespace does not exist: default_default_silver
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:46)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.iceberg.exceptions.NoSuchNamespaceException: Namespace does not exist: default_default_silver
  	at org.apache.iceberg.rest.ErrorHandlers$NamespaceErrorHandler.accept(ErrorHandlers.java:173)
  	at org.apache.iceberg.rest.ErrorHandlers$NamespaceErrorHandler.accept(ErrorHandlers.java:166)
  	at org.apache.iceberg.rest.HTTPClient.throwFailure(HTTPClient.java:224)
  	at org.apache.iceberg.rest.HTTPClient.execute(HTTPClient.java:308)
  	at org.apache.iceberg.rest.BaseHTTPClient.get(BaseHTTPClient.java:77)
  	at org.apache.iceberg.rest.RESTClient.get(RESTClient.java:97)
  	at org.apache.iceberg.rest.RESTSessionCatalog.listTables(RESTSessionCatalog.java:388)
  	at org.apache.iceberg.catalog.BaseSessionCatalog$AsCatalog.listTables(BaseSessionCatalog.java:79)
  	at org.apache.iceberg.rest.RESTCatalog.listTables(RESTCatalog.java:92)
  	at org.apache.iceberg.CachingCatalog.listTables(CachingCatalog.java:136)
  	at org.apache.iceberg.spark.SparkCatalog.listTables(SparkCatalog.java:416)
  	at org.apache.spark.sql.execution.datasources.v2.ShowTablesExec.run(ShowTablesExec.scala:40)
  	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
  	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
  	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
  	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
  	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
  	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
  	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
  	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
  	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
  	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
  	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
  	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m17:03:03.374596 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_show_tables_without_caching
[0m17:03:03.376634 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.iceberg.exceptions.NoSuchNamespaceException: Namespace does not exist: default_default_silver
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:46)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.iceberg.exceptions.NoSuchNamespaceException: Namespace does not exist: default_default_silver
    	at org.apache.iceberg.rest.ErrorHandlers$NamespaceErrorHandler.accept(ErrorHandlers.java:173)
    	at org.apache.iceberg.rest.ErrorHandlers$NamespaceErrorHandler.accept(ErrorHandlers.java:166)
    	at org.apache.iceberg.rest.HTTPClient.throwFailure(HTTPClient.java:224)
    	at org.apache.iceberg.rest.HTTPClient.execute(HTTPClient.java:308)
    	at org.apache.iceberg.rest.BaseHTTPClient.get(BaseHTTPClient.java:77)
    	at org.apache.iceberg.rest.RESTClient.get(RESTClient.java:97)
    	at org.apache.iceberg.rest.RESTSessionCatalog.listTables(RESTSessionCatalog.java:388)
    	at org.apache.iceberg.catalog.BaseSessionCatalog$AsCatalog.listTables(BaseSessionCatalog.java:79)
    	at org.apache.iceberg.rest.RESTCatalog.listTables(RESTCatalog.java:92)
    	at org.apache.iceberg.CachingCatalog.listTables(CachingCatalog.java:136)
    	at org.apache.iceberg.spark.SparkCatalog.listTables(SparkCatalog.java:416)
    	at org.apache.spark.sql.execution.datasources.v2.ShowTablesExec.run(ShowTablesExec.scala:40)
    	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
    	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
    	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
    	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
    	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
    	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
    	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
    	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
    	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
    	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
    	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
    	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
    	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
    	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m17:03:03.378853 [debug] [ThreadPool]: Spark adapter: Error while retrieving information about default_default_silver: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.iceberg.exceptions.NoSuchNamespaceException: Namespace does not exist: default_default_silver
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:46)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.iceberg.exceptions.NoSuchNamespaceException: Namespace does not exist: default_default_silver
    	at org.apache.iceberg.rest.ErrorHandlers$NamespaceErrorHandler.accept(ErrorHandlers.java:173)
    	at org.apache.iceberg.rest.ErrorHandlers$NamespaceErrorHandler.accept(ErrorHandlers.java:166)
    	at org.apache.iceberg.rest.HTTPClient.throwFailure(HTTPClient.java:224)
    	at org.apache.iceberg.rest.HTTPClient.execute(HTTPClient.java:308)
    	at org.apache.iceberg.rest.BaseHTTPClient.get(BaseHTTPClient.java:77)
    	at org.apache.iceberg.rest.RESTClient.get(RESTClient.java:97)
    	at org.apache.iceberg.rest.RESTSessionCatalog.listTables(RESTSessionCatalog.java:388)
    	at org.apache.iceberg.catalog.BaseSessionCatalog$AsCatalog.listTables(BaseSessionCatalog.java:79)
    	at org.apache.iceberg.rest.RESTCatalog.listTables(RESTCatalog.java:92)
    	at org.apache.iceberg.CachingCatalog.listTables(CachingCatalog.java:136)
    	at org.apache.iceberg.spark.SparkCatalog.listTables(SparkCatalog.java:416)
    	at org.apache.spark.sql.execution.datasources.v2.ShowTablesExec.run(ShowTablesExec.scala:40)
    	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
    	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
    	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
    	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
    	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
    	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
    	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
    	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
    	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
    	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
    	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
    	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
    	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
    	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m17:03:03.380771 [debug] [ThreadPool]: On list_None_default_default_silver: ROLLBACK
[0m17:03:03.383163 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m17:03:03.385614 [debug] [ThreadPool]: On list_None_default_default_silver: Close
[0m17:03:03.391590 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2576bf25-1922-42fd-adb5-4fff76c36a65', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x744aca523e50>]}
[0m17:03:03.393964 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:03:03.396074 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:03:03.399911 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.dim_uemoa_indicators
[0m17:03:03.402553 [info ] [Thread-1 (]: 1 of 1 START sql table model default_silver.dim_uemoa_indicators ............... [RUN]
[0m17:03:03.405000 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default_default_silver, now model.data_pipeline_poc.dim_uemoa_indicators)
[0m17:03:03.407342 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_poc.dim_uemoa_indicators
[0m17:03:03.414164 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_poc.dim_uemoa_indicators"
[0m17:03:03.439640 [debug] [Thread-1 (]: Began executing node model.data_pipeline_poc.dim_uemoa_indicators
[0m17:03:03.453177 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.dim_uemoa_indicators"
[0m17:03:03.455821 [debug] [Thread-1 (]: On model.data_pipeline_poc.dim_uemoa_indicators: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.dim_uemoa_indicators"} */
drop table if exists default_silver.dim_uemoa_indicators
[0m17:03:03.458130 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m17:03:03.525857 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m17:03:03.528153 [debug] [Thread-1 (]: SQL status: OK in 0.070 seconds
[0m17:03:03.554380 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_poc.dim_uemoa_indicators"
[0m17:03:03.582387 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m17:03:03.586116 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.dim_uemoa_indicators"
[0m17:03:03.588080 [debug] [Thread-1 (]: On model.data_pipeline_poc.dim_uemoa_indicators: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.dim_uemoa_indicators"} */

  
    
        create or replace table default_silver.dim_uemoa_indicators
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Ce modèle sert de "source de vérité" propre pour tous les modèles Gold.
-- Il sélectionne depuis la source et applique les premières transformations/nettoyages.
SELECT
    date,
    pib_nominal_milliards_fcfa,
    poids_secteur_primaire_pct,
    poids_secteur_secondaire_pct,
    poids_secteur_tertiaire_pct,
    taux_croissance_reel_pib_pct,
    taux_inflation_moyen_annuel_ipc_pct,
    recettes_fiscales,
    recettes_fiscales_pct_pib,
    depenses_totales_et_prets_nets,
    solde_budgetaire_global_avec_dons,
    solde_budgetaire_global_hors_dons,
    encours_de_la_dette,
    encours_de_la_dette_pct_pib,
    exportations_biens_fob,
    importations_biens_fob,
    balance_des_biens,
    compte_transactions_courantes,
    balance_courante_sur_pib_pct,
    agregats_monnaie_masse_monetaire_m2,
    taux_couverture_emission_monetaire
    -- Ajoutez toutes les autres colonnes dont vous avez besoin
FROM
    bronze.indicateurs_economiques_uemoa
WHERE
    pib_nominal_milliards_fcfa IS NOT NULL
  
[0m17:03:06.689210 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m17:03:06.691057 [debug] [Thread-1 (]: SQL status: OK in 3.101 seconds
[0m17:03:06.708470 [debug] [Thread-1 (]: On model.data_pipeline_poc.dim_uemoa_indicators: ROLLBACK
[0m17:03:06.711194 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m17:03:06.712820 [debug] [Thread-1 (]: On model.data_pipeline_poc.dim_uemoa_indicators: Close
[0m17:03:06.719170 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2576bf25-1922-42fd-adb5-4fff76c36a65', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x744aca37e390>]}
[0m17:03:06.721563 [info ] [Thread-1 (]: 1 of 1 OK created sql table model default_silver.dim_uemoa_indicators .......... [[32mOK[0m in 3.31s]
[0m17:03:06.723825 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.dim_uemoa_indicators
[0m17:03:06.727018 [debug] [MainThread]: On master: ROLLBACK
[0m17:03:06.728753 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:03:06.764749 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m17:03:06.766764 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:03:06.768214 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:03:06.769212 [debug] [MainThread]: On master: ROLLBACK
[0m17:03:06.771036 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m17:03:06.772783 [debug] [MainThread]: On master: Close
[0m17:03:06.776581 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:03:06.778237 [debug] [MainThread]: Connection 'model.data_pipeline_poc.dim_uemoa_indicators' was properly closed.
[0m17:03:06.779574 [info ] [MainThread]: 
[0m17:03:06.781909 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 6.30 seconds (6.30s).
[0m17:03:06.783958 [debug] [MainThread]: Command end result
[0m17:03:06.865768 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/app/dbt/target/manifest.json
[0m17:03:06.876348 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/app/dbt/target/semantic_manifest.json
[0m17:03:06.890235 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/app/dbt/target/run_results.json
[0m17:03:06.891911 [info ] [MainThread]: 
[0m17:03:06.893808 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:03:06.895180 [info ] [MainThread]: 
[0m17:03:06.897267 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m17:03:06.899985 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 10.816855, "process_in_blocks": "112", "process_kernel_time": 0.318642, "process_mem_max_rss": "110896", "process_out_blocks": "144", "process_user_time": 1.540105}
[0m17:03:06.902119 [debug] [MainThread]: Command `dbt run` succeeded at 17:03:06.902028 after 10.82 seconds
[0m17:03:06.904449 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x744ace644ed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x744ace7c9590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x744ace7c9810>]}
[0m17:03:06.906746 [debug] [MainThread]: Flushing usage events
[0m17:03:07.956495 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:03:20.945632 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77ee89d83d50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77ee89d827d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77ee89d82fd0>]}


============================== 17:03:20.949721 | 6a225c2c-d292-44d1-85ce-6f258f07aa8c ==============================
[0m17:03:20.949721 [info ] [MainThread]: Running with dbt=1.9.0
[0m17:03:20.951807 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/usr/app/dbt/logs', 'profiles_dir': '/usr/app/dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m17:03:21.003109 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m17:03:21.005662 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m17:03:21.008438 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m17:03:21.099187 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6a225c2c-d292-44d1-85ce-6f258f07aa8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77ee89de4fd0>]}
[0m17:03:21.134833 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6a225c2c-d292-44d1-85ce-6f258f07aa8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77ee8bf52cd0>]}
[0m17:03:21.138188 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m17:03:21.223995 [debug] [MainThread]: checksum: c99e828bba267739642b5a3ce85f17518764ea526e0e6c4fdc649171c1a66bff, vars: {}, profile: , target: , version: 1.9.0
[0m17:03:24.249769 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:03:24.252346 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:03:24.258362 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- seeds.data_pipeline_poc
- snapshots.data_pipeline_poc
[0m17:03:24.284571 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6a225c2c-d292-44d1-85ce-6f258f07aa8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77ee893b9fd0>]}
[0m17:03:24.411768 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/app/dbt/target/manifest.json
[0m17:03:24.427425 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/app/dbt/target/semantic_manifest.json
[0m17:03:24.457139 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6a225c2c-d292-44d1-85ce-6f258f07aa8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77ee893bb7d0>]}
[0m17:03:24.460119 [info ] [MainThread]: Found 9 models, 13 data tests, 3 sources, 583 macros
[0m17:03:24.462344 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6a225c2c-d292-44d1-85ce-6f258f07aa8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77ee8940cd10>]}
[0m17:03:24.465324 [info ] [MainThread]: 
[0m17:03:24.467345 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:03:24.468975 [info ] [MainThread]: 
[0m17:03:24.471563 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m17:03:24.478361 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m17:03:24.486955 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m17:03:24.488856 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m17:03:24.491203 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:03:24.542817 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m17:03:24.545167 [debug] [ThreadPool]: SQL status: OK in 0.054 seconds
[0m17:03:24.550328 [debug] [ThreadPool]: On list_schemas: Close
[0m17:03:24.556228 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m17:03:24.558578 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m17:03:24.560613 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:03:24.615122 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m17:03:24.618132 [debug] [ThreadPool]: SQL status: OK in 0.057 seconds
[0m17:03:24.623969 [debug] [ThreadPool]: On list_schemas: Close
[0m17:03:24.630230 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m17:03:24.632935 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m17:03:24.635208 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:03:24.687483 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m17:03:24.690779 [debug] [ThreadPool]: SQL status: OK in 0.056 seconds
[0m17:03:24.695419 [debug] [ThreadPool]: On list_schemas: Close
[0m17:03:24.701504 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m17:03:24.703785 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m17:03:24.705780 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:03:24.759064 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m17:03:24.761285 [debug] [ThreadPool]: SQL status: OK in 0.055 seconds
[0m17:03:24.765972 [debug] [ThreadPool]: On list_schemas: Close
[0m17:03:24.771254 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__default_default_silver)
[0m17:03:24.773943 [debug] [ThreadPool]: Creating schema "schema: "default_default_silver"
"
[0m17:03:24.778822 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:03:24.780911 [debug] [ThreadPool]: Using spark connection "create__default_default_silver"
[0m17:03:24.782755 [debug] [ThreadPool]: On create__default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "create__default_default_silver"} */
create schema if not exists default_default_silver
  
[0m17:03:24.784697 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:03:24.848017 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m17:03:24.850647 [debug] [ThreadPool]: SQL status: OK in 0.066 seconds
[0m17:03:24.853153 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m17:03:24.855111 [debug] [ThreadPool]: On create__default_default_silver: ROLLBACK
[0m17:03:24.858035 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m17:03:24.859965 [debug] [ThreadPool]: On create__default_default_silver: Close
[0m17:03:24.865212 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create__default_default_silver, now create__default_default_gold)
[0m17:03:24.867504 [debug] [ThreadPool]: Creating schema "schema: "default_default_gold"
"
[0m17:03:24.870633 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:03:24.872743 [debug] [ThreadPool]: Using spark connection "create__default_default_gold"
[0m17:03:24.874820 [debug] [ThreadPool]: On create__default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "create__default_default_gold"} */
create schema if not exists default_default_gold
  
[0m17:03:24.877092 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:03:24.929523 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m17:03:24.931912 [debug] [ThreadPool]: SQL status: OK in 0.055 seconds
[0m17:03:24.934849 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m17:03:24.937189 [debug] [ThreadPool]: On create__default_default_gold: ROLLBACK
[0m17:03:24.939267 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m17:03:24.942189 [debug] [ThreadPool]: On create__default_default_gold: Close
[0m17:03:24.950076 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create__default_default_gold, now list_None_default_default_silver)
[0m17:03:24.956506 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:03:24.958965 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m17:03:24.961090 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show table extended in default_default_silver like '*'
  
[0m17:03:24.963252 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:03:24.997834 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#205, tableName#206, isTemporary#207, information#208]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@5732c2f6, [default_default_silver]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#205, tableName#206, isTemporary#207, information#208]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@5732c2f6, [default_default_silver]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m17:03:25.001052 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m17:03:25.002653 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show table extended in default_default_silver like '*'
  
[0m17:03:25.004296 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#205, tableName#206, isTemporary#207, information#208]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@5732c2f6, [default_default_silver]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#205, tableName#206, isTemporary#207, information#208]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@5732c2f6, [default_default_silver]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m17:03:25.006632 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m17:03:25.008217 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#205, tableName#206, isTemporary#207, information#208]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@5732c2f6, [default_default_silver]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#205, tableName#206, isTemporary#207, information#208]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@5732c2f6, [default_default_silver]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m17:03:25.012441 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m17:03:25.013944 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show tables in default_default_silver like '*'
  
[0m17:03:25.033617 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m17:03:25.035823 [debug] [ThreadPool]: SQL status: OK in 0.021 seconds
[0m17:03:25.040646 [debug] [ThreadPool]: On list_None_default_default_silver: ROLLBACK
[0m17:03:25.043073 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m17:03:25.045233 [debug] [ThreadPool]: On list_None_default_default_silver: Close
[0m17:03:25.052624 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default_default_silver, now list_None_default_gold)
[0m17:03:25.056564 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:03:25.058291 [debug] [ThreadPool]: Using spark connection "list_None_default_gold"
[0m17:03:25.060136 [debug] [ThreadPool]: On list_None_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_gold"} */
show table extended in default_gold like '*'
  
[0m17:03:25.061865 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:03:25.108180 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#220, tableName#221, isTemporary#222, information#223]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@312ce756, [default_gold]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#220, tableName#221, isTemporary#222, information#223]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@312ce756, [default_gold]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m17:03:25.110753 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m17:03:25.113006 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_gold"} */
show table extended in default_gold like '*'
  
[0m17:03:25.115010 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#220, tableName#221, isTemporary#222, information#223]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@312ce756, [default_gold]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#220, tableName#221, isTemporary#222, information#223]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@312ce756, [default_gold]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m17:03:25.117472 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m17:03:25.119569 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#220, tableName#221, isTemporary#222, information#223]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@312ce756, [default_gold]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#220, tableName#221, isTemporary#222, information#223]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@312ce756, [default_gold]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m17:03:25.123223 [debug] [ThreadPool]: Using spark connection "list_None_default_gold"
[0m17:03:25.124947 [debug] [ThreadPool]: On list_None_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_gold"} */
show tables in default_gold like '*'
  
[0m17:03:25.147473 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m17:03:25.149673 [debug] [ThreadPool]: SQL status: OK in 0.023 seconds
[0m17:03:25.153285 [debug] [ThreadPool]: On list_None_default_gold: ROLLBACK
[0m17:03:25.154729 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m17:03:25.156912 [debug] [ThreadPool]: On list_None_default_gold: Close
[0m17:03:25.162362 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default_gold, now list_None_default_silver)
[0m17:03:25.165943 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:03:25.168654 [debug] [ThreadPool]: Using spark connection "list_None_default_silver"
[0m17:03:25.170969 [debug] [ThreadPool]: On list_None_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_silver"} */
show table extended in default_silver like '*'
  
[0m17:03:25.173842 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:03:25.212610 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#235, tableName#236, isTemporary#237, information#238]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@6c4fedb0, [default_silver]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#235, tableName#236, isTemporary#237, information#238]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@6c4fedb0, [default_silver]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m17:03:25.214469 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m17:03:25.215945 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_silver"} */
show table extended in default_silver like '*'
  
[0m17:03:25.217421 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#235, tableName#236, isTemporary#237, information#238]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@6c4fedb0, [default_silver]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#235, tableName#236, isTemporary#237, information#238]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@6c4fedb0, [default_silver]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m17:03:25.218895 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m17:03:25.220666 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#235, tableName#236, isTemporary#237, information#238]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@6c4fedb0, [default_silver]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#235, tableName#236, isTemporary#237, information#238]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@6c4fedb0, [default_silver]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m17:03:25.223735 [debug] [ThreadPool]: Using spark connection "list_None_default_silver"
[0m17:03:25.225729 [debug] [ThreadPool]: On list_None_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_silver"} */
show tables in default_silver like '*'
  
[0m17:03:25.289024 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m17:03:25.308311 [debug] [ThreadPool]: SQL status: OK in 0.081 seconds
[0m17:03:25.317262 [debug] [ThreadPool]: Using spark connection "list_None_default_silver"
[0m17:03:25.320088 [debug] [ThreadPool]: On list_None_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_silver"} */
describe extended default_silver.dim_uemoa_indicators
  
[0m17:03:25.427873 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m17:03:25.432511 [debug] [ThreadPool]: SQL status: OK in 0.110 seconds
[0m17:03:25.439360 [debug] [ThreadPool]: On list_None_default_silver: ROLLBACK
[0m17:03:25.441704 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m17:03:25.443125 [debug] [ThreadPool]: On list_None_default_silver: Close
[0m17:03:25.448200 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default_silver, now list_None_default_default_gold)
[0m17:03:25.451896 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:03:25.454045 [debug] [ThreadPool]: Using spark connection "list_None_default_default_gold"
[0m17:03:25.455982 [debug] [ThreadPool]: On list_None_default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show table extended in default_default_gold like '*'
  
[0m17:03:25.458233 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:03:25.493562 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#301, tableName#302, isTemporary#303, information#304]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@672fe7f1, [default_default_gold]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#301, tableName#302, isTemporary#303, information#304]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@672fe7f1, [default_default_gold]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m17:03:25.496087 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m17:03:25.498434 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show table extended in default_default_gold like '*'
  
[0m17:03:25.500753 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#301, tableName#302, isTemporary#303, information#304]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@672fe7f1, [default_default_gold]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#301, tableName#302, isTemporary#303, information#304]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@672fe7f1, [default_default_gold]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m17:03:25.503255 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m17:03:25.505285 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#301, tableName#302, isTemporary#303, information#304]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@672fe7f1, [default_default_gold]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#301, tableName#302, isTemporary#303, information#304]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@672fe7f1, [default_default_gold]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m17:03:25.509468 [debug] [ThreadPool]: Using spark connection "list_None_default_default_gold"
[0m17:03:25.511918 [debug] [ThreadPool]: On list_None_default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show tables in default_default_gold like '*'
  
[0m17:03:25.532515 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m17:03:25.534952 [debug] [ThreadPool]: SQL status: OK in 0.021 seconds
[0m17:03:25.539089 [debug] [ThreadPool]: On list_None_default_default_gold: ROLLBACK
[0m17:03:25.541287 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m17:03:25.543281 [debug] [ThreadPool]: On list_None_default_default_gold: Close
[0m17:03:25.549958 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6a225c2c-d292-44d1-85ce-6f258f07aa8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77ee89c28210>]}
[0m17:03:25.552596 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:03:25.554456 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:03:25.558097 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.dim_uemoa_indicators
[0m17:03:25.560275 [info ] [Thread-1 (]: 1 of 9 START sql table model default_silver.dim_uemoa_indicators ............... [RUN]
[0m17:03:25.562175 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default_default_gold, now model.data_pipeline_poc.dim_uemoa_indicators)
[0m17:03:25.563821 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_poc.dim_uemoa_indicators
[0m17:03:25.572416 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_poc.dim_uemoa_indicators"
[0m17:03:25.594968 [debug] [Thread-1 (]: Began executing node model.data_pipeline_poc.dim_uemoa_indicators
[0m17:03:25.627650 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_poc.dim_uemoa_indicators"
[0m17:03:25.654976 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m17:03:25.657773 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.dim_uemoa_indicators"
[0m17:03:25.659850 [debug] [Thread-1 (]: On model.data_pipeline_poc.dim_uemoa_indicators: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.dim_uemoa_indicators"} */

  
    
        create or replace table default_silver.dim_uemoa_indicators
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Ce modèle sert de "source de vérité" propre pour tous les modèles Gold.
-- Il sélectionne depuis la source et applique les premières transformations/nettoyages.
SELECT
    date,
    pib_nominal_milliards_fcfa,
    poids_secteur_primaire_pct,
    poids_secteur_secondaire_pct,
    poids_secteur_tertiaire_pct,
    taux_croissance_reel_pib_pct,
    taux_inflation_moyen_annuel_ipc_pct,
    recettes_fiscales,
    recettes_fiscales_pct_pib,
    depenses_totales_et_prets_nets,
    solde_budgetaire_global_avec_dons,
    solde_budgetaire_global_hors_dons,
    encours_de_la_dette,
    encours_de_la_dette_pct_pib,
    exportations_biens_fob,
    importations_biens_fob,
    balance_des_biens,
    compte_transactions_courantes,
    balance_courante_sur_pib_pct,
    agregats_monnaie_masse_monetaire_m2,
    taux_couverture_emission_monetaire
    -- Ajoutez toutes les autres colonnes dont vous avez besoin
FROM
    bronze.indicateurs_economiques_uemoa
WHERE
    pib_nominal_milliards_fcfa IS NOT NULL
  
[0m17:03:25.661595 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m17:03:26.544767 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m17:03:26.546720 [debug] [Thread-1 (]: SQL status: OK in 0.885 seconds
[0m17:03:26.561029 [debug] [Thread-1 (]: On model.data_pipeline_poc.dim_uemoa_indicators: ROLLBACK
[0m17:03:26.563080 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m17:03:26.564827 [debug] [Thread-1 (]: On model.data_pipeline_poc.dim_uemoa_indicators: Close
[0m17:03:26.569997 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6a225c2c-d292-44d1-85ce-6f258f07aa8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77ee897afc50>]}
[0m17:03:26.572145 [info ] [Thread-1 (]: 1 of 9 OK created sql table model default_silver.dim_uemoa_indicators .......... [[32mOK[0m in 1.01s]
[0m17:03:26.574701 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.dim_uemoa_indicators
[0m17:03:26.576627 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.stg_events
[0m17:03:26.578707 [info ] [Thread-1 (]: 2 of 9 START sql table model default_default_silver.stg_events ................. [RUN]
[0m17:03:26.580670 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline_poc.dim_uemoa_indicators, now model.data_pipeline_poc.stg_events)
[0m17:03:26.582480 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_poc.stg_events
[0m17:03:26.586630 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_poc.stg_events"
[0m17:03:26.608750 [debug] [Thread-1 (]: Began executing node model.data_pipeline_poc.stg_events
[0m17:03:26.615249 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.stg_events"
[0m17:03:26.618193 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_events: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.stg_events"} */
drop table if exists default_default_silver.stg_events
[0m17:03:26.619958 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m17:03:26.662067 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m17:03:26.663994 [debug] [Thread-1 (]: SQL status: OK in 0.044 seconds
[0m17:03:26.667079 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_poc.stg_events"
[0m17:03:26.693875 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m17:03:26.696073 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.stg_events"
[0m17:03:26.698037 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_events: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.stg_events"} */

  
    
        create or replace table default_default_silver.stg_events
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Staging model for raw events from bronze layer
-- This model cleans and standardizes the raw event data

SELECT
    event_id,
    event_type,
    user_id,
    event_timestamp,
    event_data,
    current_timestamp() as dbt_loaded_at
FROM bronze.raw_events
WHERE event_id IS NOT NULL
  
[0m17:03:27.641028 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m17:03:27.643207 [debug] [Thread-1 (]: SQL status: OK in 0.943 seconds
[0m17:03:27.646331 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_events: ROLLBACK
[0m17:03:27.648708 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m17:03:27.651335 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_events: Close
[0m17:03:27.657542 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6a225c2c-d292-44d1-85ce-6f258f07aa8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77ee89352150>]}
[0m17:03:27.660135 [info ] [Thread-1 (]: 2 of 9 OK created sql table model default_default_silver.stg_events ............ [[32mOK[0m in 1.08s]
[0m17:03:27.662537 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.stg_events
[0m17:03:27.664819 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.stg_users
[0m17:03:27.667016 [info ] [Thread-1 (]: 3 of 9 START sql table model default_default_silver.stg_users .................. [RUN]
[0m17:03:27.669885 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline_poc.stg_events, now model.data_pipeline_poc.stg_users)
[0m17:03:27.672144 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_poc.stg_users
[0m17:03:27.676634 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_poc.stg_users"
[0m17:03:27.700414 [debug] [Thread-1 (]: Began executing node model.data_pipeline_poc.stg_users
[0m17:03:27.705557 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.stg_users"
[0m17:03:27.707928 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_users: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.stg_users"} */
drop table if exists default_default_silver.stg_users
[0m17:03:27.709346 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m17:03:27.755890 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m17:03:27.759126 [debug] [Thread-1 (]: SQL status: OK in 0.050 seconds
[0m17:03:27.762676 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_poc.stg_users"
[0m17:03:27.783548 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m17:03:27.785464 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.stg_users"
[0m17:03:27.787265 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_users: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.stg_users"} */

  
    
        create or replace table default_default_silver.stg_users
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Staging model for raw users from bronze layer
-- This model cleans and standardizes the raw user data

SELECT
    user_id,
    user_name,
    email as user_email,
    created_at,
    current_timestamp() as dbt_loaded_at
FROM bronze.raw_users
WHERE user_id IS NOT NULL
  
[0m17:03:28.627523 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m17:03:28.629204 [debug] [Thread-1 (]: SQL status: OK in 0.840 seconds
[0m17:03:28.631707 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_users: ROLLBACK
[0m17:03:28.632964 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m17:03:28.634580 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_users: Close
[0m17:03:28.640190 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6a225c2c-d292-44d1-85ce-6f258f07aa8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77ee88833b90>]}
[0m17:03:28.641966 [info ] [Thread-1 (]: 3 of 9 OK created sql table model default_default_silver.stg_users ............. [[32mOK[0m in 0.97s]
[0m17:03:28.643382 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.stg_users
[0m17:03:28.645328 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.gold_kpi_uemoa_growth_yoy
[0m17:03:28.647287 [info ] [Thread-1 (]: 4 of 9 START sql table model default_gold.gold_kpi_uemoa_growth_yoy ............ [RUN]
[0m17:03:28.649293 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline_poc.stg_users, now model.data_pipeline_poc.gold_kpi_uemoa_growth_yoy)
[0m17:03:28.650884 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_poc.gold_kpi_uemoa_growth_yoy
[0m17:03:28.656203 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_poc.gold_kpi_uemoa_growth_yoy"
[0m17:03:28.668691 [debug] [Thread-1 (]: Began executing node model.data_pipeline_poc.gold_kpi_uemoa_growth_yoy
[0m17:03:28.715944 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.gold_kpi_uemoa_growth_yoy"
[0m17:03:28.718399 [debug] [Thread-1 (]: On model.data_pipeline_poc.gold_kpi_uemoa_growth_yoy: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.gold_kpi_uemoa_growth_yoy"} */
drop table if exists default_gold.gold_kpi_uemoa_growth_yoy
[0m17:03:28.721562 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m17:03:28.771667 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m17:03:28.773789 [debug] [Thread-1 (]: SQL status: OK in 0.052 seconds
[0m17:03:28.777119 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_poc.gold_kpi_uemoa_growth_yoy"
[0m17:03:28.813010 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m17:03:28.815229 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.gold_kpi_uemoa_growth_yoy"
[0m17:03:28.817495 [debug] [Thread-1 (]: On model.data_pipeline_poc.gold_kpi_uemoa_growth_yoy: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.gold_kpi_uemoa_growth_yoy"} */

  
    
        create or replace table default_gold.gold_kpi_uemoa_growth_yoy
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Modèle Gold : Calcule la croissance Année-sur-Année (YoY) des KPIs
-- Idéal pour les graphiques de tendance et l'analyse de la performance.
SELECT
    date,
    pib_nominal_milliards_fcfa,
    taux_croissance_reel_pib_pct,
    taux_inflation_moyen_annuel_ipc_pct,
    
    -- Calcul de la croissance YoY du PIB nominal
    LAG(pib_nominal_milliards_fcfa, 1) OVER (ORDER BY date) as pib_nominal_annee_precedente,
    (pib_nominal_milliards_fcfa - LAG(pib_nominal_milliards_fcfa, 1) OVER (ORDER BY date)) 
        / LAG(pib_nominal_milliards_fcfa, 1) OVER (ORDER BY date) * 100 as pib_nominal_croissance_yoy_pct,

    -- Calcul de la croissance YoY des recettes fiscales
    recettes_fiscales,
    (recettes_fiscales - LAG(recettes_fiscales, 1) OVER (ORDER BY date))
        / LAG(recettes_fiscales, 1) OVER (ORDER BY date) * 100 as recettes_fiscales_croissance_yoy_pct,

    -- Calcul de la croissance YoY de la masse monétaire M2
    agregats_monnaie_masse_monetaire_m2,
    (agregats_monnaie_masse_monetaire_m2 - LAG(agregats_monnaie_masse_monetaire_m2, 1) OVER (ORDER BY date))
        / LAG(agregats_monnaie_masse_monetaire_m2, 1) OVER (ORDER BY date) * 100 as m2_croissance_yoy_pct

FROM
    default_silver.dim_uemoa_indicators
ORDER BY
    date DESC
  
[0m17:03:30.174291 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m17:03:30.177026 [debug] [Thread-1 (]: SQL status: OK in 1.357 seconds
[0m17:03:30.180307 [debug] [Thread-1 (]: On model.data_pipeline_poc.gold_kpi_uemoa_growth_yoy: ROLLBACK
[0m17:03:30.182675 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m17:03:30.184599 [debug] [Thread-1 (]: On model.data_pipeline_poc.gold_kpi_uemoa_growth_yoy: Close
[0m17:03:30.191168 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6a225c2c-d292-44d1-85ce-6f258f07aa8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77ee891d96d0>]}
[0m17:03:30.193747 [info ] [Thread-1 (]: 4 of 9 OK created sql table model default_gold.gold_kpi_uemoa_growth_yoy ....... [[32mOK[0m in 1.54s]
[0m17:03:30.195882 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.gold_kpi_uemoa_growth_yoy
[0m17:03:30.198247 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.gold_mart_uemoa_external_stability
[0m17:03:30.200703 [info ] [Thread-1 (]: 5 of 9 START sql table model default_gold.gold_mart_uemoa_external_stability ... [RUN]
[0m17:03:30.202877 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline_poc.gold_kpi_uemoa_growth_yoy, now model.data_pipeline_poc.gold_mart_uemoa_external_stability)
[0m17:03:30.207301 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_poc.gold_mart_uemoa_external_stability
[0m17:03:30.211973 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_poc.gold_mart_uemoa_external_stability"
[0m17:03:30.234525 [debug] [Thread-1 (]: Began executing node model.data_pipeline_poc.gold_mart_uemoa_external_stability
[0m17:03:30.239679 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.gold_mart_uemoa_external_stability"
[0m17:03:30.241983 [debug] [Thread-1 (]: On model.data_pipeline_poc.gold_mart_uemoa_external_stability: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.gold_mart_uemoa_external_stability"} */
drop table if exists default_gold.gold_mart_uemoa_external_stability
[0m17:03:30.244241 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m17:03:30.285489 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m17:03:30.288437 [debug] [Thread-1 (]: SQL status: OK in 0.044 seconds
[0m17:03:30.292348 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_poc.gold_mart_uemoa_external_stability"
[0m17:03:30.312792 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m17:03:30.314830 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.gold_mart_uemoa_external_stability"
[0m17:03:30.317010 [debug] [Thread-1 (]: On model.data_pipeline_poc.gold_mart_uemoa_external_stability: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.gold_mart_uemoa_external_stability"} */

  
    
        create or replace table default_gold.gold_mart_uemoa_external_stability
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Modèle Gold : Indicateurs de soutenabilité externe.
-- Calcule la couverture des importations par les réserves de change.

SELECT
    date,
    
    -- Avoirs officiels de réserve (nos devises)
    actifs_exterieurs_nets_bceao_avoirs_officiels,
    
    -- Importations annuelles
    importations_biens_fob,
    
    -- Importations mensuelles moyennes
    (importations_biens_fob / 12) as importations_mensuelles_moyennes,
    
    -- LE KPI CLÉ: Réserves en Mois d'Importations
    -- (Combien de mois d'importations peut-on couvrir avec nos réserves ?)
    (actifs_exterieurs_nets_bceao_avoirs_officiels / (importations_biens_fob / 12)) as reserves_en_mois_importations,

    -- Taux de couverture (déjà présent, mais essentiel ici)
    taux_couverture_emission_monetaire

FROM
    default_silver.dim_uemoa_indicators
-- NOTE: Assurez-vous que votre modèle Silver 'dim_uemoa_indicators'
-- inclut 'actifs_exterieurs_nets_bceao_avoirs_officiels' et 'importations_biens_fob'
WHERE
    importations_biens_fob IS NOT NULL AND importations_biens_fob != 0
ORDER BY
    date DESC
  
[0m17:03:30.382556 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='42703', errorCode=0, errorMessage="org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `actifs_exterieurs_nets_bceao_avoirs_officiels` cannot be resolved. Did you mean one of the following? [`poids_secteur_secondaire_pct`, `poids_secteur_tertiaire_pct`, `poids_secteur_primaire_pct`, `compte_transactions_courantes`, `encours_de_la_dette_pct_pib`].; line 26 pos 4;\n'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false\n:- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@46a6df7a, default_gold.gold_mart_uemoa_external_stability\n+- 'Sort ['date DESC NULLS LAST], true\n   +- 'Project [date#544, 'actifs_exterieurs_nets_bceao_avoirs_officiels, importations_biens_fob#559, (importations_biens_fob#559 / cast(12 as double)) AS importations_mensuelles_moyennes#542, ('actifs_exterieurs_nets_bceao_avoirs_officiels / (importations_biens_fob#559 / cast(12 as double))) AS reserves_en_mois_importations#543, taux_couverture_emission_monetaire#564]\n      +- Filter (isnotnull(importations_biens_fob#559) AND NOT (importations_biens_fob#559 = cast(0 as double)))\n         +- SubqueryAlias spark_catalog.default_silver.dim_uemoa_indicators\n            +- RelationV2[date#544, pib_nominal_milliards_fcfa#545, poids_secteur_primaire_pct#546, poids_secteur_secondaire_pct#547, poids_secteur_tertiaire_pct#548, taux_croissance_reel_pib_pct#549, taux_inflation_moyen_annuel_ipc_pct#550, recettes_fiscales#551, recettes_fiscales_pct_pib#552, depenses_totales_et_prets_nets#553, solde_budgetaire_global_avec_dons#554, solde_budgetaire_global_hors_dons#555, encours_de_la_dette#556, encours_de_la_dette_pct_pib#557, exportations_biens_fob#558, importations_biens_fob#559, balance_des_biens#560, compte_transactions_courantes#561, balance_courante_sur_pib_pct#562, agregats_monnaie_masse_monetaire_m2#563, taux_couverture_emission_monetaire#564] spark_catalog.default_silver.dim_uemoa_indicators spark_catalog.default_silver.dim_uemoa_indicators\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `actifs_exterieurs_nets_bceao_avoirs_officiels` cannot be resolved. Did you mean one of the following? [`poids_secteur_secondaire_pct`, `poids_secteur_tertiaire_pct`, `poids_secteur_primaire_pct`, `compte_transactions_courantes`, `encours_de_la_dette_pct_pib`].; line 26 pos 4;\n'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false\n:- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@46a6df7a, default_gold.gold_mart_uemoa_external_stability\n+- 'Sort ['date DESC NULLS LAST], true\n   +- 'Project [date#544, 'actifs_exterieurs_nets_bceao_avoirs_officiels, importations_biens_fob#559, (importations_biens_fob#559 / cast(12 as double)) AS importations_mensuelles_moyennes#542, ('actifs_exterieurs_nets_bceao_avoirs_officiels / (importations_biens_fob#559 / cast(12 as double))) AS reserves_en_mois_importations#543, taux_couverture_emission_monetaire#564]\n      +- Filter (isnotnull(importations_biens_fob#559) AND NOT (importations_biens_fob#559 = cast(0 as double)))\n         +- SubqueryAlias spark_catalog.default_silver.dim_uemoa_indicators\n            +- RelationV2[date#544, pib_nominal_milliards_fcfa#545, poids_secteur_primaire_pct#546, poids_secteur_secondaire_pct#547, poids_secteur_tertiaire_pct#548, taux_croissance_reel_pib_pct#549, taux_inflation_moyen_annuel_ipc_pct#550, recettes_fiscales#551, recettes_fiscales_pct_pib#552, depenses_totales_et_prets_nets#553, solde_budgetaire_global_avec_dons#554, solde_budgetaire_global_hors_dons#555, encours_de_la_dette#556, encours_de_la_dette_pct_pib#557, exportations_biens_fob#558, importations_biens_fob#559, balance_des_biens#560, compte_transactions_courantes#561, balance_courante_sur_pib_pct#562, agregats_monnaie_masse_monetaire_m2#563, taux_couverture_emission_monetaire#564] spark_catalog.default_silver.dim_uemoa_indicators spark_catalog.default_silver.dim_uemoa_indicators\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)\n\tat scala.collection.immutable.Stream.foreach(Stream.scala:533)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n", taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m17:03:30.388619 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m17:03:30.392199 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.gold_mart_uemoa_external_stability"} */

  
    
        create or replace table default_gold.gold_mart_uemoa_external_stability
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Modèle Gold : Indicateurs de soutenabilité externe.
-- Calcule la couverture des importations par les réserves de change.

SELECT
    date,
    
    -- Avoirs officiels de réserve (nos devises)
    actifs_exterieurs_nets_bceao_avoirs_officiels,
    
    -- Importations annuelles
    importations_biens_fob,
    
    -- Importations mensuelles moyennes
    (importations_biens_fob / 12) as importations_mensuelles_moyennes,
    
    -- LE KPI CLÉ: Réserves en Mois d'Importations
    -- (Combien de mois d'importations peut-on couvrir avec nos réserves ?)
    (actifs_exterieurs_nets_bceao_avoirs_officiels / (importations_biens_fob / 12)) as reserves_en_mois_importations,

    -- Taux de couverture (déjà présent, mais essentiel ici)
    taux_couverture_emission_monetaire

FROM
    default_silver.dim_uemoa_indicators
-- NOTE: Assurez-vous que votre modèle Silver 'dim_uemoa_indicators'
-- inclut 'actifs_exterieurs_nets_bceao_avoirs_officiels' et 'importations_biens_fob'
WHERE
    importations_biens_fob IS NOT NULL AND importations_biens_fob != 0
ORDER BY
    date DESC
  
[0m17:03:30.395451 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `actifs_exterieurs_nets_bceao_avoirs_officiels` cannot be resolved. Did you mean one of the following? [`poids_secteur_secondaire_pct`, `poids_secteur_tertiaire_pct`, `poids_secteur_primaire_pct`, `compte_transactions_courantes`, `encours_de_la_dette_pct_pib`].; line 26 pos 4;
  'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false
  :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@46a6df7a, default_gold.gold_mart_uemoa_external_stability
  +- 'Sort ['date DESC NULLS LAST], true
     +- 'Project [date#544, 'actifs_exterieurs_nets_bceao_avoirs_officiels, importations_biens_fob#559, (importations_biens_fob#559 / cast(12 as double)) AS importations_mensuelles_moyennes#542, ('actifs_exterieurs_nets_bceao_avoirs_officiels / (importations_biens_fob#559 / cast(12 as double))) AS reserves_en_mois_importations#543, taux_couverture_emission_monetaire#564]
        +- Filter (isnotnull(importations_biens_fob#559) AND NOT (importations_biens_fob#559 = cast(0 as double)))
           +- SubqueryAlias spark_catalog.default_silver.dim_uemoa_indicators
              +- RelationV2[date#544, pib_nominal_milliards_fcfa#545, poids_secteur_primaire_pct#546, poids_secteur_secondaire_pct#547, poids_secteur_tertiaire_pct#548, taux_croissance_reel_pib_pct#549, taux_inflation_moyen_annuel_ipc_pct#550, recettes_fiscales#551, recettes_fiscales_pct_pib#552, depenses_totales_et_prets_nets#553, solde_budgetaire_global_avec_dons#554, solde_budgetaire_global_hors_dons#555, encours_de_la_dette#556, encours_de_la_dette_pct_pib#557, exportations_biens_fob#558, importations_biens_fob#559, balance_des_biens#560, compte_transactions_courantes#561, balance_courante_sur_pib_pct#562, agregats_monnaie_masse_monetaire_m2#563, taux_couverture_emission_monetaire#564] spark_catalog.default_silver.dim_uemoa_indicators spark_catalog.default_silver.dim_uemoa_indicators
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `actifs_exterieurs_nets_bceao_avoirs_officiels` cannot be resolved. Did you mean one of the following? [`poids_secteur_secondaire_pct`, `poids_secteur_tertiaire_pct`, `poids_secteur_primaire_pct`, `compte_transactions_courantes`, `encours_de_la_dette_pct_pib`].; line 26 pos 4;
  'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false
  :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@46a6df7a, default_gold.gold_mart_uemoa_external_stability
  +- 'Sort ['date DESC NULLS LAST], true
     +- 'Project [date#544, 'actifs_exterieurs_nets_bceao_avoirs_officiels, importations_biens_fob#559, (importations_biens_fob#559 / cast(12 as double)) AS importations_mensuelles_moyennes#542, ('actifs_exterieurs_nets_bceao_avoirs_officiels / (importations_biens_fob#559 / cast(12 as double))) AS reserves_en_mois_importations#543, taux_couverture_emission_monetaire#564]
        +- Filter (isnotnull(importations_biens_fob#559) AND NOT (importations_biens_fob#559 = cast(0 as double)))
           +- SubqueryAlias spark_catalog.default_silver.dim_uemoa_indicators
              +- RelationV2[date#544, pib_nominal_milliards_fcfa#545, poids_secteur_primaire_pct#546, poids_secteur_secondaire_pct#547, poids_secteur_tertiaire_pct#548, taux_croissance_reel_pib_pct#549, taux_inflation_moyen_annuel_ipc_pct#550, recettes_fiscales#551, recettes_fiscales_pct_pib#552, depenses_totales_et_prets_nets#553, solde_budgetaire_global_avec_dons#554, solde_budgetaire_global_hors_dons#555, encours_de_la_dette#556, encours_de_la_dette_pct_pib#557, exportations_biens_fob#558, importations_biens_fob#559, balance_des_biens#560, compte_transactions_courantes#561, balance_courante_sur_pib_pct#562, agregats_monnaie_masse_monetaire_m2#563, taux_couverture_emission_monetaire#564] spark_catalog.default_silver.dim_uemoa_indicators spark_catalog.default_silver.dim_uemoa_indicators
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
  	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
  	at scala.collection.immutable.List.foreach(List.scala:431)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m17:03:30.399117 [debug] [Thread-1 (]: On model.data_pipeline_poc.gold_mart_uemoa_external_stability: ROLLBACK
[0m17:03:30.402297 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m17:03:30.405416 [debug] [Thread-1 (]: On model.data_pipeline_poc.gold_mart_uemoa_external_stability: Close
[0m17:03:30.414882 [debug] [Thread-1 (]: Runtime Error in model gold_mart_uemoa_external_stability (models/gold/gold_mart_uemoa_external_stability.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `actifs_exterieurs_nets_bceao_avoirs_officiels` cannot be resolved. Did you mean one of the following? [`poids_secteur_secondaire_pct`, `poids_secteur_tertiaire_pct`, `poids_secteur_primaire_pct`, `compte_transactions_courantes`, `encours_de_la_dette_pct_pib`].; line 26 pos 4;
    'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false
    :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@46a6df7a, default_gold.gold_mart_uemoa_external_stability
    +- 'Sort ['date DESC NULLS LAST], true
       +- 'Project [date#544, 'actifs_exterieurs_nets_bceao_avoirs_officiels, importations_biens_fob#559, (importations_biens_fob#559 / cast(12 as double)) AS importations_mensuelles_moyennes#542, ('actifs_exterieurs_nets_bceao_avoirs_officiels / (importations_biens_fob#559 / cast(12 as double))) AS reserves_en_mois_importations#543, taux_couverture_emission_monetaire#564]
          +- Filter (isnotnull(importations_biens_fob#559) AND NOT (importations_biens_fob#559 = cast(0 as double)))
             +- SubqueryAlias spark_catalog.default_silver.dim_uemoa_indicators
                +- RelationV2[date#544, pib_nominal_milliards_fcfa#545, poids_secteur_primaire_pct#546, poids_secteur_secondaire_pct#547, poids_secteur_tertiaire_pct#548, taux_croissance_reel_pib_pct#549, taux_inflation_moyen_annuel_ipc_pct#550, recettes_fiscales#551, recettes_fiscales_pct_pib#552, depenses_totales_et_prets_nets#553, solde_budgetaire_global_avec_dons#554, solde_budgetaire_global_hors_dons#555, encours_de_la_dette#556, encours_de_la_dette_pct_pib#557, exportations_biens_fob#558, importations_biens_fob#559, balance_des_biens#560, compte_transactions_courantes#561, balance_courante_sur_pib_pct#562, agregats_monnaie_masse_monetaire_m2#563, taux_couverture_emission_monetaire#564] spark_catalog.default_silver.dim_uemoa_indicators spark_catalog.default_silver.dim_uemoa_indicators
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `actifs_exterieurs_nets_bceao_avoirs_officiels` cannot be resolved. Did you mean one of the following? [`poids_secteur_secondaire_pct`, `poids_secteur_tertiaire_pct`, `poids_secteur_primaire_pct`, `compte_transactions_courantes`, `encours_de_la_dette_pct_pib`].; line 26 pos 4;
    'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false
    :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@46a6df7a, default_gold.gold_mart_uemoa_external_stability
    +- 'Sort ['date DESC NULLS LAST], true
       +- 'Project [date#544, 'actifs_exterieurs_nets_bceao_avoirs_officiels, importations_biens_fob#559, (importations_biens_fob#559 / cast(12 as double)) AS importations_mensuelles_moyennes#542, ('actifs_exterieurs_nets_bceao_avoirs_officiels / (importations_biens_fob#559 / cast(12 as double))) AS reserves_en_mois_importations#543, taux_couverture_emission_monetaire#564]
          +- Filter (isnotnull(importations_biens_fob#559) AND NOT (importations_biens_fob#559 = cast(0 as double)))
             +- SubqueryAlias spark_catalog.default_silver.dim_uemoa_indicators
                +- RelationV2[date#544, pib_nominal_milliards_fcfa#545, poids_secteur_primaire_pct#546, poids_secteur_secondaire_pct#547, poids_secteur_tertiaire_pct#548, taux_croissance_reel_pib_pct#549, taux_inflation_moyen_annuel_ipc_pct#550, recettes_fiscales#551, recettes_fiscales_pct_pib#552, depenses_totales_et_prets_nets#553, solde_budgetaire_global_avec_dons#554, solde_budgetaire_global_hors_dons#555, encours_de_la_dette#556, encours_de_la_dette_pct_pib#557, exportations_biens_fob#558, importations_biens_fob#559, balance_des_biens#560, compte_transactions_courantes#561, balance_courante_sur_pib_pct#562, agregats_monnaie_masse_monetaire_m2#563, taux_couverture_emission_monetaire#564] spark_catalog.default_silver.dim_uemoa_indicators spark_catalog.default_silver.dim_uemoa_indicators
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m17:03:30.417875 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6a225c2c-d292-44d1-85ce-6f258f07aa8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77ee8885ae90>]}
[0m17:03:30.420236 [error] [Thread-1 (]: 5 of 9 ERROR creating sql table model default_gold.gold_mart_uemoa_external_stability  [[31mERROR[0m in 0.22s]
[0m17:03:30.423820 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.gold_mart_uemoa_external_stability
[0m17:03:30.426672 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.gold_mart_uemoa_external_trade
[0m17:03:30.427393 [debug] [Thread-4 (]: Marking all children of 'model.data_pipeline_poc.gold_mart_uemoa_external_stability' to be skipped because of status 'error'.  Reason: Runtime Error in model gold_mart_uemoa_external_stability (models/gold/gold_mart_uemoa_external_stability.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `actifs_exterieurs_nets_bceao_avoirs_officiels` cannot be resolved. Did you mean one of the following? [`poids_secteur_secondaire_pct`, `poids_secteur_tertiaire_pct`, `poids_secteur_primaire_pct`, `compte_transactions_courantes`, `encours_de_la_dette_pct_pib`].; line 26 pos 4;
    'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false
    :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@46a6df7a, default_gold.gold_mart_uemoa_external_stability
    +- 'Sort ['date DESC NULLS LAST], true
       +- 'Project [date#544, 'actifs_exterieurs_nets_bceao_avoirs_officiels, importations_biens_fob#559, (importations_biens_fob#559 / cast(12 as double)) AS importations_mensuelles_moyennes#542, ('actifs_exterieurs_nets_bceao_avoirs_officiels / (importations_biens_fob#559 / cast(12 as double))) AS reserves_en_mois_importations#543, taux_couverture_emission_monetaire#564]
          +- Filter (isnotnull(importations_biens_fob#559) AND NOT (importations_biens_fob#559 = cast(0 as double)))
             +- SubqueryAlias spark_catalog.default_silver.dim_uemoa_indicators
                +- RelationV2[date#544, pib_nominal_milliards_fcfa#545, poids_secteur_primaire_pct#546, poids_secteur_secondaire_pct#547, poids_secteur_tertiaire_pct#548, taux_croissance_reel_pib_pct#549, taux_inflation_moyen_annuel_ipc_pct#550, recettes_fiscales#551, recettes_fiscales_pct_pib#552, depenses_totales_et_prets_nets#553, solde_budgetaire_global_avec_dons#554, solde_budgetaire_global_hors_dons#555, encours_de_la_dette#556, encours_de_la_dette_pct_pib#557, exportations_biens_fob#558, importations_biens_fob#559, balance_des_biens#560, compte_transactions_courantes#561, balance_courante_sur_pib_pct#562, agregats_monnaie_masse_monetaire_m2#563, taux_couverture_emission_monetaire#564] spark_catalog.default_silver.dim_uemoa_indicators spark_catalog.default_silver.dim_uemoa_indicators
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `actifs_exterieurs_nets_bceao_avoirs_officiels` cannot be resolved. Did you mean one of the following? [`poids_secteur_secondaire_pct`, `poids_secteur_tertiaire_pct`, `poids_secteur_primaire_pct`, `compte_transactions_courantes`, `encours_de_la_dette_pct_pib`].; line 26 pos 4;
    'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false
    :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@46a6df7a, default_gold.gold_mart_uemoa_external_stability
    +- 'Sort ['date DESC NULLS LAST], true
       +- 'Project [date#544, 'actifs_exterieurs_nets_bceao_avoirs_officiels, importations_biens_fob#559, (importations_biens_fob#559 / cast(12 as double)) AS importations_mensuelles_moyennes#542, ('actifs_exterieurs_nets_bceao_avoirs_officiels / (importations_biens_fob#559 / cast(12 as double))) AS reserves_en_mois_importations#543, taux_couverture_emission_monetaire#564]
          +- Filter (isnotnull(importations_biens_fob#559) AND NOT (importations_biens_fob#559 = cast(0 as double)))
             +- SubqueryAlias spark_catalog.default_silver.dim_uemoa_indicators
                +- RelationV2[date#544, pib_nominal_milliards_fcfa#545, poids_secteur_primaire_pct#546, poids_secteur_secondaire_pct#547, poids_secteur_tertiaire_pct#548, taux_croissance_reel_pib_pct#549, taux_inflation_moyen_annuel_ipc_pct#550, recettes_fiscales#551, recettes_fiscales_pct_pib#552, depenses_totales_et_prets_nets#553, solde_budgetaire_global_avec_dons#554, solde_budgetaire_global_hors_dons#555, encours_de_la_dette#556, encours_de_la_dette_pct_pib#557, exportations_biens_fob#558, importations_biens_fob#559, balance_des_biens#560, compte_transactions_courantes#561, balance_courante_sur_pib_pct#562, agregats_monnaie_masse_monetaire_m2#563, taux_couverture_emission_monetaire#564] spark_catalog.default_silver.dim_uemoa_indicators spark_catalog.default_silver.dim_uemoa_indicators
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    .
[0m17:03:30.429426 [info ] [Thread-1 (]: 6 of 9 START sql table model default_gold.gold_mart_uemoa_external_trade ....... [RUN]
[0m17:03:30.434673 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline_poc.gold_mart_uemoa_external_stability, now model.data_pipeline_poc.gold_mart_uemoa_external_trade)
[0m17:03:30.437121 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_poc.gold_mart_uemoa_external_trade
[0m17:03:30.442595 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_poc.gold_mart_uemoa_external_trade"
[0m17:03:30.469544 [debug] [Thread-1 (]: Began executing node model.data_pipeline_poc.gold_mart_uemoa_external_trade
[0m17:03:30.475691 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.gold_mart_uemoa_external_trade"
[0m17:03:30.478012 [debug] [Thread-1 (]: On model.data_pipeline_poc.gold_mart_uemoa_external_trade: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.gold_mart_uemoa_external_trade"} */
drop table if exists default_gold.gold_mart_uemoa_external_trade
[0m17:03:30.479906 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m17:03:30.523631 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m17:03:30.526243 [debug] [Thread-1 (]: SQL status: OK in 0.046 seconds
[0m17:03:30.529799 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_poc.gold_mart_uemoa_external_trade"
[0m17:03:30.552502 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m17:03:30.554557 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.gold_mart_uemoa_external_trade"
[0m17:03:30.557088 [debug] [Thread-1 (]: On model.data_pipeline_poc.gold_mart_uemoa_external_trade: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.gold_mart_uemoa_external_trade"} */

  
    
        create or replace table default_gold.gold_mart_uemoa_external_trade
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Modèle Gold : Data mart focalisé sur le commerce extérieur et la balance des paiements.
-- Prêt à être consommé par un outil de BI pour la BCEAO ou le Commerce.
SELECT
    date,
    pib_nominal_milliards_fcfa,
    
    -- Balance commerciale
    exportations_biens_fob,
    importations_biens_fob,
    balance_des_biens,
    (balance_des_biens / pib_nominal_milliards_fcfa) * 100 as balance_des_biens_pct_pib,

    -- Balance courante
    compte_transactions_courantes,
    balance_courante_sur_pib_pct

FROM
    default_silver.dim_uemoa_indicators
ORDER BY
    date DESC
  
[0m17:03:31.591802 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m17:03:31.593277 [debug] [Thread-1 (]: SQL status: OK in 1.034 seconds
[0m17:03:31.595375 [debug] [Thread-1 (]: On model.data_pipeline_poc.gold_mart_uemoa_external_trade: ROLLBACK
[0m17:03:31.596318 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m17:03:31.596994 [debug] [Thread-1 (]: On model.data_pipeline_poc.gold_mart_uemoa_external_trade: Close
[0m17:03:31.600667 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6a225c2c-d292-44d1-85ce-6f258f07aa8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77ee898dd990>]}
[0m17:03:31.602740 [info ] [Thread-1 (]: 6 of 9 OK created sql table model default_gold.gold_mart_uemoa_external_trade .. [[32mOK[0m in 1.17s]
[0m17:03:31.604456 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.gold_mart_uemoa_external_trade
[0m17:03:31.607640 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.gold_mart_uemoa_monetary_dashboard
[0m17:03:31.609721 [info ] [Thread-1 (]: 7 of 9 START sql table model default_gold.gold_mart_uemoa_monetary_dashboard ... [RUN]
[0m17:03:31.611789 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline_poc.gold_mart_uemoa_external_trade, now model.data_pipeline_poc.gold_mart_uemoa_monetary_dashboard)
[0m17:03:31.613649 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_poc.gold_mart_uemoa_monetary_dashboard
[0m17:03:31.618861 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_poc.gold_mart_uemoa_monetary_dashboard"
[0m17:03:31.641335 [debug] [Thread-1 (]: Began executing node model.data_pipeline_poc.gold_mart_uemoa_monetary_dashboard
[0m17:03:31.646404 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.gold_mart_uemoa_monetary_dashboard"
[0m17:03:31.648883 [debug] [Thread-1 (]: On model.data_pipeline_poc.gold_mart_uemoa_monetary_dashboard: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.gold_mart_uemoa_monetary_dashboard"} */
drop table if exists default_gold.gold_mart_uemoa_monetary_dashboard
[0m17:03:31.650960 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m17:03:31.690143 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m17:03:31.693358 [debug] [Thread-1 (]: SQL status: OK in 0.042 seconds
[0m17:03:31.697556 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_poc.gold_mart_uemoa_monetary_dashboard"
[0m17:03:31.730030 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m17:03:31.732233 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.gold_mart_uemoa_monetary_dashboard"
[0m17:03:31.734255 [debug] [Thread-1 (]: On model.data_pipeline_poc.gold_mart_uemoa_monetary_dashboard: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.gold_mart_uemoa_monetary_dashboard"} */

  
    
        create or replace table default_gold.gold_mart_uemoa_monetary_dashboard
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Modèle Gold : Prêt pour un tableau de bord de pilotage monétaire.
-- Calcule la croissance des agrégats et les ratios clés.

SELECT
    date,
    pib_nominal_milliards_fcfa,
    agregats_monnaie_masse_monetaire_m2,
    taux_inflation_moyen_annuel_ipc_pct,
    
    -- Croissance Année-sur-Année (YoY) de la masse monétaire
    (agregats_monnaie_masse_monetaire_m2 - LAG(agregats_monnaie_masse_monetaire_m2, 1) OVER (ORDER BY date))
        / LAG(agregats_monnaie_masse_monetaire_m2, 1) OVER (ORDER BY date) * 100 as m2_croissance_yoy_pct,

    -- Vélocité de la monnaie (V) : vitesse à laquelle l'argent circule
    -- V = PIB Nominal / M2
    pib_nominal_milliards_fcfa / agregats_monnaie_masse_monetaire_m2 as velocite_monnaie,

    -- Taux de couverture de l'émission (déjà présent, mais vital)
    taux_couverture_emission_monetaire

FROM
    default_silver.dim_uemoa_indicators
-- NOTE: Assurez-vous que votre modèle Silver 'dim_uemoa_indicators'
-- inclut bien 'agregats_monnaie_masse_monetaire_m2' et 'taux_couverture_emission_monetaire'
WHERE
    agregats_monnaie_masse_monetaire_m2 IS NOT NULL
ORDER BY
    date DESC
  
[0m17:03:32.454955 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m17:03:32.457911 [debug] [Thread-1 (]: SQL status: OK in 0.722 seconds
[0m17:03:32.460907 [debug] [Thread-1 (]: On model.data_pipeline_poc.gold_mart_uemoa_monetary_dashboard: ROLLBACK
[0m17:03:32.462607 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m17:03:32.464051 [debug] [Thread-1 (]: On model.data_pipeline_poc.gold_mart_uemoa_monetary_dashboard: Close
[0m17:03:32.470638 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6a225c2c-d292-44d1-85ce-6f258f07aa8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77ee89125f10>]}
[0m17:03:32.474195 [info ] [Thread-1 (]: 7 of 9 OK created sql table model default_gold.gold_mart_uemoa_monetary_dashboard  [[32mOK[0m in 0.86s]
[0m17:03:32.475918 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.gold_mart_uemoa_monetary_dashboard
[0m17:03:32.477387 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.gold_mart_uemoa_public_finance
[0m17:03:32.478708 [info ] [Thread-1 (]: 8 of 9 START sql table model default_gold.gold_mart_uemoa_public_finance ....... [RUN]
[0m17:03:32.479988 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline_poc.gold_mart_uemoa_monetary_dashboard, now model.data_pipeline_poc.gold_mart_uemoa_public_finance)
[0m17:03:32.481356 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_poc.gold_mart_uemoa_public_finance
[0m17:03:32.484724 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_poc.gold_mart_uemoa_public_finance"
[0m17:03:32.503531 [debug] [Thread-1 (]: Began executing node model.data_pipeline_poc.gold_mart_uemoa_public_finance
[0m17:03:32.508148 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.gold_mart_uemoa_public_finance"
[0m17:03:32.509557 [debug] [Thread-1 (]: On model.data_pipeline_poc.gold_mart_uemoa_public_finance: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.gold_mart_uemoa_public_finance"} */
drop table if exists default_gold.gold_mart_uemoa_public_finance
[0m17:03:32.510677 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m17:03:32.554268 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m17:03:32.557529 [debug] [Thread-1 (]: SQL status: OK in 0.047 seconds
[0m17:03:32.560903 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_poc.gold_mart_uemoa_public_finance"
[0m17:03:32.578935 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m17:03:32.580035 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.gold_mart_uemoa_public_finance"
[0m17:03:32.581513 [debug] [Thread-1 (]: On model.data_pipeline_poc.gold_mart_uemoa_public_finance: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.gold_mart_uemoa_public_finance"} */

  
    
        create or replace table default_gold.gold_mart_uemoa_public_finance
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Modèle Gold : Data mart focalisé sur les finances publiques.
-- Prêt à être consommé par un outil de BI pour le Ministère des Finances.
SELECT
    date,
    pib_nominal_milliards_fcfa,
    
    -- Section Recettes
    recettes_fiscales,
    recettes_fiscales_pct_pib,
    
    -- Section Dépenses
    depenses_totales_et_prets_nets,
    
    -- Section Soldes
    solde_budgetaire_global_avec_dons,
    solde_budgetaire_global_hors_dons,
    -- On recalcule le solde en % du PIB pour être sûr
    (solde_budgetaire_global_avec_dons / pib_nominal_milliards_fcfa) * 100 as solde_budgetaire_avec_dons_pct_pib,
    
    -- Section Dette
    encours_de_la_dette,
    encours_de_la_dette_pct_pib

FROM
    default_silver.dim_uemoa_indicators
WHERE
    -- On peut se concentrer sur une période plus récente pour un dashboard
    EXTRACT(YEAR FROM date) >= 2010
ORDER BY
    date DESC
  
[0m17:03:33.461183 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m17:03:33.463232 [debug] [Thread-1 (]: SQL status: OK in 0.880 seconds
[0m17:03:33.466246 [debug] [Thread-1 (]: On model.data_pipeline_poc.gold_mart_uemoa_public_finance: ROLLBACK
[0m17:03:33.467866 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m17:03:33.469351 [debug] [Thread-1 (]: On model.data_pipeline_poc.gold_mart_uemoa_public_finance: Close
[0m17:03:33.474783 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6a225c2c-d292-44d1-85ce-6f258f07aa8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77ee894fe610>]}
[0m17:03:33.477242 [info ] [Thread-1 (]: 8 of 9 OK created sql table model default_gold.gold_mart_uemoa_public_finance .. [[32mOK[0m in 0.99s]
[0m17:03:33.479657 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.gold_mart_uemoa_public_finance
[0m17:03:33.481816 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.fct_events_enriched
[0m17:03:33.483877 [info ] [Thread-1 (]: 9 of 9 START sql table model default_default_gold.fct_events_enriched .......... [RUN]
[0m17:03:33.486150 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline_poc.gold_mart_uemoa_public_finance, now model.data_pipeline_poc.fct_events_enriched)
[0m17:03:33.489325 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_poc.fct_events_enriched
[0m17:03:33.493876 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_poc.fct_events_enriched"
[0m17:03:33.515648 [debug] [Thread-1 (]: Began executing node model.data_pipeline_poc.fct_events_enriched
[0m17:03:33.520491 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.fct_events_enriched"
[0m17:03:33.522705 [debug] [Thread-1 (]: On model.data_pipeline_poc.fct_events_enriched: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.fct_events_enriched"} */
drop table if exists default_default_gold.fct_events_enriched
[0m17:03:33.524747 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m17:03:33.564630 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m17:03:33.567462 [debug] [Thread-1 (]: SQL status: OK in 0.043 seconds
[0m17:03:33.571327 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_poc.fct_events_enriched"
[0m17:03:33.594335 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m17:03:33.596498 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.fct_events_enriched"
[0m17:03:33.598630 [debug] [Thread-1 (]: On model.data_pipeline_poc.fct_events_enriched: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.fct_events_enriched"} */

  
    
        create or replace table default_default_gold.fct_events_enriched
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Mart model: Events enriched with user information
-- This model joins event data with user data for analytics

WITH events AS (
    SELECT * FROM default_default_silver.stg_events
),

users AS (
    SELECT * FROM default_default_silver.stg_users
),

enriched AS (
    SELECT
        e.event_id,
        e.event_type,
        e.event_timestamp,
        e.event_data,
        e.user_id,
        u.user_name,
        u.user_email,
        u.created_at as user_created_at,
        e.dbt_loaded_at
    FROM events e
    LEFT JOIN users u ON e.user_id = u.user_id
)

SELECT * FROM enriched
  
[0m17:03:34.734409 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m17:03:34.737474 [debug] [Thread-1 (]: SQL status: OK in 1.137 seconds
[0m17:03:34.741833 [debug] [Thread-1 (]: On model.data_pipeline_poc.fct_events_enriched: ROLLBACK
[0m17:03:34.744552 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m17:03:34.746950 [debug] [Thread-1 (]: On model.data_pipeline_poc.fct_events_enriched: Close
[0m17:03:34.753193 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6a225c2c-d292-44d1-85ce-6f258f07aa8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77ee897232d0>]}
[0m17:03:34.756628 [info ] [Thread-1 (]: 9 of 9 OK created sql table model default_default_gold.fct_events_enriched ..... [[32mOK[0m in 1.27s]
[0m17:03:34.758819 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.fct_events_enriched
[0m17:03:34.762181 [debug] [MainThread]: On master: ROLLBACK
[0m17:03:34.763972 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:03:34.799696 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m17:03:34.802042 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:03:34.804105 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:03:34.806028 [debug] [MainThread]: On master: ROLLBACK
[0m17:03:34.808415 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m17:03:34.810606 [debug] [MainThread]: On master: Close
[0m17:03:34.815205 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:03:34.817630 [debug] [MainThread]: Connection 'model.data_pipeline_poc.fct_events_enriched' was properly closed.
[0m17:03:34.819632 [info ] [MainThread]: 
[0m17:03:34.821161 [info ] [MainThread]: Finished running 9 table models in 0 hours 0 minutes and 10.35 seconds (10.35s).
[0m17:03:34.825046 [debug] [MainThread]: Command end result
[0m17:03:34.908548 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/app/dbt/target/manifest.json
[0m17:03:34.921871 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/app/dbt/target/semantic_manifest.json
[0m17:03:34.943485 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/app/dbt/target/run_results.json
[0m17:03:34.945805 [info ] [MainThread]: 
[0m17:03:34.948209 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m17:03:34.950728 [info ] [MainThread]: 
[0m17:03:34.953776 [error] [MainThread]:   Runtime Error in model gold_mart_uemoa_external_stability (models/gold/gold_mart_uemoa_external_stability.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `actifs_exterieurs_nets_bceao_avoirs_officiels` cannot be resolved. Did you mean one of the following? [`poids_secteur_secondaire_pct`, `poids_secteur_tertiaire_pct`, `poids_secteur_primaire_pct`, `compte_transactions_courantes`, `encours_de_la_dette_pct_pib`].; line 26 pos 4;
    'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false
    :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@46a6df7a, default_gold.gold_mart_uemoa_external_stability
    +- 'Sort ['date DESC NULLS LAST], true
       +- 'Project [date#544, 'actifs_exterieurs_nets_bceao_avoirs_officiels, importations_biens_fob#559, (importations_biens_fob#559 / cast(12 as double)) AS importations_mensuelles_moyennes#542, ('actifs_exterieurs_nets_bceao_avoirs_officiels / (importations_biens_fob#559 / cast(12 as double))) AS reserves_en_mois_importations#543, taux_couverture_emission_monetaire#564]
          +- Filter (isnotnull(importations_biens_fob#559) AND NOT (importations_biens_fob#559 = cast(0 as double)))
             +- SubqueryAlias spark_catalog.default_silver.dim_uemoa_indicators
                +- RelationV2[date#544, pib_nominal_milliards_fcfa#545, poids_secteur_primaire_pct#546, poids_secteur_secondaire_pct#547, poids_secteur_tertiaire_pct#548, taux_croissance_reel_pib_pct#549, taux_inflation_moyen_annuel_ipc_pct#550, recettes_fiscales#551, recettes_fiscales_pct_pib#552, depenses_totales_et_prets_nets#553, solde_budgetaire_global_avec_dons#554, solde_budgetaire_global_hors_dons#555, encours_de_la_dette#556, encours_de_la_dette_pct_pib#557, exportations_biens_fob#558, importations_biens_fob#559, balance_des_biens#560, compte_transactions_courantes#561, balance_courante_sur_pib_pct#562, agregats_monnaie_masse_monetaire_m2#563, taux_couverture_emission_monetaire#564] spark_catalog.default_silver.dim_uemoa_indicators spark_catalog.default_silver.dim_uemoa_indicators
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `actifs_exterieurs_nets_bceao_avoirs_officiels` cannot be resolved. Did you mean one of the following? [`poids_secteur_secondaire_pct`, `poids_secteur_tertiaire_pct`, `poids_secteur_primaire_pct`, `compte_transactions_courantes`, `encours_de_la_dette_pct_pib`].; line 26 pos 4;
    'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false
    :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@46a6df7a, default_gold.gold_mart_uemoa_external_stability
    +- 'Sort ['date DESC NULLS LAST], true
       +- 'Project [date#544, 'actifs_exterieurs_nets_bceao_avoirs_officiels, importations_biens_fob#559, (importations_biens_fob#559 / cast(12 as double)) AS importations_mensuelles_moyennes#542, ('actifs_exterieurs_nets_bceao_avoirs_officiels / (importations_biens_fob#559 / cast(12 as double))) AS reserves_en_mois_importations#543, taux_couverture_emission_monetaire#564]
          +- Filter (isnotnull(importations_biens_fob#559) AND NOT (importations_biens_fob#559 = cast(0 as double)))
             +- SubqueryAlias spark_catalog.default_silver.dim_uemoa_indicators
                +- RelationV2[date#544, pib_nominal_milliards_fcfa#545, poids_secteur_primaire_pct#546, poids_secteur_secondaire_pct#547, poids_secteur_tertiaire_pct#548, taux_croissance_reel_pib_pct#549, taux_inflation_moyen_annuel_ipc_pct#550, recettes_fiscales#551, recettes_fiscales_pct_pib#552, depenses_totales_et_prets_nets#553, solde_budgetaire_global_avec_dons#554, solde_budgetaire_global_hors_dons#555, encours_de_la_dette#556, encours_de_la_dette_pct_pib#557, exportations_biens_fob#558, importations_biens_fob#559, balance_des_biens#560, compte_transactions_courantes#561, balance_courante_sur_pib_pct#562, agregats_monnaie_masse_monetaire_m2#563, taux_couverture_emission_monetaire#564] spark_catalog.default_silver.dim_uemoa_indicators spark_catalog.default_silver.dim_uemoa_indicators
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m17:03:34.956545 [info ] [MainThread]: 
[0m17:03:34.960200 [info ] [MainThread]: Done. PASS=8 WARN=0 ERROR=1 SKIP=0 TOTAL=9
[0m17:03:34.963437 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 14.105348, "process_in_blocks": "0", "process_kernel_time": 0.301256, "process_mem_max_rss": "106432", "process_out_blocks": "0", "process_user_time": 1.579561}
[0m17:03:34.965797 [debug] [MainThread]: Command `dbt run` failed at 17:03:34.965673 after 14.11 seconds
[0m17:03:34.967988 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77ee8d6dd590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77ee89de0a10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77ee89d88ad0>]}
[0m17:03:34.970066 [debug] [MainThread]: Flushing usage events
[0m17:03:35.980697 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:09:33.916659 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4b1bbbf750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4b1bb63f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4b1bbbf1d0>]}


============================== 17:09:33.921149 | 6adaba87-4955-46e5-bfb7-1478795b3324 ==============================
[0m17:09:33.921149 [info ] [MainThread]: Running with dbt=1.9.0
[0m17:09:33.923275 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/usr/app/dbt/logs', 'version_check': 'True', 'profiles_dir': '/usr/app/dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m17:09:33.976329 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m17:09:33.979904 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m17:09:33.982169 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m17:09:34.083008 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6adaba87-4955-46e5-bfb7-1478795b3324', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4b1bbc6a90>]}
[0m17:09:34.120740 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6adaba87-4955-46e5-bfb7-1478795b3324', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4b1ca9add0>]}
[0m17:09:34.124632 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m17:09:34.211165 [debug] [MainThread]: checksum: c99e828bba267739642b5a3ce85f17518764ea526e0e6c4fdc649171c1a66bff, vars: {}, profile: , target: , version: 1.9.0
[0m17:09:37.656593 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m17:09:37.658838 [debug] [MainThread]: Partial parsing: updated file: data_pipeline_poc://models/gold/gold_mart_uemoa_external_stability.sql
[0m17:09:37.840914 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- seeds.data_pipeline_poc
- snapshots.data_pipeline_poc
[0m17:09:37.852091 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6adaba87-4955-46e5-bfb7-1478795b3324', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4b1af761d0>]}
[0m17:09:37.954528 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/app/dbt/target/manifest.json
[0m17:09:37.967552 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/app/dbt/target/semantic_manifest.json
[0m17:09:37.999113 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6adaba87-4955-46e5-bfb7-1478795b3324', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4b1afd4650>]}
[0m17:09:38.001464 [info ] [MainThread]: Found 9 models, 13 data tests, 3 sources, 583 macros
[0m17:09:38.003491 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6adaba87-4955-46e5-bfb7-1478795b3324', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4b1b193150>]}
[0m17:09:38.007086 [info ] [MainThread]: 
[0m17:09:38.008842 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:09:38.010986 [info ] [MainThread]: 
[0m17:09:38.013326 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m17:09:38.021937 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m17:09:38.030645 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m17:09:38.032422 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m17:09:38.035350 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:09:38.094474 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m17:09:38.097022 [debug] [ThreadPool]: SQL status: OK in 0.062 seconds
[0m17:09:38.102763 [debug] [ThreadPool]: On list_schemas: Close
[0m17:09:38.110299 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m17:09:38.112854 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m17:09:38.115165 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:09:38.156655 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m17:09:38.159126 [debug] [ThreadPool]: SQL status: OK in 0.044 seconds
[0m17:09:38.163626 [debug] [ThreadPool]: On list_schemas: Close
[0m17:09:38.169301 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m17:09:38.171535 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m17:09:38.173332 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:09:38.214530 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m17:09:38.217864 [debug] [ThreadPool]: SQL status: OK in 0.044 seconds
[0m17:09:38.223144 [debug] [ThreadPool]: On list_schemas: Close
[0m17:09:38.229584 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m17:09:38.232352 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m17:09:38.235246 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:09:38.276313 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m17:09:38.278879 [debug] [ThreadPool]: SQL status: OK in 0.044 seconds
[0m17:09:38.283690 [debug] [ThreadPool]: On list_schemas: Close
[0m17:09:38.289829 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default_default_silver)
[0m17:09:38.295285 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:09:38.297480 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m17:09:38.298829 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show table extended in default_default_silver like '*'
  
[0m17:09:38.300236 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:09:38.326511 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#791, tableName#792, isTemporary#793, information#794]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@61fdd4f6, [default_default_silver]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#791, tableName#792, isTemporary#793, information#794]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@61fdd4f6, [default_default_silver]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m17:09:38.328561 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m17:09:38.330067 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show table extended in default_default_silver like '*'
  
[0m17:09:38.331678 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#791, tableName#792, isTemporary#793, information#794]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@61fdd4f6, [default_default_silver]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#791, tableName#792, isTemporary#793, information#794]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@61fdd4f6, [default_default_silver]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m17:09:38.333595 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m17:09:38.335189 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#791, tableName#792, isTemporary#793, information#794]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@61fdd4f6, [default_default_silver]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#791, tableName#792, isTemporary#793, information#794]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@61fdd4f6, [default_default_silver]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m17:09:38.339480 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m17:09:38.340669 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show tables in default_default_silver like '*'
  
[0m17:09:38.360556 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m17:09:38.362333 [debug] [ThreadPool]: SQL status: OK in 0.020 seconds
[0m17:09:38.369123 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m17:09:38.370773 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
describe extended default_default_silver.stg_events
  
[0m17:09:38.414890 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m17:09:38.416792 [debug] [ThreadPool]: SQL status: OK in 0.044 seconds
[0m17:09:38.423293 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m17:09:38.424862 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
describe extended default_default_silver.stg_users
  
[0m17:09:38.464479 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m17:09:38.467936 [debug] [ThreadPool]: SQL status: OK in 0.042 seconds
[0m17:09:38.473371 [debug] [ThreadPool]: On list_None_default_default_silver: ROLLBACK
[0m17:09:38.475385 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m17:09:38.477598 [debug] [ThreadPool]: On list_None_default_default_silver: Close
[0m17:09:38.482214 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default_default_silver, now list_None_default_silver)
[0m17:09:38.486427 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:09:38.488966 [debug] [ThreadPool]: Using spark connection "list_None_default_silver"
[0m17:09:38.490999 [debug] [ThreadPool]: On list_None_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_silver"} */
show table extended in default_silver like '*'
  
[0m17:09:38.492804 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:09:38.521684 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#867, tableName#868, isTemporary#869, information#870]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@2d58721, [default_silver]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#867, tableName#868, isTemporary#869, information#870]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@2d58721, [default_silver]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m17:09:38.524153 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m17:09:38.526140 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_silver"} */
show table extended in default_silver like '*'
  
[0m17:09:38.528443 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#867, tableName#868, isTemporary#869, information#870]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@2d58721, [default_silver]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#867, tableName#868, isTemporary#869, information#870]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@2d58721, [default_silver]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m17:09:38.531040 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m17:09:38.533420 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#867, tableName#868, isTemporary#869, information#870]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@2d58721, [default_silver]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#867, tableName#868, isTemporary#869, information#870]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@2d58721, [default_silver]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m17:09:38.539753 [debug] [ThreadPool]: Using spark connection "list_None_default_silver"
[0m17:09:38.541752 [debug] [ThreadPool]: On list_None_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_silver"} */
show tables in default_silver like '*'
  
[0m17:09:38.562481 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m17:09:38.564648 [debug] [ThreadPool]: SQL status: OK in 0.021 seconds
[0m17:09:38.609337 [debug] [ThreadPool]: Using spark connection "list_None_default_silver"
[0m17:09:38.611901 [debug] [ThreadPool]: On list_None_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_silver"} */
describe extended default_silver.dim_uemoa_indicators
  
[0m17:09:38.653462 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m17:09:38.656608 [debug] [ThreadPool]: SQL status: OK in 0.043 seconds
[0m17:09:38.662520 [debug] [ThreadPool]: On list_None_default_silver: ROLLBACK
[0m17:09:38.665229 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m17:09:38.668766 [debug] [ThreadPool]: On list_None_default_silver: Close
[0m17:09:38.674959 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default_silver, now list_None_default_gold)
[0m17:09:38.678742 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:09:38.681311 [debug] [ThreadPool]: Using spark connection "list_None_default_gold"
[0m17:09:38.684260 [debug] [ThreadPool]: On list_None_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_gold"} */
show table extended in default_gold like '*'
  
[0m17:09:38.687016 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:09:38.718709 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#933, tableName#934, isTemporary#935, information#936]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@6194ac7f, [default_gold]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#933, tableName#934, isTemporary#935, information#936]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@6194ac7f, [default_gold]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m17:09:38.721709 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m17:09:38.723790 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_gold"} */
show table extended in default_gold like '*'
  
[0m17:09:38.725976 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#933, tableName#934, isTemporary#935, information#936]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@6194ac7f, [default_gold]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#933, tableName#934, isTemporary#935, information#936]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@6194ac7f, [default_gold]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m17:09:38.728311 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m17:09:38.730329 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#933, tableName#934, isTemporary#935, information#936]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@6194ac7f, [default_gold]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#933, tableName#934, isTemporary#935, information#936]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@6194ac7f, [default_gold]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m17:09:38.734215 [debug] [ThreadPool]: Using spark connection "list_None_default_gold"
[0m17:09:38.736494 [debug] [ThreadPool]: On list_None_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_gold"} */
show tables in default_gold like '*'
  
[0m17:09:38.759535 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m17:09:38.763266 [debug] [ThreadPool]: SQL status: OK in 0.024 seconds
[0m17:09:38.770635 [debug] [ThreadPool]: Using spark connection "list_None_default_gold"
[0m17:09:38.772916 [debug] [ThreadPool]: On list_None_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_gold"} */
describe extended default_gold.gold_kpi_uemoa_growth_yoy
  
[0m17:09:38.814068 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m17:09:38.816357 [debug] [ThreadPool]: SQL status: OK in 0.041 seconds
[0m17:09:38.825148 [debug] [ThreadPool]: Using spark connection "list_None_default_gold"
[0m17:09:38.827179 [debug] [ThreadPool]: On list_None_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_gold"} */
describe extended default_gold.gold_mart_uemoa_external_trade
  
[0m17:09:38.865616 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m17:09:38.868061 [debug] [ThreadPool]: SQL status: OK in 0.039 seconds
[0m17:09:38.875136 [debug] [ThreadPool]: Using spark connection "list_None_default_gold"
[0m17:09:38.878663 [debug] [ThreadPool]: On list_None_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_gold"} */
describe extended default_gold.gold_mart_uemoa_monetary_dashboard
  
[0m17:09:38.917921 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m17:09:38.920173 [debug] [ThreadPool]: SQL status: OK in 0.040 seconds
[0m17:09:38.926422 [debug] [ThreadPool]: Using spark connection "list_None_default_gold"
[0m17:09:38.929270 [debug] [ThreadPool]: On list_None_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_gold"} */
describe extended default_gold.gold_mart_uemoa_public_finance
  
[0m17:09:38.965731 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m17:09:38.969096 [debug] [ThreadPool]: SQL status: OK in 0.038 seconds
[0m17:09:38.974733 [debug] [ThreadPool]: On list_None_default_gold: ROLLBACK
[0m17:09:38.977235 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m17:09:38.979460 [debug] [ThreadPool]: On list_None_default_gold: Close
[0m17:09:38.984181 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default_gold, now list_None_default_default_gold)
[0m17:09:38.987590 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:09:38.989641 [debug] [ThreadPool]: Using spark connection "list_None_default_default_gold"
[0m17:09:38.991337 [debug] [ThreadPool]: On list_None_default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show table extended in default_default_gold like '*'
  
[0m17:09:38.993268 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:09:39.022873 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#1073, tableName#1074, isTemporary#1075, information#1076]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7b4c28fc, [default_default_gold]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#1073, tableName#1074, isTemporary#1075, information#1076]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7b4c28fc, [default_default_gold]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m17:09:39.025911 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m17:09:39.028171 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show table extended in default_default_gold like '*'
  
[0m17:09:39.030485 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#1073, tableName#1074, isTemporary#1075, information#1076]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7b4c28fc, [default_default_gold]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#1073, tableName#1074, isTemporary#1075, information#1076]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7b4c28fc, [default_default_gold]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m17:09:39.032858 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m17:09:39.035222 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#1073, tableName#1074, isTemporary#1075, information#1076]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7b4c28fc, [default_default_gold]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#1073, tableName#1074, isTemporary#1075, information#1076]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7b4c28fc, [default_default_gold]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m17:09:39.038772 [debug] [ThreadPool]: Using spark connection "list_None_default_default_gold"
[0m17:09:39.040546 [debug] [ThreadPool]: On list_None_default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show tables in default_default_gold like '*'
  
[0m17:09:39.060162 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m17:09:39.062103 [debug] [ThreadPool]: SQL status: OK in 0.020 seconds
[0m17:09:39.068182 [debug] [ThreadPool]: Using spark connection "list_None_default_default_gold"
[0m17:09:39.070480 [debug] [ThreadPool]: On list_None_default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
describe extended default_default_gold.fct_events_enriched
  
[0m17:09:39.108838 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m17:09:39.111649 [debug] [ThreadPool]: SQL status: OK in 0.039 seconds
[0m17:09:39.117939 [debug] [ThreadPool]: On list_None_default_default_gold: ROLLBACK
[0m17:09:39.120131 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m17:09:39.121911 [debug] [ThreadPool]: On list_None_default_default_gold: Close
[0m17:09:39.127112 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6adaba87-4955-46e5-bfb7-1478795b3324', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4b1b0df710>]}
[0m17:09:39.129166 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:09:39.130731 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:09:39.134749 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.dim_uemoa_indicators
[0m17:09:39.137337 [info ] [Thread-1 (]: 1 of 9 START sql table model default_silver.dim_uemoa_indicators ............... [RUN]
[0m17:09:39.139495 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default_default_gold, now model.data_pipeline_poc.dim_uemoa_indicators)
[0m17:09:39.141343 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_poc.dim_uemoa_indicators
[0m17:09:39.150023 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_poc.dim_uemoa_indicators"
[0m17:09:39.172458 [debug] [Thread-1 (]: Began executing node model.data_pipeline_poc.dim_uemoa_indicators
[0m17:09:39.206913 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_poc.dim_uemoa_indicators"
[0m17:09:39.235774 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m17:09:39.238117 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.dim_uemoa_indicators"
[0m17:09:39.240170 [debug] [Thread-1 (]: On model.data_pipeline_poc.dim_uemoa_indicators: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.dim_uemoa_indicators"} */

  
    
        create or replace table default_silver.dim_uemoa_indicators
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Ce modèle sert de "source de vérité" propre pour tous les modèles Gold.
-- Il sélectionne depuis la source et applique les premières transformations/nettoyages.
SELECT
    date,
    pib_nominal_milliards_fcfa,
    poids_secteur_primaire_pct,
    poids_secteur_secondaire_pct,
    poids_secteur_tertiaire_pct,
    taux_croissance_reel_pib_pct,
    taux_inflation_moyen_annuel_ipc_pct,
    recettes_fiscales,
    recettes_fiscales_pct_pib,
    depenses_totales_et_prets_nets,
    solde_budgetaire_global_avec_dons,
    solde_budgetaire_global_hors_dons,
    encours_de_la_dette,
    encours_de_la_dette_pct_pib,
    exportations_biens_fob,
    importations_biens_fob,
    balance_des_biens,
    compte_transactions_courantes,
    balance_courante_sur_pib_pct,
    agregats_monnaie_masse_monetaire_m2,
    taux_couverture_emission_monetaire
    -- Ajoutez toutes les autres colonnes dont vous avez besoin
FROM
    bronze.indicateurs_economiques_uemoa
WHERE
    pib_nominal_milliards_fcfa IS NOT NULL
  
[0m17:09:39.241999 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m17:09:40.073319 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m17:09:40.075358 [debug] [Thread-1 (]: SQL status: OK in 0.833 seconds
[0m17:09:40.091420 [debug] [Thread-1 (]: On model.data_pipeline_poc.dim_uemoa_indicators: ROLLBACK
[0m17:09:40.093941 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m17:09:40.096232 [debug] [Thread-1 (]: On model.data_pipeline_poc.dim_uemoa_indicators: Close
[0m17:09:40.102755 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6adaba87-4955-46e5-bfb7-1478795b3324', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4b1bbd84d0>]}
[0m17:09:40.103983 [info ] [Thread-1 (]: 1 of 9 OK created sql table model default_silver.dim_uemoa_indicators .......... [[32mOK[0m in 0.96s]
[0m17:09:40.105970 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.dim_uemoa_indicators
[0m17:09:40.107660 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.stg_events
[0m17:09:40.110378 [info ] [Thread-1 (]: 2 of 9 START sql table model default_default_silver.stg_events ................. [RUN]
[0m17:09:40.112406 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline_poc.dim_uemoa_indicators, now model.data_pipeline_poc.stg_events)
[0m17:09:40.114329 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_poc.stg_events
[0m17:09:40.118562 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_poc.stg_events"
[0m17:09:40.140098 [debug] [Thread-1 (]: Began executing node model.data_pipeline_poc.stg_events
[0m17:09:40.144314 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_poc.stg_events"
[0m17:09:40.167302 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m17:09:40.169384 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.stg_events"
[0m17:09:40.171784 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_events: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.stg_events"} */

  
    
        create or replace table default_default_silver.stg_events
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Staging model for raw events from bronze layer
-- This model cleans and standardizes the raw event data

SELECT
    event_id,
    event_type,
    user_id,
    event_timestamp,
    event_data,
    current_timestamp() as dbt_loaded_at
FROM bronze.raw_events
WHERE event_id IS NOT NULL
  
[0m17:09:40.174652 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m17:09:41.079227 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m17:09:41.081155 [debug] [Thread-1 (]: SQL status: OK in 0.907 seconds
[0m17:09:41.083992 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_events: ROLLBACK
[0m17:09:41.085445 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m17:09:41.087016 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_events: Close
[0m17:09:41.091195 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6adaba87-4955-46e5-bfb7-1478795b3324', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4b1b4d9c90>]}
[0m17:09:41.096039 [info ] [Thread-1 (]: 2 of 9 OK created sql table model default_default_silver.stg_events ............ [[32mOK[0m in 0.98s]
[0m17:09:41.097765 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.stg_events
[0m17:09:41.099194 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.stg_users
[0m17:09:41.100805 [info ] [Thread-1 (]: 3 of 9 START sql table model default_default_silver.stg_users .................. [RUN]
[0m17:09:41.102498 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline_poc.stg_events, now model.data_pipeline_poc.stg_users)
[0m17:09:41.103857 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_poc.stg_users
[0m17:09:41.107575 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_poc.stg_users"
[0m17:09:41.119887 [debug] [Thread-1 (]: Began executing node model.data_pipeline_poc.stg_users
[0m17:09:41.124539 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_poc.stg_users"
[0m17:09:41.140319 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m17:09:41.143920 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.stg_users"
[0m17:09:41.145465 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_users: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.stg_users"} */

  
    
        create or replace table default_default_silver.stg_users
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Staging model for raw users from bronze layer
-- This model cleans and standardizes the raw user data

SELECT
    user_id,
    user_name,
    email as user_email,
    created_at,
    current_timestamp() as dbt_loaded_at
FROM bronze.raw_users
WHERE user_id IS NOT NULL
  
[0m17:09:41.147165 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m17:09:41.933241 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m17:09:41.935488 [debug] [Thread-1 (]: SQL status: OK in 0.788 seconds
[0m17:09:41.940682 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_users: ROLLBACK
[0m17:09:41.942607 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m17:09:41.944652 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_users: Close
[0m17:09:41.950464 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6adaba87-4955-46e5-bfb7-1478795b3324', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4b1aefa950>]}
[0m17:09:41.952743 [info ] [Thread-1 (]: 3 of 9 OK created sql table model default_default_silver.stg_users ............. [[32mOK[0m in 0.85s]
[0m17:09:41.955073 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.stg_users
[0m17:09:41.956891 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.gold_kpi_uemoa_growth_yoy
[0m17:09:41.958900 [info ] [Thread-1 (]: 4 of 9 START sql table model default_gold.gold_kpi_uemoa_growth_yoy ............ [RUN]
[0m17:09:41.960210 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline_poc.stg_users, now model.data_pipeline_poc.gold_kpi_uemoa_growth_yoy)
[0m17:09:41.962263 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_poc.gold_kpi_uemoa_growth_yoy
[0m17:09:41.967214 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_poc.gold_kpi_uemoa_growth_yoy"
[0m17:09:41.987168 [debug] [Thread-1 (]: Began executing node model.data_pipeline_poc.gold_kpi_uemoa_growth_yoy
[0m17:09:41.991367 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_poc.gold_kpi_uemoa_growth_yoy"
[0m17:09:42.005501 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m17:09:42.006984 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.gold_kpi_uemoa_growth_yoy"
[0m17:09:42.008075 [debug] [Thread-1 (]: On model.data_pipeline_poc.gold_kpi_uemoa_growth_yoy: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.gold_kpi_uemoa_growth_yoy"} */

  
    
        create or replace table default_gold.gold_kpi_uemoa_growth_yoy
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Modèle Gold : Calcule la croissance Année-sur-Année (YoY) des KPIs
-- Idéal pour les graphiques de tendance et l'analyse de la performance.
SELECT
    date,
    pib_nominal_milliards_fcfa,
    taux_croissance_reel_pib_pct,
    taux_inflation_moyen_annuel_ipc_pct,
    
    -- Calcul de la croissance YoY du PIB nominal
    LAG(pib_nominal_milliards_fcfa, 1) OVER (ORDER BY date) as pib_nominal_annee_precedente,
    (pib_nominal_milliards_fcfa - LAG(pib_nominal_milliards_fcfa, 1) OVER (ORDER BY date)) 
        / LAG(pib_nominal_milliards_fcfa, 1) OVER (ORDER BY date) * 100 as pib_nominal_croissance_yoy_pct,

    -- Calcul de la croissance YoY des recettes fiscales
    recettes_fiscales,
    (recettes_fiscales - LAG(recettes_fiscales, 1) OVER (ORDER BY date))
        / LAG(recettes_fiscales, 1) OVER (ORDER BY date) * 100 as recettes_fiscales_croissance_yoy_pct,

    -- Calcul de la croissance YoY de la masse monétaire M2
    agregats_monnaie_masse_monetaire_m2,
    (agregats_monnaie_masse_monetaire_m2 - LAG(agregats_monnaie_masse_monetaire_m2, 1) OVER (ORDER BY date))
        / LAG(agregats_monnaie_masse_monetaire_m2, 1) OVER (ORDER BY date) * 100 as m2_croissance_yoy_pct

FROM
    default_silver.dim_uemoa_indicators
ORDER BY
    date DESC
  
[0m17:09:42.009106 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m17:09:42.804104 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m17:09:42.806552 [debug] [Thread-1 (]: SQL status: OK in 0.797 seconds
[0m17:09:42.811084 [debug] [Thread-1 (]: On model.data_pipeline_poc.gold_kpi_uemoa_growth_yoy: ROLLBACK
[0m17:09:42.813325 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m17:09:42.816014 [debug] [Thread-1 (]: On model.data_pipeline_poc.gold_kpi_uemoa_growth_yoy: Close
[0m17:09:42.821784 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6adaba87-4955-46e5-bfb7-1478795b3324', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4b1b550c90>]}
[0m17:09:42.824449 [info ] [Thread-1 (]: 4 of 9 OK created sql table model default_gold.gold_kpi_uemoa_growth_yoy ....... [[32mOK[0m in 0.86s]
[0m17:09:42.826648 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.gold_kpi_uemoa_growth_yoy
[0m17:09:42.828770 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.gold_mart_uemoa_external_stability
[0m17:09:42.830783 [info ] [Thread-1 (]: 5 of 9 START sql table model default_gold.gold_mart_uemoa_external_stability ... [RUN]
[0m17:09:42.832884 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline_poc.gold_kpi_uemoa_growth_yoy, now model.data_pipeline_poc.gold_mart_uemoa_external_stability)
[0m17:09:42.835127 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_poc.gold_mart_uemoa_external_stability
[0m17:09:42.840499 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_poc.gold_mart_uemoa_external_stability"
[0m17:09:42.867166 [debug] [Thread-1 (]: Began executing node model.data_pipeline_poc.gold_mart_uemoa_external_stability
[0m17:09:42.874807 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.gold_mart_uemoa_external_stability"
[0m17:09:42.877381 [debug] [Thread-1 (]: On model.data_pipeline_poc.gold_mart_uemoa_external_stability: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.gold_mart_uemoa_external_stability"} */
drop table if exists default_gold.gold_mart_uemoa_external_stability
[0m17:09:42.879627 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m17:09:42.917089 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m17:09:42.919287 [debug] [Thread-1 (]: SQL status: OK in 0.040 seconds
[0m17:09:42.922417 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_poc.gold_mart_uemoa_external_stability"
[0m17:09:42.947030 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m17:09:42.950071 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.gold_mart_uemoa_external_stability"
[0m17:09:42.952595 [debug] [Thread-1 (]: On model.data_pipeline_poc.gold_mart_uemoa_external_stability: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.gold_mart_uemoa_external_stability"} */

  
    
        create or replace table default_gold.gold_mart_uemoa_external_stability
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Modèle Gold : Indicateurs de soutenabilité externe.
-- Calcule les indicateurs clés de la stabilité externe de l'UEMOA.

SELECT
    date,
    
    -- Balance commerciale et compte courant
    exportations_biens_fob,
    importations_biens_fob,
    balance_des_biens,
    compte_transactions_courantes,
    balance_courante_sur_pib_pct,
    
    -- Importations mensuelles moyennes
    (importations_biens_fob / 12) as importations_mensuelles_moyennes,
    
    -- Taux de couverture des importations par les exportations
    CASE 
        WHEN importations_biens_fob != 0 
        THEN (exportations_biens_fob / importations_biens_fob) * 100
        ELSE NULL
    END as taux_couverture_importations_pct,
    
    -- Degré d'ouverture commerciale
    -- (Exportations + Importations) / PIB * 100
    CASE 
        WHEN pib_nominal_milliards_fcfa != 0 
        THEN ((exportations_biens_fob + importations_biens_fob) / pib_nominal_milliards_fcfa) * 100
        ELSE NULL
    END as degre_ouverture_commerciale_pct,

    -- Taux de couverture de l'émission monétaire (indicateur de réserves)
    taux_couverture_emission_monetaire,
    
    -- Indicateur de soutenabilité de la dette externe
    encours_de_la_dette,
    encours_de_la_dette_pct_pib

FROM
    default_silver.dim_uemoa_indicators
WHERE
    importations_biens_fob IS NOT NULL AND importations_biens_fob != 0
ORDER BY
    date DESC
  
[0m17:09:43.886025 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m17:09:43.888511 [debug] [Thread-1 (]: SQL status: OK in 0.934 seconds
[0m17:09:43.891830 [debug] [Thread-1 (]: On model.data_pipeline_poc.gold_mart_uemoa_external_stability: ROLLBACK
[0m17:09:43.893845 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m17:09:43.895853 [debug] [Thread-1 (]: On model.data_pipeline_poc.gold_mart_uemoa_external_stability: Close
[0m17:09:43.902552 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6adaba87-4955-46e5-bfb7-1478795b3324', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4b1b099c10>]}
[0m17:09:43.905077 [info ] [Thread-1 (]: 5 of 9 OK created sql table model default_gold.gold_mart_uemoa_external_stability  [[32mOK[0m in 1.07s]
[0m17:09:43.907324 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.gold_mart_uemoa_external_stability
[0m17:09:43.911254 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.gold_mart_uemoa_external_trade
[0m17:09:43.913419 [info ] [Thread-1 (]: 6 of 9 START sql table model default_gold.gold_mart_uemoa_external_trade ....... [RUN]
[0m17:09:43.916117 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline_poc.gold_mart_uemoa_external_stability, now model.data_pipeline_poc.gold_mart_uemoa_external_trade)
[0m17:09:43.918025 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_poc.gold_mart_uemoa_external_trade
[0m17:09:43.922083 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_poc.gold_mart_uemoa_external_trade"
[0m17:09:43.945740 [debug] [Thread-1 (]: Began executing node model.data_pipeline_poc.gold_mart_uemoa_external_trade
[0m17:09:43.951038 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_poc.gold_mart_uemoa_external_trade"
[0m17:09:43.975639 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m17:09:43.977909 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.gold_mart_uemoa_external_trade"
[0m17:09:43.980106 [debug] [Thread-1 (]: On model.data_pipeline_poc.gold_mart_uemoa_external_trade: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.gold_mart_uemoa_external_trade"} */

  
    
        create or replace table default_gold.gold_mart_uemoa_external_trade
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Modèle Gold : Data mart focalisé sur le commerce extérieur et la balance des paiements.
-- Prêt à être consommé par un outil de BI pour la BCEAO ou le Commerce.
SELECT
    date,
    pib_nominal_milliards_fcfa,
    
    -- Balance commerciale
    exportations_biens_fob,
    importations_biens_fob,
    balance_des_biens,
    (balance_des_biens / pib_nominal_milliards_fcfa) * 100 as balance_des_biens_pct_pib,

    -- Balance courante
    compte_transactions_courantes,
    balance_courante_sur_pib_pct

FROM
    default_silver.dim_uemoa_indicators
ORDER BY
    date DESC
  
[0m17:09:43.982729 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m17:09:44.822153 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m17:09:44.824430 [debug] [Thread-1 (]: SQL status: OK in 0.842 seconds
[0m17:09:44.827773 [debug] [Thread-1 (]: On model.data_pipeline_poc.gold_mart_uemoa_external_trade: ROLLBACK
[0m17:09:44.830283 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m17:09:44.833074 [debug] [Thread-1 (]: On model.data_pipeline_poc.gold_mart_uemoa_external_trade: Close
[0m17:09:44.837696 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6adaba87-4955-46e5-bfb7-1478795b3324', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4b1b546750>]}
[0m17:09:44.840056 [info ] [Thread-1 (]: 6 of 9 OK created sql table model default_gold.gold_mart_uemoa_external_trade .. [[32mOK[0m in 0.92s]
[0m17:09:44.842870 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.gold_mart_uemoa_external_trade
[0m17:09:44.844868 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.gold_mart_uemoa_monetary_dashboard
[0m17:09:44.846753 [info ] [Thread-1 (]: 7 of 9 START sql table model default_gold.gold_mart_uemoa_monetary_dashboard ... [RUN]
[0m17:09:44.849834 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline_poc.gold_mart_uemoa_external_trade, now model.data_pipeline_poc.gold_mart_uemoa_monetary_dashboard)
[0m17:09:44.851858 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_poc.gold_mart_uemoa_monetary_dashboard
[0m17:09:44.855977 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_poc.gold_mart_uemoa_monetary_dashboard"
[0m17:09:44.875030 [debug] [Thread-1 (]: Began executing node model.data_pipeline_poc.gold_mart_uemoa_monetary_dashboard
[0m17:09:44.880646 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_poc.gold_mart_uemoa_monetary_dashboard"
[0m17:09:44.892085 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m17:09:44.893320 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.gold_mart_uemoa_monetary_dashboard"
[0m17:09:44.894429 [debug] [Thread-1 (]: On model.data_pipeline_poc.gold_mart_uemoa_monetary_dashboard: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.gold_mart_uemoa_monetary_dashboard"} */

  
    
        create or replace table default_gold.gold_mart_uemoa_monetary_dashboard
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Modèle Gold : Prêt pour un tableau de bord de pilotage monétaire.
-- Calcule la croissance des agrégats et les ratios clés.

SELECT
    date,
    pib_nominal_milliards_fcfa,
    agregats_monnaie_masse_monetaire_m2,
    taux_inflation_moyen_annuel_ipc_pct,
    
    -- Croissance Année-sur-Année (YoY) de la masse monétaire
    (agregats_monnaie_masse_monetaire_m2 - LAG(agregats_monnaie_masse_monetaire_m2, 1) OVER (ORDER BY date))
        / LAG(agregats_monnaie_masse_monetaire_m2, 1) OVER (ORDER BY date) * 100 as m2_croissance_yoy_pct,

    -- Vélocité de la monnaie (V) : vitesse à laquelle l'argent circule
    -- V = PIB Nominal / M2
    pib_nominal_milliards_fcfa / agregats_monnaie_masse_monetaire_m2 as velocite_monnaie,

    -- Taux de couverture de l'émission (déjà présent, mais vital)
    taux_couverture_emission_monetaire

FROM
    default_silver.dim_uemoa_indicators
-- NOTE: Assurez-vous que votre modèle Silver 'dim_uemoa_indicators'
-- inclut bien 'agregats_monnaie_masse_monetaire_m2' et 'taux_couverture_emission_monetaire'
WHERE
    agregats_monnaie_masse_monetaire_m2 IS NOT NULL
ORDER BY
    date DESC
  
[0m17:09:44.895460 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m17:09:45.595087 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m17:09:45.597097 [debug] [Thread-1 (]: SQL status: OK in 0.702 seconds
[0m17:09:45.600823 [debug] [Thread-1 (]: On model.data_pipeline_poc.gold_mart_uemoa_monetary_dashboard: ROLLBACK
[0m17:09:45.602459 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m17:09:45.604583 [debug] [Thread-1 (]: On model.data_pipeline_poc.gold_mart_uemoa_monetary_dashboard: Close
[0m17:09:45.609014 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6adaba87-4955-46e5-bfb7-1478795b3324', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4b13f7ca50>]}
[0m17:09:45.612049 [info ] [Thread-1 (]: 7 of 9 OK created sql table model default_gold.gold_mart_uemoa_monetary_dashboard  [[32mOK[0m in 0.76s]
[0m17:09:45.614308 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.gold_mart_uemoa_monetary_dashboard
[0m17:09:45.616674 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.gold_mart_uemoa_public_finance
[0m17:09:45.618928 [info ] [Thread-1 (]: 8 of 9 START sql table model default_gold.gold_mart_uemoa_public_finance ....... [RUN]
[0m17:09:45.620886 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline_poc.gold_mart_uemoa_monetary_dashboard, now model.data_pipeline_poc.gold_mart_uemoa_public_finance)
[0m17:09:45.622574 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_poc.gold_mart_uemoa_public_finance
[0m17:09:45.627249 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_poc.gold_mart_uemoa_public_finance"
[0m17:09:45.648785 [debug] [Thread-1 (]: Began executing node model.data_pipeline_poc.gold_mart_uemoa_public_finance
[0m17:09:45.652829 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_poc.gold_mart_uemoa_public_finance"
[0m17:09:45.667273 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m17:09:45.668794 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.gold_mart_uemoa_public_finance"
[0m17:09:45.670343 [debug] [Thread-1 (]: On model.data_pipeline_poc.gold_mart_uemoa_public_finance: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.gold_mart_uemoa_public_finance"} */

  
    
        create or replace table default_gold.gold_mart_uemoa_public_finance
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Modèle Gold : Data mart focalisé sur les finances publiques.
-- Prêt à être consommé par un outil de BI pour le Ministère des Finances.
SELECT
    date,
    pib_nominal_milliards_fcfa,
    
    -- Section Recettes
    recettes_fiscales,
    recettes_fiscales_pct_pib,
    
    -- Section Dépenses
    depenses_totales_et_prets_nets,
    
    -- Section Soldes
    solde_budgetaire_global_avec_dons,
    solde_budgetaire_global_hors_dons,
    -- On recalcule le solde en % du PIB pour être sûr
    (solde_budgetaire_global_avec_dons / pib_nominal_milliards_fcfa) * 100 as solde_budgetaire_avec_dons_pct_pib,
    
    -- Section Dette
    encours_de_la_dette,
    encours_de_la_dette_pct_pib

FROM
    default_silver.dim_uemoa_indicators
WHERE
    -- On peut se concentrer sur une période plus récente pour un dashboard
    EXTRACT(YEAR FROM date) >= 2010
ORDER BY
    date DESC
  
[0m17:09:45.672094 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m17:09:46.531606 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m17:09:46.533822 [debug] [Thread-1 (]: SQL status: OK in 0.862 seconds
[0m17:09:46.537103 [debug] [Thread-1 (]: On model.data_pipeline_poc.gold_mart_uemoa_public_finance: ROLLBACK
[0m17:09:46.539086 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m17:09:46.540929 [debug] [Thread-1 (]: On model.data_pipeline_poc.gold_mart_uemoa_public_finance: Close
[0m17:09:46.545387 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6adaba87-4955-46e5-bfb7-1478795b3324', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4b13f7e650>]}
[0m17:09:46.547782 [info ] [Thread-1 (]: 8 of 9 OK created sql table model default_gold.gold_mart_uemoa_public_finance .. [[32mOK[0m in 0.92s]
[0m17:09:46.549733 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.gold_mart_uemoa_public_finance
[0m17:09:46.551662 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.fct_events_enriched
[0m17:09:46.553506 [info ] [Thread-1 (]: 9 of 9 START sql table model default_default_gold.fct_events_enriched .......... [RUN]
[0m17:09:46.555376 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline_poc.gold_mart_uemoa_public_finance, now model.data_pipeline_poc.fct_events_enriched)
[0m17:09:46.557054 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_poc.fct_events_enriched
[0m17:09:46.561301 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_poc.fct_events_enriched"
[0m17:09:46.583485 [debug] [Thread-1 (]: Began executing node model.data_pipeline_poc.fct_events_enriched
[0m17:09:46.588042 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_poc.fct_events_enriched"
[0m17:09:46.610672 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m17:09:46.612951 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.fct_events_enriched"
[0m17:09:46.615266 [debug] [Thread-1 (]: On model.data_pipeline_poc.fct_events_enriched: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.fct_events_enriched"} */

  
    
        create or replace table default_default_gold.fct_events_enriched
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Mart model: Events enriched with user information
-- This model joins event data with user data for analytics

WITH events AS (
    SELECT * FROM default_default_silver.stg_events
),

users AS (
    SELECT * FROM default_default_silver.stg_users
),

enriched AS (
    SELECT
        e.event_id,
        e.event_type,
        e.event_timestamp,
        e.event_data,
        e.user_id,
        u.user_name,
        u.user_email,
        u.created_at as user_created_at,
        e.dbt_loaded_at
    FROM events e
    LEFT JOIN users u ON e.user_id = u.user_id
)

SELECT * FROM enriched
  
[0m17:09:46.617387 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m17:09:47.634298 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m17:09:47.636748 [debug] [Thread-1 (]: SQL status: OK in 1.019 seconds
[0m17:09:47.640324 [debug] [Thread-1 (]: On model.data_pipeline_poc.fct_events_enriched: ROLLBACK
[0m17:09:47.642432 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m17:09:47.645002 [debug] [Thread-1 (]: On model.data_pipeline_poc.fct_events_enriched: Close
[0m17:09:47.650471 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6adaba87-4955-46e5-bfb7-1478795b3324', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4b13f7c1d0>]}
[0m17:09:47.652819 [info ] [Thread-1 (]: 9 of 9 OK created sql table model default_default_gold.fct_events_enriched ..... [[32mOK[0m in 1.10s]
[0m17:09:47.655429 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.fct_events_enriched
[0m17:09:47.658798 [debug] [MainThread]: On master: ROLLBACK
[0m17:09:47.661331 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:09:47.686798 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m17:09:47.689254 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:09:47.691311 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:09:47.693290 [debug] [MainThread]: On master: ROLLBACK
[0m17:09:47.695236 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m17:09:47.697276 [debug] [MainThread]: On master: Close
[0m17:09:47.701477 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:09:47.703637 [debug] [MainThread]: Connection 'model.data_pipeline_poc.fct_events_enriched' was properly closed.
[0m17:09:47.705465 [info ] [MainThread]: 
[0m17:09:47.707631 [info ] [MainThread]: Finished running 9 table models in 0 hours 0 minutes and 9.69 seconds (9.69s).
[0m17:09:47.710499 [debug] [MainThread]: Command end result
[0m17:09:47.781772 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/app/dbt/target/manifest.json
[0m17:09:47.792059 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/app/dbt/target/semantic_manifest.json
[0m17:09:47.807137 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/app/dbt/target/run_results.json
[0m17:09:47.809189 [info ] [MainThread]: 
[0m17:09:47.811045 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:09:47.812410 [info ] [MainThread]: 
[0m17:09:47.813864 [info ] [MainThread]: Done. PASS=9 WARN=0 ERROR=0 SKIP=0 TOTAL=9
[0m17:09:47.816225 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 13.995847, "process_in_blocks": "0", "process_kernel_time": 0.230799, "process_mem_max_rss": "109104", "process_out_blocks": "0", "process_user_time": 1.743363}
[0m17:09:47.818523 [debug] [MainThread]: Command `dbt run` succeeded at 17:09:47.818359 after 14.00 seconds
[0m17:09:47.820252 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4b1c052b10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4b1f4e8ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4b1f4e9790>]}
[0m17:09:47.822037 [debug] [MainThread]: Flushing usage events
[0m17:09:48.983189 [debug] [MainThread]: An error was encountered while trying to flush usage events
