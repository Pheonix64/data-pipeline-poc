[0m12:37:24.187764 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7af073f58250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7af073f5aa90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7af073f5a350>]}


============================== 12:37:24.190839 | 89d1e40e-684c-4a21-833d-1191c5840000 ==============================
[0m12:37:24.190839 [info ] [MainThread]: Running with dbt=1.9.0
[0m12:37:24.192549 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/app/dbt', 'version_check': 'True', 'debug': 'False', 'log_path': '/usr/app/dbt/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt debug', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m12:37:24.208528 [info ] [MainThread]: dbt version: 1.9.0
[0m12:37:24.209433 [info ] [MainThread]: python version: 3.11.2
[0m12:37:24.212174 [info ] [MainThread]: python path: /usr/local/bin/python
[0m12:37:24.212973 [info ] [MainThread]: os info: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.31
[0m12:37:24.258015 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:37:24.259263 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:37:24.260216 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:37:24.282162 [info ] [MainThread]: Using profiles dir at /usr/app/dbt
[0m12:37:24.283526 [info ] [MainThread]: Using profiles.yml file at /usr/app/dbt/profiles.yml
[0m12:37:24.284492 [info ] [MainThread]: Using dbt_project.yml file at /usr/app/dbt/dbt_project.yml
[0m12:37:24.285253 [info ] [MainThread]: adapter type: spark
[0m12:37:24.286227 [info ] [MainThread]: adapter version: 1.9.0
[0m12:37:24.356987 [info ] [MainThread]: Configuration:
[0m12:37:24.358466 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m12:37:24.359522 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m12:37:24.360352 [info ] [MainThread]: Required dependencies:
[0m12:37:24.361230 [debug] [MainThread]: Executing "git --help"
[0m12:37:24.372311 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone             Clone a repository into a new directory\n   init              Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add               Add file contents to the index\n   mv                Move or rename a file, a directory, or a symlink\n   restore           Restore working tree files\n   rm                Remove files from the working tree and from the index\n   sparse-checkout   Initialize and modify the sparse-checkout\n\nexamine the history and state (see also: git help revisions)\n   bisect            Use binary search to find the commit that introduced a bug\n   diff              Show changes between commits, commit and working tree, etc\n   grep              Print lines matching a pattern\n   log               Show commit logs\n   show              Show various types of objects\n   status            Show the working tree status\n\ngrow, mark and tweak your common history\n   branch            List, create, or delete branches\n   commit            Record changes to the repository\n   merge             Join two or more development histories together\n   rebase            Reapply commits on top of another base tip\n   reset             Reset current HEAD to the specified state\n   switch            Switch branches\n   tag               Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch             Download objects and refs from another repository\n   pull              Fetch from and integrate with another repository or a local branch\n   push              Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m12:37:24.373367 [debug] [MainThread]: STDERR: "b''"
[0m12:37:24.374299 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m12:37:24.375407 [info ] [MainThread]: Connection:
[0m12:37:24.376321 [info ] [MainThread]:   host: spark-iceberg
[0m12:37:24.377009 [info ] [MainThread]:   port: 10000
[0m12:37:24.377785 [info ] [MainThread]:   cluster: None
[0m12:37:24.378509 [info ] [MainThread]:   endpoint: None
[0m12:37:24.379205 [info ] [MainThread]:   schema: default
[0m12:37:24.379915 [info ] [MainThread]:   organization: 0
[0m12:37:24.380805 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m12:37:24.419715 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m12:37:24.420485 [debug] [MainThread]: Using spark connection "debug"
[0m12:37:24.421126 [debug] [MainThread]: On debug: select 1 as id
[0m12:37:24.421686 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:37:26.097646 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m12:37:26.098928 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m12:37:26.099977 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m12:37:26.101149 [info ] [MainThread]: [31m1 check failed:[0m
[0m12:37:26.102512 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m12:37:26.105211 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 1.9690187, "process_in_blocks": "1456", "process_kernel_time": 0.140245, "process_mem_max_rss": "98960", "process_out_blocks": "24", "process_user_time": 0.96569}
[0m12:37:26.106703 [debug] [MainThread]: Command `dbt debug` failed at 12:37:26.106475 after 1.97 seconds
[0m12:37:26.107994 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m12:37:26.108962 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7af073f5a490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7af0778d92d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7af07783a5d0>]}
[0m12:37:26.109792 [debug] [MainThread]: Flushing usage events
[0m12:37:30.257991 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:37:31.465790 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ca993ee8c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ca993eeb450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ca993eebcd0>]}


============================== 12:37:31.468449 | 2ecad8cb-bebd-4c21-86ca-bff8d596d20c ==============================
[0m12:37:31.468449 [info ] [MainThread]: Running with dbt=1.9.0
[0m12:37:31.469435 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/usr/app/dbt/logs', 'profiles_dir': '/usr/app/dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt deps', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m12:37:31.548881 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2ecad8cb-bebd-4c21-86ca-bff8d596d20c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ca993d89250>]}
[0m12:37:31.563870 [debug] [MainThread]: Set downloads directory='/tmp/dbt-downloads-_8szoh32'
[0m12:37:31.564762 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m12:37:41.508428 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m12:37:41.510184 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m12:37:41.799668 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m12:37:41.805782 [error] [MainThread]: Encountered an error:
Package dbt-labs/dbt_expectations was not found in the package index
[0m12:37:41.808640 [error] [MainThread]: Traceback (most recent call last):
  File "/usr/local/lib/python3.11/site-packages/dbt/cli/requires.py", line 153, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/dbt/cli/requires.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/dbt/cli/requires.py", line 218, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/dbt/cli/requires.py", line 264, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/dbt/cli/main.py", line 455, in deps
    results = task.run()
              ^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/dbt/task/deps.py", line 211, in run
    self.lock()
  File "/usr/local/lib/python3.11/site-packages/dbt/task/deps.py", line 187, in lock
    resolved_deps = resolve_packages(packages, self.project, self.cli_vars)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/dbt/deps/resolver.py", line 131, in resolve_packages
    target = final[package].resolved().fetch_metadata(project, renderer)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/dbt/deps/registry.py", line 98, in resolved
    self._check_in_index()
  File "/usr/local/lib/python3.11/site-packages/dbt/deps/registry.py", line 77, in _check_in_index
    raise PackageNotFoundError(self.package)
dbt.exceptions.PackageNotFoundError: Package dbt-labs/dbt_expectations was not found in the package index

[0m12:37:41.809998 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": false, "command_wall_clock_time": 10.388398, "process_in_blocks": "712", "process_kernel_time": 0.122414, "process_mem_max_rss": "90104", "process_out_blocks": "8", "process_user_time": 0.817412}
[0m12:37:41.810802 [debug] [MainThread]: Command `dbt deps` failed at 12:37:41.810720 after 10.39 seconds
[0m12:37:41.812013 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ca993eeaa90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ca993e1e9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ca997869650>]}
[0m12:37:41.812688 [debug] [MainThread]: Flushing usage events
[0m12:37:42.419290 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:41:26.167881 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76522443e250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x765224493cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76522443fd10>]}


============================== 12:41:26.170686 | 6a31db74-14dc-4d82-af47-2fca6f6ec31a ==============================
[0m12:41:26.170686 [info ] [MainThread]: Running with dbt=1.9.0
[0m12:41:26.172466 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/app/dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/usr/app/dbt/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt debug', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m12:41:26.179810 [info ] [MainThread]: dbt version: 1.9.0
[0m12:41:26.180787 [info ] [MainThread]: python version: 3.11.2
[0m12:41:26.181668 [info ] [MainThread]: python path: /usr/local/bin/python
[0m12:41:26.182599 [info ] [MainThread]: os info: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.31
[0m12:41:26.229305 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:41:26.230772 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:41:26.231771 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:41:26.255140 [info ] [MainThread]: Using profiles dir at /usr/app/dbt
[0m12:41:26.256577 [info ] [MainThread]: Using profiles.yml file at /usr/app/dbt/profiles.yml
[0m12:41:26.257806 [info ] [MainThread]: Using dbt_project.yml file at /usr/app/dbt/dbt_project.yml
[0m12:41:26.258585 [info ] [MainThread]: adapter type: spark
[0m12:41:26.259293 [info ] [MainThread]: adapter version: 1.9.0
[0m12:41:26.326283 [info ] [MainThread]: Configuration:
[0m12:41:26.327568 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m12:41:26.328609 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m12:41:26.329423 [info ] [MainThread]: Required dependencies:
[0m12:41:26.330176 [debug] [MainThread]: Executing "git --help"
[0m12:41:26.332428 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone             Clone a repository into a new directory\n   init              Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add               Add file contents to the index\n   mv                Move or rename a file, a directory, or a symlink\n   restore           Restore working tree files\n   rm                Remove files from the working tree and from the index\n   sparse-checkout   Initialize and modify the sparse-checkout\n\nexamine the history and state (see also: git help revisions)\n   bisect            Use binary search to find the commit that introduced a bug\n   diff              Show changes between commits, commit and working tree, etc\n   grep              Print lines matching a pattern\n   log               Show commit logs\n   show              Show various types of objects\n   status            Show the working tree status\n\ngrow, mark and tweak your common history\n   branch            List, create, or delete branches\n   commit            Record changes to the repository\n   merge             Join two or more development histories together\n   rebase            Reapply commits on top of another base tip\n   reset             Reset current HEAD to the specified state\n   switch            Switch branches\n   tag               Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch             Download objects and refs from another repository\n   pull              Fetch from and integrate with another repository or a local branch\n   push              Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m12:41:26.333491 [debug] [MainThread]: STDERR: "b''"
[0m12:41:26.334166 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m12:41:26.335157 [info ] [MainThread]: Connection:
[0m12:41:26.337163 [info ] [MainThread]:   host: spark-iceberg
[0m12:41:26.338155 [info ] [MainThread]:   port: 10000
[0m12:41:26.339324 [info ] [MainThread]:   cluster: None
[0m12:41:26.340241 [info ] [MainThread]:   endpoint: None
[0m12:41:26.341209 [info ] [MainThread]:   schema: default
[0m12:41:26.342092 [info ] [MainThread]:   organization: 0
[0m12:41:26.343070 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m12:41:26.376613 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m12:41:26.377460 [debug] [MainThread]: Using spark connection "debug"
[0m12:41:26.378196 [debug] [MainThread]: On debug: select 1 as id
[0m12:41:26.378816 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:41:27.543262 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m12:41:27.544799 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m12:41:27.546269 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m12:41:27.547408 [info ] [MainThread]: [31m1 check failed:[0m
[0m12:41:27.548648 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m12:41:27.550466 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 1.429126, "process_in_blocks": "0", "process_kernel_time": 0.12654, "process_mem_max_rss": "98884", "process_out_blocks": "0", "process_user_time": 0.897645}
[0m12:41:27.551761 [debug] [MainThread]: Command `dbt debug` failed at 12:41:27.551631 after 1.43 seconds
[0m12:41:27.552955 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m12:41:27.554386 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x765227dc1410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x765223e7a3d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x765227dc1510>]}
[0m12:41:27.555539 [debug] [MainThread]: Flushing usage events
[0m12:41:30.800270 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:48:22.920566 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x713cbf749c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x713cbf7494d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x713cbf74bf90>]}


============================== 12:48:22.923513 | 9b6bbe8d-a6aa-4544-98dc-c759148f9c71 ==============================
[0m12:48:22.923513 [info ] [MainThread]: Running with dbt=1.9.0
[0m12:48:22.925173 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/usr/app/dbt/logs', 'profiles_dir': '/usr/app/dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt debug', 'send_anonymous_usage_stats': 'True'}
[0m12:48:22.931603 [info ] [MainThread]: dbt version: 1.9.0
[0m12:48:22.932462 [info ] [MainThread]: python version: 3.11.2
[0m12:48:22.933769 [info ] [MainThread]: python path: /usr/local/bin/python
[0m12:48:22.935165 [info ] [MainThread]: os info: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.31
[0m12:48:22.986766 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:48:22.988081 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:48:22.989090 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:48:23.012580 [info ] [MainThread]: Using profiles dir at /usr/app/dbt
[0m12:48:23.014292 [info ] [MainThread]: Using profiles.yml file at /usr/app/dbt/profiles.yml
[0m12:48:23.015418 [info ] [MainThread]: Using dbt_project.yml file at /usr/app/dbt/dbt_project.yml
[0m12:48:23.016478 [info ] [MainThread]: adapter type: spark
[0m12:48:23.017217 [info ] [MainThread]: adapter version: 1.9.0
[0m12:48:23.093335 [info ] [MainThread]: Configuration:
[0m12:48:23.094589 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m12:48:23.095819 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m12:48:23.096618 [info ] [MainThread]: Required dependencies:
[0m12:48:23.097490 [debug] [MainThread]: Executing "git --help"
[0m12:48:23.099659 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone             Clone a repository into a new directory\n   init              Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add               Add file contents to the index\n   mv                Move or rename a file, a directory, or a symlink\n   restore           Restore working tree files\n   rm                Remove files from the working tree and from the index\n   sparse-checkout   Initialize and modify the sparse-checkout\n\nexamine the history and state (see also: git help revisions)\n   bisect            Use binary search to find the commit that introduced a bug\n   diff              Show changes between commits, commit and working tree, etc\n   grep              Print lines matching a pattern\n   log               Show commit logs\n   show              Show various types of objects\n   status            Show the working tree status\n\ngrow, mark and tweak your common history\n   branch            List, create, or delete branches\n   commit            Record changes to the repository\n   merge             Join two or more development histories together\n   rebase            Reapply commits on top of another base tip\n   reset             Reset current HEAD to the specified state\n   switch            Switch branches\n   tag               Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch             Download objects and refs from another repository\n   pull              Fetch from and integrate with another repository or a local branch\n   push              Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m12:48:23.100421 [debug] [MainThread]: STDERR: "b''"
[0m12:48:23.101128 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m12:48:23.101918 [info ] [MainThread]: Connection:
[0m12:48:23.103337 [info ] [MainThread]:   host: spark-iceberg
[0m12:48:23.104376 [info ] [MainThread]:   port: 10000
[0m12:48:23.105268 [info ] [MainThread]:   cluster: None
[0m12:48:23.106110 [info ] [MainThread]:   endpoint: None
[0m12:48:23.107011 [info ] [MainThread]:   schema: default
[0m12:48:23.107904 [info ] [MainThread]:   organization: 0
[0m12:48:23.108926 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m12:48:23.146347 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m12:48:23.147377 [debug] [MainThread]: Using spark connection "debug"
[0m12:48:23.148072 [debug] [MainThread]: On debug: select 1 as id
[0m12:48:23.148700 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:48:23.185249 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m12:48:23.186456 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m12:48:23.187978 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m12:48:23.189132 [info ] [MainThread]: [31m1 check failed:[0m
[0m12:48:23.190096 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m12:48:23.191569 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 0.31958392, "process_in_blocks": "0", "process_kernel_time": 0.119626, "process_mem_max_rss": "99312", "process_out_blocks": "0", "process_user_time": 1.012838}
[0m12:48:23.192503 [debug] [MainThread]: Command `dbt debug` failed at 12:48:23.192397 after 0.32 seconds
[0m12:48:23.193209 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m12:48:23.193890 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x713cbf7a3bd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x713cbf2ace10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x713cbf79fa50>]}
[0m12:48:23.194582 [debug] [MainThread]: Flushing usage events
[0m12:48:25.316890 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:49:40.300261 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x766320745590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76632079b850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76632079bc90>]}


============================== 12:49:40.303013 | a4e81be3-9227-4441-abc2-7487c2f77073 ==============================
[0m12:49:40.303013 [info ] [MainThread]: Running with dbt=1.9.0
[0m12:49:40.304232 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/app/dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': '/usr/app/dbt/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt debug', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m12:49:40.311457 [info ] [MainThread]: dbt version: 1.9.0
[0m12:49:40.312796 [info ] [MainThread]: python version: 3.11.2
[0m12:49:40.313604 [info ] [MainThread]: python path: /usr/local/bin/python
[0m12:49:40.314414 [info ] [MainThread]: os info: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.31
[0m12:49:40.359915 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:49:40.361473 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:49:40.362157 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:49:40.382587 [info ] [MainThread]: Using profiles dir at /usr/app/dbt
[0m12:49:40.384191 [info ] [MainThread]: Using profiles.yml file at /usr/app/dbt/profiles.yml
[0m12:49:40.385071 [info ] [MainThread]: Using dbt_project.yml file at /usr/app/dbt/dbt_project.yml
[0m12:49:40.385796 [info ] [MainThread]: adapter type: spark
[0m12:49:40.386932 [info ] [MainThread]: adapter version: 1.9.0
[0m12:49:40.455889 [info ] [MainThread]: Configuration:
[0m12:49:40.458037 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m12:49:40.459326 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m12:49:40.460457 [info ] [MainThread]: Required dependencies:
[0m12:49:40.461466 [debug] [MainThread]: Executing "git --help"
[0m12:49:40.463414 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone             Clone a repository into a new directory\n   init              Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add               Add file contents to the index\n   mv                Move or rename a file, a directory, or a symlink\n   restore           Restore working tree files\n   rm                Remove files from the working tree and from the index\n   sparse-checkout   Initialize and modify the sparse-checkout\n\nexamine the history and state (see also: git help revisions)\n   bisect            Use binary search to find the commit that introduced a bug\n   diff              Show changes between commits, commit and working tree, etc\n   grep              Print lines matching a pattern\n   log               Show commit logs\n   show              Show various types of objects\n   status            Show the working tree status\n\ngrow, mark and tweak your common history\n   branch            List, create, or delete branches\n   commit            Record changes to the repository\n   merge             Join two or more development histories together\n   rebase            Reapply commits on top of another base tip\n   reset             Reset current HEAD to the specified state\n   switch            Switch branches\n   tag               Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch             Download objects and refs from another repository\n   pull              Fetch from and integrate with another repository or a local branch\n   push              Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m12:49:40.464703 [debug] [MainThread]: STDERR: "b''"
[0m12:49:40.465464 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m12:49:40.466480 [info ] [MainThread]: Connection:
[0m12:49:40.467280 [info ] [MainThread]:   host: spark-iceberg
[0m12:49:40.468196 [info ] [MainThread]:   port: 10000
[0m12:49:40.468976 [info ] [MainThread]:   cluster: None
[0m12:49:40.469859 [info ] [MainThread]:   endpoint: None
[0m12:49:40.470693 [info ] [MainThread]:   schema: default
[0m12:49:40.471749 [info ] [MainThread]:   organization: 0
[0m12:49:40.472706 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m12:49:40.506064 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m12:49:40.507982 [debug] [MainThread]: Using spark connection "debug"
[0m12:49:40.508957 [debug] [MainThread]: On debug: select 1 as id
[0m12:49:40.509993 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:49:40.541645 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m12:49:40.542924 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m12:49:40.544112 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m12:49:40.544997 [info ] [MainThread]: [31m1 check failed:[0m
[0m12:49:40.546091 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m12:49:40.547423 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 0.29426196, "process_in_blocks": "0", "process_kernel_time": 0.131699, "process_mem_max_rss": "99108", "process_out_blocks": "0", "process_user_time": 0.889972}
[0m12:49:40.548241 [debug] [MainThread]: Command `dbt debug` failed at 12:49:40.548153 after 0.30 seconds
[0m12:49:40.548853 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m12:49:40.549483 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76632079bdd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x766320745b90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7663200b0090>]}
[0m12:49:40.550257 [debug] [MainThread]: Flushing usage events
[0m12:49:43.541790 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:36:52.598971 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x779ea409f810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x779ea409f2d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x779ea409f210>]}


============================== 13:36:52.603767 | aa884cce-597f-45e9-8b0e-13f9ba02114f ==============================
[0m13:36:52.603767 [info ] [MainThread]: Running with dbt=1.9.0
[0m13:36:52.605342 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/usr/app/dbt/logs', 'profiles_dir': '/usr/app/dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt run', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m13:36:52.655457 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:36:52.657014 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:36:52.658133 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:36:52.755720 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'aa884cce-597f-45e9-8b0e-13f9ba02114f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x779ea4102cd0>]}
[0m13:36:52.794607 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'aa884cce-597f-45e9-8b0e-13f9ba02114f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x779ea45e6f10>]}
[0m13:36:52.795936 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m13:36:52.836002 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found 2 package(s) specified in packages.yml, but only 0 package(s) installed in dbt_packages. Run "dbt deps" to install package dependencies.
[0m13:36:52.837997 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.29578948, "process_in_blocks": "1224", "process_kernel_time": 0.144069, "process_mem_max_rss": "98876", "process_out_blocks": "0", "process_user_time": 0.852411}
[0m13:36:52.839077 [debug] [MainThread]: Command `dbt run` failed at 13:36:52.838966 after 0.30 seconds
[0m13:36:52.839899 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x779ea41008d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x779ea3a7ec10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x779ea79f9510>]}
[0m13:36:52.840727 [debug] [MainThread]: Flushing usage events
[0m13:37:13.105905 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:37:29.941807 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7457aee23b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7457af221ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7457af2211d0>]}


============================== 13:37:29.944150 | e39c6694-a175-488f-8bce-d25ae07e7f23 ==============================
[0m13:37:29.944150 [info ] [MainThread]: Running with dbt=1.9.0
[0m13:37:29.945404 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/app/dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/usr/app/dbt/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m13:37:29.981901 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:37:29.983478 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:37:29.984761 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:37:30.069959 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e39c6694-a175-488f-8bce-d25ae07e7f23', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7457ae975d90>]}
[0m13:37:30.104572 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e39c6694-a175-488f-8bce-d25ae07e7f23', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7457afbc8950>]}
[0m13:37:30.106264 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m13:37:30.139340 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found 2 package(s) specified in packages.yml, but only 0 package(s) installed in dbt_packages. Run "dbt deps" to install package dependencies.
[0m13:37:30.141940 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.23982829, "process_in_blocks": "0", "process_kernel_time": 0.122594, "process_mem_max_rss": "98324", "process_out_blocks": "0", "process_user_time": 0.830482}
[0m13:37:30.143615 [debug] [MainThread]: Command `dbt run` failed at 13:37:30.143476 after 0.24 seconds
[0m13:37:30.144880 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7457b277d3d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7457af2211d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7457ae5923d0>]}
[0m13:37:30.146410 [debug] [MainThread]: Flushing usage events
[0m13:38:07.845561 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d8805da850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d8805d8b10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d8805d9c50>]}


============================== 13:38:07.847986 | d5d3c157-4f3f-4530-9795-52d5b1cec30e ==============================
[0m13:38:07.847986 [info ] [MainThread]: Running with dbt=1.9.0
[0m13:38:07.849103 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/app/dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/usr/app/dbt/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt deps', 'send_anonymous_usage_stats': 'True'}
[0m13:38:07.920791 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd5d3c157-4f3f-4530-9795-52d5b1cec30e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d880456450>]}
[0m13:38:07.943533 [debug] [MainThread]: Set downloads directory='/tmp/dbt-downloads-5xmflmba'
[0m13:38:07.944758 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m13:38:08.362008 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m13:38:08.363824 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m13:38:08.644915 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m13:38:08.654998 [error] [MainThread]: Encountered an error:
Package dbt-labs/dbt_expectations was not found in the package index
[0m13:38:08.667648 [error] [MainThread]: Traceback (most recent call last):
  File "/usr/local/lib/python3.11/site-packages/dbt/cli/requires.py", line 153, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/dbt/cli/requires.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/dbt/cli/requires.py", line 218, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/dbt/cli/requires.py", line 264, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/dbt/cli/main.py", line 455, in deps
    results = task.run()
              ^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/dbt/task/deps.py", line 211, in run
    self.lock()
  File "/usr/local/lib/python3.11/site-packages/dbt/task/deps.py", line 187, in lock
    resolved_deps = resolve_packages(packages, self.project, self.cli_vars)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/dbt/deps/resolver.py", line 131, in resolve_packages
    target = final[package].resolved().fetch_metadata(project, renderer)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/dbt/deps/registry.py", line 98, in resolved
    self._check_in_index()
  File "/usr/local/lib/python3.11/site-packages/dbt/deps/registry.py", line 77, in _check_in_index
    raise PackageNotFoundError(self.package)
dbt.exceptions.PackageNotFoundError: Package dbt-labs/dbt_expectations was not found in the package index

[0m13:38:08.669948 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": false, "command_wall_clock_time": 0.8642403, "process_in_blocks": "1224", "process_kernel_time": 0.092193, "process_mem_max_rss": "90788", "process_out_blocks": "8", "process_user_time": 0.789654}
[0m13:38:08.671321 [debug] [MainThread]: Command `dbt deps` failed at 13:38:08.671196 after 0.87 seconds
[0m13:38:08.672301 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d880630e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d883f2d510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d883f2d410>]}
[0m13:38:08.674041 [debug] [MainThread]: Flushing usage events
[0m13:38:18.874082 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:39:05.611874 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eccd7a360d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eccd7a8be90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eccd7a35e90>]}


============================== 13:39:05.614721 | c8df0aeb-1b32-4be6-9ae7-40b3177505d5 ==============================
[0m13:39:05.614721 [info ] [MainThread]: Running with dbt=1.9.0
[0m13:39:05.616004 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/app/dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/usr/app/dbt/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt deps', 'send_anonymous_usage_stats': 'True'}
[0m13:39:05.685037 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c8df0aeb-1b32-4be6-9ae7-40b3177505d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eccd7a36190>]}
[0m13:39:05.696507 [debug] [MainThread]: Set downloads directory='/tmp/dbt-downloads-e36vhjl9'
[0m13:39:05.698102 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m13:39:05.975013 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m13:39:05.977691 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m13:39:06.938842 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m13:39:06.952116 [info ] [MainThread]: Updating lock file in file path: /usr/app/dbt/package-lock.yml
[0m13:39:06.958223 [debug] [MainThread]: Set downloads directory='/tmp/dbt-downloads-ht6yaqfk'
[0m13:39:06.962146 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m13:39:10.003838 [info ] [MainThread]: Installed from version 1.1.1
[0m13:39:10.005176 [info ] [MainThread]: Updated version available: 1.3.1
[0m13:39:10.006375 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': 'c8df0aeb-1b32-4be6-9ae7-40b3177505d5', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eccd78f3a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eccd89b42d0>]}
[0m13:39:10.007107 [info ] [MainThread]: 
[0m13:39:10.008126 [info ] [MainThread]: Updates available for packages: ['dbt-labs/dbt_utils']                 
Update your versions in packages.yml, then run dbt deps
[0m13:39:10.010039 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 4.4435964, "process_in_blocks": "168", "process_kernel_time": 0.279291, "process_mem_max_rss": "90496", "process_out_blocks": "200", "process_user_time": 0.907697}
[0m13:39:10.011283 [debug] [MainThread]: Command `dbt deps` succeeded at 13:39:10.011065 after 4.44 seconds
[0m13:39:10.012040 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eccd7a661d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eccdb352710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eccdb294d50>]}
[0m13:39:10.012807 [debug] [MainThread]: Flushing usage events
[0m13:39:20.182171 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:40:19.693114 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x713cc365f8d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x713cc36b2e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x713cc365d0d0>]}


============================== 13:40:19.696651 | 6b5d511c-4937-4315-be82-e8cb9aee4666 ==============================
[0m13:40:19.696651 [info ] [MainThread]: Running with dbt=1.9.0
[0m13:40:19.698099 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/app/dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/usr/app/dbt/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt debug', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m13:40:19.708959 [info ] [MainThread]: dbt version: 1.9.0
[0m13:40:19.709903 [info ] [MainThread]: python version: 3.11.2
[0m13:40:19.711007 [info ] [MainThread]: python path: /usr/local/bin/python
[0m13:40:19.711890 [info ] [MainThread]: os info: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.31
[0m13:40:19.758962 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:40:19.760030 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:40:19.760998 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:40:19.782608 [info ] [MainThread]: Using profiles dir at /usr/app/dbt
[0m13:40:19.784004 [info ] [MainThread]: Using profiles.yml file at /usr/app/dbt/profiles.yml
[0m13:40:19.785485 [info ] [MainThread]: Using dbt_project.yml file at /usr/app/dbt/dbt_project.yml
[0m13:40:19.786483 [info ] [MainThread]: adapter type: spark
[0m13:40:19.787330 [info ] [MainThread]: adapter version: 1.9.0
[0m13:40:19.856254 [info ] [MainThread]: Configuration:
[0m13:40:19.857655 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m13:40:19.859020 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m13:40:19.860084 [info ] [MainThread]: Required dependencies:
[0m13:40:19.861106 [debug] [MainThread]: Executing "git --help"
[0m13:40:19.872194 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone             Clone a repository into a new directory\n   init              Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add               Add file contents to the index\n   mv                Move or rename a file, a directory, or a symlink\n   restore           Restore working tree files\n   rm                Remove files from the working tree and from the index\n   sparse-checkout   Initialize and modify the sparse-checkout\n\nexamine the history and state (see also: git help revisions)\n   bisect            Use binary search to find the commit that introduced a bug\n   diff              Show changes between commits, commit and working tree, etc\n   grep              Print lines matching a pattern\n   log               Show commit logs\n   show              Show various types of objects\n   status            Show the working tree status\n\ngrow, mark and tweak your common history\n   branch            List, create, or delete branches\n   commit            Record changes to the repository\n   merge             Join two or more development histories together\n   rebase            Reapply commits on top of another base tip\n   reset             Reset current HEAD to the specified state\n   switch            Switch branches\n   tag               Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch             Download objects and refs from another repository\n   pull              Fetch from and integrate with another repository or a local branch\n   push              Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m13:40:19.873160 [debug] [MainThread]: STDERR: "b''"
[0m13:40:19.873842 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m13:40:19.874671 [info ] [MainThread]: Connection:
[0m13:40:19.875792 [info ] [MainThread]:   host: spark-iceberg
[0m13:40:19.876595 [info ] [MainThread]:   port: 10000
[0m13:40:19.877283 [info ] [MainThread]:   cluster: None
[0m13:40:19.877950 [info ] [MainThread]:   endpoint: None
[0m13:40:19.878703 [info ] [MainThread]:   schema: default
[0m13:40:19.879621 [info ] [MainThread]:   organization: 0
[0m13:40:19.880587 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m13:40:19.916781 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m13:40:19.917814 [debug] [MainThread]: Using spark connection "debug"
[0m13:40:19.918536 [debug] [MainThread]: On debug: select 1 as id
[0m13:40:19.919178 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:40:21.640653 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m13:40:21.641840 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m13:40:21.642703 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m13:40:21.643941 [info ] [MainThread]: [31m1 check failed:[0m
[0m13:40:21.645223 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m13:40:21.646950 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 1.9987814, "process_in_blocks": "112", "process_kernel_time": 0.144258, "process_mem_max_rss": "99224", "process_out_blocks": "0", "process_user_time": 0.849521}
[0m13:40:21.648186 [debug] [MainThread]: Command `dbt debug` failed at 13:40:21.648061 after 2.00 seconds
[0m13:40:21.649108 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m13:40:21.650371 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x713cc365ec90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x713cc36b5350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x713cc2e06250>]}
[0m13:40:21.651898 [debug] [MainThread]: Flushing usage events
[0m13:40:31.951976 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:44:20.272103 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x707fecd97850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x707fecd97c90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x707fecd97d90>]}


============================== 13:44:20.274747 | db806628-37a8-4602-9509-44ae62389539 ==============================
[0m13:44:20.274747 [info ] [MainThread]: Running with dbt=1.9.0
[0m13:44:20.277116 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/usr/app/dbt/logs', 'profiles_dir': '/usr/app/dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'invocation_command': 'dbt debug', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m13:44:20.292222 [info ] [MainThread]: dbt version: 1.9.0
[0m13:44:20.294264 [info ] [MainThread]: python version: 3.11.2
[0m13:44:20.295386 [info ] [MainThread]: python path: /usr/local/bin/python
[0m13:44:20.296541 [info ] [MainThread]: os info: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.31
[0m13:44:20.342082 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:44:20.343860 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:44:20.344619 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:44:20.366082 [info ] [MainThread]: Using profiles dir at /usr/app/dbt
[0m13:44:20.367685 [info ] [MainThread]: Using profiles.yml file at /usr/app/dbt/profiles.yml
[0m13:44:20.368602 [info ] [MainThread]: Using dbt_project.yml file at /usr/app/dbt/dbt_project.yml
[0m13:44:20.369513 [info ] [MainThread]: adapter type: spark
[0m13:44:20.370589 [info ] [MainThread]: adapter version: 1.9.0
[0m13:44:20.437297 [info ] [MainThread]: Configuration:
[0m13:44:20.438614 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m13:44:20.439549 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m13:44:20.440492 [info ] [MainThread]: Required dependencies:
[0m13:44:20.441338 [debug] [MainThread]: Executing "git --help"
[0m13:44:20.451698 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone             Clone a repository into a new directory\n   init              Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add               Add file contents to the index\n   mv                Move or rename a file, a directory, or a symlink\n   restore           Restore working tree files\n   rm                Remove files from the working tree and from the index\n   sparse-checkout   Initialize and modify the sparse-checkout\n\nexamine the history and state (see also: git help revisions)\n   bisect            Use binary search to find the commit that introduced a bug\n   diff              Show changes between commits, commit and working tree, etc\n   grep              Print lines matching a pattern\n   log               Show commit logs\n   show              Show various types of objects\n   status            Show the working tree status\n\ngrow, mark and tweak your common history\n   branch            List, create, or delete branches\n   commit            Record changes to the repository\n   merge             Join two or more development histories together\n   rebase            Reapply commits on top of another base tip\n   reset             Reset current HEAD to the specified state\n   switch            Switch branches\n   tag               Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch             Download objects and refs from another repository\n   pull              Fetch from and integrate with another repository or a local branch\n   push              Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m13:44:20.452783 [debug] [MainThread]: STDERR: "b''"
[0m13:44:20.453502 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m13:44:20.454614 [info ] [MainThread]: Connection:
[0m13:44:20.455490 [info ] [MainThread]:   host: spark-iceberg
[0m13:44:20.456588 [info ] [MainThread]:   port: 10000
[0m13:44:20.457425 [info ] [MainThread]:   cluster: None
[0m13:44:20.458168 [info ] [MainThread]:   endpoint: None
[0m13:44:20.459203 [info ] [MainThread]:   schema: default
[0m13:44:20.460534 [info ] [MainThread]:   organization: 0
[0m13:44:20.461801 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m13:44:20.500139 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m13:44:20.501010 [debug] [MainThread]: Using spark connection "debug"
[0m13:44:20.501747 [debug] [MainThread]: On debug: select 1 as id
[0m13:44:20.502427 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:44:20.535160 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m13:44:20.536466 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m13:44:20.537280 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m13:44:20.538270 [info ] [MainThread]: [31m1 check failed:[0m
[0m13:44:20.538901 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m13:44:20.540893 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 0.31613076, "process_in_blocks": "1440", "process_kernel_time": 0.145942, "process_mem_max_rss": "99268", "process_out_blocks": "0", "process_user_time": 0.8716}
[0m13:44:20.541731 [debug] [MainThread]: Command `dbt debug` failed at 13:44:20.541644 after 0.32 seconds
[0m13:44:20.542623 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m13:44:20.543367 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x707ff06954d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x707ff0801750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x707fec716c10>]}
[0m13:44:20.544296 [debug] [MainThread]: Flushing usage events
[0m13:44:40.805307 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:10:49.821446 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bacd75ab350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bacd75ab450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bacd7959950>]}


============================== 15:10:49.824986 | c37a7045-fb1c-490f-8f8e-e3fac8fefc8c ==============================
[0m15:10:49.824986 [info ] [MainThread]: Running with dbt=1.9.0
[0m15:10:49.826089 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/usr/app/dbt/logs', 'profiles_dir': '/usr/app/dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt debug', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:10:49.843237 [info ] [MainThread]: dbt version: 1.9.0
[0m15:10:49.844417 [info ] [MainThread]: python version: 3.11.2
[0m15:10:49.845464 [info ] [MainThread]: python path: /usr/local/bin/python
[0m15:10:49.846377 [info ] [MainThread]: os info: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.31
[0m15:10:49.895912 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m15:10:49.897011 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m15:10:49.898384 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m15:10:49.923525 [info ] [MainThread]: Using profiles dir at /usr/app/dbt
[0m15:10:49.924620 [info ] [MainThread]: Using profiles.yml file at /usr/app/dbt/profiles.yml
[0m15:10:49.925479 [info ] [MainThread]: Using dbt_project.yml file at /usr/app/dbt/dbt_project.yml
[0m15:10:49.926327 [info ] [MainThread]: adapter type: spark
[0m15:10:49.927107 [info ] [MainThread]: adapter version: 1.9.0
[0m15:10:49.995449 [info ] [MainThread]: Configuration:
[0m15:10:49.996868 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m15:10:49.997784 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m15:10:49.998740 [info ] [MainThread]: Required dependencies:
[0m15:10:50.000145 [debug] [MainThread]: Executing "git --help"
[0m15:10:50.013017 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone             Clone a repository into a new directory\n   init              Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add               Add file contents to the index\n   mv                Move or rename a file, a directory, or a symlink\n   restore           Restore working tree files\n   rm                Remove files from the working tree and from the index\n   sparse-checkout   Initialize and modify the sparse-checkout\n\nexamine the history and state (see also: git help revisions)\n   bisect            Use binary search to find the commit that introduced a bug\n   diff              Show changes between commits, commit and working tree, etc\n   grep              Print lines matching a pattern\n   log               Show commit logs\n   show              Show various types of objects\n   status            Show the working tree status\n\ngrow, mark and tweak your common history\n   branch            List, create, or delete branches\n   commit            Record changes to the repository\n   merge             Join two or more development histories together\n   rebase            Reapply commits on top of another base tip\n   reset             Reset current HEAD to the specified state\n   switch            Switch branches\n   tag               Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch             Download objects and refs from another repository\n   pull              Fetch from and integrate with another repository or a local branch\n   push              Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m15:10:50.013773 [debug] [MainThread]: STDERR: "b''"
[0m15:10:50.014380 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m15:10:50.016135 [info ] [MainThread]: Connection:
[0m15:10:50.017542 [info ] [MainThread]:   host: spark-iceberg
[0m15:10:50.018644 [info ] [MainThread]:   port: 10000
[0m15:10:50.019724 [info ] [MainThread]:   cluster: None
[0m15:10:50.020544 [info ] [MainThread]:   endpoint: None
[0m15:10:50.021538 [info ] [MainThread]:   schema: default
[0m15:10:50.022406 [info ] [MainThread]:   organization: 0
[0m15:10:50.023499 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m15:10:50.074566 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m15:10:50.075779 [debug] [MainThread]: Using spark connection "debug"
[0m15:10:50.076824 [debug] [MainThread]: On debug: select 1 as id
[0m15:10:50.077746 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:10:52.175515 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m15:10:52.176567 [debug] [MainThread]: SQL status: OK in 2.099 seconds
[0m15:10:52.177824 [debug] [MainThread]: On debug: Close
[0m15:10:52.203220 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m15:10:52.204485 [info ] [MainThread]: [32mAll checks passed![0m
[0m15:10:52.207013 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 2.4388652, "process_in_blocks": "1456", "process_kernel_time": 0.171828, "process_mem_max_rss": "99196", "process_out_blocks": "24", "process_user_time": 0.931068}
[0m15:10:52.208076 [debug] [MainThread]: Command `dbt debug` succeeded at 15:10:52.207967 after 2.44 seconds
[0m15:10:52.208892 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m15:10:52.209797 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bacd6fa0190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bacd794a4d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bacdae3e6d0>]}
[0m15:10:52.210608 [debug] [MainThread]: Flushing usage events
[0m15:10:53.079168 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:11:07.561977 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x769a384f12d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x769a384eaf50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x769a3854b650>]}


============================== 15:11:07.565185 | b9cba657-fdec-450a-98ad-e9f93a6794ae ==============================
[0m15:11:07.565185 [info ] [MainThread]: Running with dbt=1.9.0
[0m15:11:07.566136 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/app/dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/usr/app/dbt/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m15:11:07.612863 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m15:11:07.614108 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m15:11:07.614935 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m15:11:07.727346 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b9cba657-fdec-450a-98ad-e9f93a6794ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x769a37dbe210>]}
[0m15:11:07.765761 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b9cba657-fdec-450a-98ad-e9f93a6794ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x769a3929f150>]}
[0m15:11:07.766916 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m15:11:07.825193 [debug] [MainThread]: checksum: c99e828bba267739642b5a3ce85f17518764ea526e0e6c4fdc649171c1a66bff, vars: {}, profile: , target: , version: 1.9.0
[0m15:11:07.830062 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m15:11:07.831000 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'b9cba657-fdec-450a-98ad-e9f93a6794ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x769a37c5ce90>]}
[0m15:11:09.580395 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- seeds.data_pipeline_poc
- snapshots.data_pipeline_poc
[0m15:11:09.588508 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b9cba657-fdec-450a-98ad-e9f93a6794ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x769a37c3f490>]}
[0m15:11:09.666061 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/app/dbt/target/manifest.json
[0m15:11:09.671354 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/app/dbt/target/semantic_manifest.json
[0m15:11:09.695594 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b9cba657-fdec-450a-98ad-e9f93a6794ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x769a346a2dd0>]}
[0m15:11:09.696614 [info ] [MainThread]: Found 3 models, 4 data tests, 2 sources, 583 macros
[0m15:11:09.697740 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b9cba657-fdec-450a-98ad-e9f93a6794ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x769a34765f50>]}
[0m15:11:09.699458 [info ] [MainThread]: 
[0m15:11:09.700420 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:11:09.701160 [info ] [MainThread]: 
[0m15:11:09.702136 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m15:11:09.707606 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m15:11:09.715356 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m15:11:09.716074 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m15:11:09.716738 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:11:10.421740 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:11:10.423062 [debug] [ThreadPool]: SQL status: OK in 0.706 seconds
[0m15:11:10.474751 [debug] [ThreadPool]: On list_schemas: Close
[0m15:11:10.480643 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m15:11:10.481635 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m15:11:10.482402 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:11:10.549794 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:11:10.550840 [debug] [ThreadPool]: SQL status: OK in 0.068 seconds
[0m15:11:10.554347 [debug] [ThreadPool]: On list_schemas: Close
[0m15:11:10.558551 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__default_default_gold)
[0m15:11:10.559584 [debug] [ThreadPool]: Creating schema "schema: "default_default_gold"
"
[0m15:11:10.562947 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:11:10.563659 [debug] [ThreadPool]: Using spark connection "create__default_default_gold"
[0m15:11:10.564321 [debug] [ThreadPool]: On create__default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "create__default_default_gold"} */
create schema if not exists default_default_gold
  
[0m15:11:10.564930 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:11:10.669958 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:11:10.671277 [debug] [ThreadPool]: SQL status: OK in 0.106 seconds
[0m15:11:10.672802 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m15:11:10.673781 [debug] [ThreadPool]: On create__default_default_gold: ROLLBACK
[0m15:11:10.674759 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m15:11:10.675666 [debug] [ThreadPool]: On create__default_default_gold: Close
[0m15:11:10.680344 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create__default_default_gold, now create__default_default_silver)
[0m15:11:10.681645 [debug] [ThreadPool]: Creating schema "schema: "default_default_silver"
"
[0m15:11:10.684029 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:11:10.685066 [debug] [ThreadPool]: Using spark connection "create__default_default_silver"
[0m15:11:10.685868 [debug] [ThreadPool]: On create__default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "create__default_default_silver"} */
create schema if not exists default_default_silver
  
[0m15:11:10.686744 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:11:10.777162 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:11:10.778844 [debug] [ThreadPool]: SQL status: OK in 0.092 seconds
[0m15:11:10.780595 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m15:11:10.781812 [debug] [ThreadPool]: On create__default_default_silver: ROLLBACK
[0m15:11:10.783190 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m15:11:10.784279 [debug] [ThreadPool]: On create__default_default_silver: Close
[0m15:11:10.794163 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create__default_default_silver, now list_None_default_default_gold)
[0m15:11:10.800090 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:11:10.801099 [debug] [ThreadPool]: Using spark connection "list_None_default_default_gold"
[0m15:11:10.801884 [debug] [ThreadPool]: On list_None_default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show table extended in default_default_gold like '*'
  
[0m15:11:10.802973 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:11:10.887770 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#28, tableName#29, isTemporary#30, information#31]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@64947ef7, [default_default_gold]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#28, tableName#29, isTemporary#30, information#31]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@64947ef7, [default_default_gold]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m15:11:10.889700 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m15:11:10.891024 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show table extended in default_default_gold like '*'
  
[0m15:11:10.893946 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#28, tableName#29, isTemporary#30, information#31]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@64947ef7, [default_default_gold]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#28, tableName#29, isTemporary#30, information#31]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@64947ef7, [default_default_gold]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m15:11:10.895971 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m15:11:10.898083 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#28, tableName#29, isTemporary#30, information#31]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@64947ef7, [default_default_gold]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#28, tableName#29, isTemporary#30, information#31]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@64947ef7, [default_default_gold]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m15:11:10.906821 [debug] [ThreadPool]: Using spark connection "list_None_default_default_gold"
[0m15:11:10.910095 [debug] [ThreadPool]: On list_None_default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show tables in default_default_gold like '*'
  
[0m15:11:10.965350 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:11:10.966817 [debug] [ThreadPool]: SQL status: OK in 0.055 seconds
[0m15:11:10.973201 [debug] [ThreadPool]: On list_None_default_default_gold: ROLLBACK
[0m15:11:10.974467 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m15:11:10.975485 [debug] [ThreadPool]: On list_None_default_default_gold: Close
[0m15:11:10.982773 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default_default_gold, now list_None_default_default_silver)
[0m15:11:10.985970 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:11:10.988380 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m15:11:10.991939 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show table extended in default_default_silver like '*'
  
[0m15:11:10.993366 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:11:11.040753 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#43, tableName#44, isTemporary#45, information#46]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@4011a307, [default_default_silver]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#43, tableName#44, isTemporary#45, information#46]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@4011a307, [default_default_silver]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m15:11:11.042319 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m15:11:11.043333 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show table extended in default_default_silver like '*'
  
[0m15:11:11.044502 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#43, tableName#44, isTemporary#45, information#46]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@4011a307, [default_default_silver]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#43, tableName#44, isTemporary#45, information#46]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@4011a307, [default_default_silver]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m15:11:11.045716 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m15:11:11.046916 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#43, tableName#44, isTemporary#45, information#46]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@4011a307, [default_default_silver]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#43, tableName#44, isTemporary#45, information#46]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@4011a307, [default_default_silver]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m15:11:11.050669 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m15:11:11.051991 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show tables in default_default_silver like '*'
  
[0m15:11:11.080587 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:11:11.081701 [debug] [ThreadPool]: SQL status: OK in 0.028 seconds
[0m15:11:11.084687 [debug] [ThreadPool]: On list_None_default_default_silver: ROLLBACK
[0m15:11:11.085512 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m15:11:11.086248 [debug] [ThreadPool]: On list_None_default_default_silver: Close
[0m15:11:11.090381 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b9cba657-fdec-450a-98ad-e9f93a6794ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x769a346e7d50>]}
[0m15:11:11.091594 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:11:11.092406 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:11:11.094599 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.stg_events
[0m15:11:11.095483 [info ] [Thread-1 (]: 1 of 3 START sql view model default_default_silver.stg_events .................. [RUN]
[0m15:11:11.096472 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default_default_silver, now model.data_pipeline_poc.stg_events)
[0m15:11:11.097304 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_poc.stg_events
[0m15:11:11.103581 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_poc.stg_events"
[0m15:11:11.117124 [debug] [Thread-1 (]: Began executing node model.data_pipeline_poc.stg_events
[0m15:11:11.135482 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_poc.stg_events"
[0m15:11:11.148712 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:11:11.149587 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.stg_events"
[0m15:11:11.150360 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_events: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.stg_events"} */
create or replace view default_default_silver.stg_events
  
  
  as
    

-- Staging model for raw events from bronze layer
-- This model cleans and standardizes the raw event data

SELECT
    event_id,
    event_type,
    user_id,
    event_timestamp,
    event_data,
    current_timestamp() as dbt_loaded_at
FROM bronze.raw_events
WHERE event_id IS NOT NULL

[0m15:11:11.151062 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:11:12.153453 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.UnsupportedOperationException: Server does not support endpoint: POST /v1/{prefix}/namespaces/{namespace}/views\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:46)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.UnsupportedOperationException: Server does not support endpoint: POST /v1/{prefix}/namespaces/{namespace}/views\n\tat org.apache.iceberg.rest.Endpoint.check(Endpoint.java:140)\n\tat org.apache.iceberg.rest.RESTSessionCatalog$RESTViewBuilder.create(RESTSessionCatalog.java:1400)\n\tat org.apache.iceberg.rest.RESTSessionCatalog$RESTViewBuilder.createOrReplace(RESTSessionCatalog.java:1448)\n\tat org.apache.iceberg.spark.SparkCatalog.replaceView(SparkCatalog.java:641)\n\tat org.apache.spark.sql.execution.datasources.v2.CreateV2ViewExec.replaceView(CreateV2ViewExec.scala:103)\n\tat org.apache.spark.sql.execution.datasources.v2.CreateV2ViewExec.run(CreateV2ViewExec.scala:64)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m15:11:12.154864 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m15:11:12.156232 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.stg_events"} */
create or replace view default_default_silver.stg_events
  
  
  as
    

-- Staging model for raw events from bronze layer
-- This model cleans and standardizes the raw event data

SELECT
    event_id,
    event_type,
    user_id,
    event_timestamp,
    event_data,
    current_timestamp() as dbt_loaded_at
FROM bronze.raw_events
WHERE event_id IS NOT NULL

[0m15:11:12.157519 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.UnsupportedOperationException: Server does not support endpoint: POST /v1/{prefix}/namespaces/{namespace}/views
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:46)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: java.lang.UnsupportedOperationException: Server does not support endpoint: POST /v1/{prefix}/namespaces/{namespace}/views
  	at org.apache.iceberg.rest.Endpoint.check(Endpoint.java:140)
  	at org.apache.iceberg.rest.RESTSessionCatalog$RESTViewBuilder.create(RESTSessionCatalog.java:1400)
  	at org.apache.iceberg.rest.RESTSessionCatalog$RESTViewBuilder.createOrReplace(RESTSessionCatalog.java:1448)
  	at org.apache.iceberg.spark.SparkCatalog.replaceView(SparkCatalog.java:641)
  	at org.apache.spark.sql.execution.datasources.v2.CreateV2ViewExec.replaceView(CreateV2ViewExec.scala:103)
  	at org.apache.spark.sql.execution.datasources.v2.CreateV2ViewExec.run(CreateV2ViewExec.scala:64)
  	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
  	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
  	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
  	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
  	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
  	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
  	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
  	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
  	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
  	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
  	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
  	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m15:11:12.158711 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_events: ROLLBACK
[0m15:11:12.159542 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:11:12.160361 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_events: Close
[0m15:11:12.176756 [debug] [Thread-1 (]: Runtime Error in model stg_events (models/staging/stg_events.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.UnsupportedOperationException: Server does not support endpoint: POST /v1/{prefix}/namespaces/{namespace}/views
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:46)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: java.lang.UnsupportedOperationException: Server does not support endpoint: POST /v1/{prefix}/namespaces/{namespace}/views
    	at org.apache.iceberg.rest.Endpoint.check(Endpoint.java:140)
    	at org.apache.iceberg.rest.RESTSessionCatalog$RESTViewBuilder.create(RESTSessionCatalog.java:1400)
    	at org.apache.iceberg.rest.RESTSessionCatalog$RESTViewBuilder.createOrReplace(RESTSessionCatalog.java:1448)
    	at org.apache.iceberg.spark.SparkCatalog.replaceView(SparkCatalog.java:641)
    	at org.apache.spark.sql.execution.datasources.v2.CreateV2ViewExec.replaceView(CreateV2ViewExec.scala:103)
    	at org.apache.spark.sql.execution.datasources.v2.CreateV2ViewExec.run(CreateV2ViewExec.scala:64)
    	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
    	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
    	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
    	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
    	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
    	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
    	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
    	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
    	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
    	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
    	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
    	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
    	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
    	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m15:11:12.183114 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b9cba657-fdec-450a-98ad-e9f93a6794ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x769a37a60c10>]}
[0m15:11:12.184421 [error] [Thread-1 (]: 1 of 3 ERROR creating sql view model default_default_silver.stg_events ......... [[31mERROR[0m in 1.09s]
[0m15:11:12.186088 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.stg_events
[0m15:11:12.187065 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.stg_users
[0m15:11:12.187801 [debug] [Thread-4 (]: Marking all children of 'model.data_pipeline_poc.stg_events' to be skipped because of status 'error'.  Reason: Runtime Error in model stg_events (models/staging/stg_events.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.UnsupportedOperationException: Server does not support endpoint: POST /v1/{prefix}/namespaces/{namespace}/views
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:46)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: java.lang.UnsupportedOperationException: Server does not support endpoint: POST /v1/{prefix}/namespaces/{namespace}/views
    	at org.apache.iceberg.rest.Endpoint.check(Endpoint.java:140)
    	at org.apache.iceberg.rest.RESTSessionCatalog$RESTViewBuilder.create(RESTSessionCatalog.java:1400)
    	at org.apache.iceberg.rest.RESTSessionCatalog$RESTViewBuilder.createOrReplace(RESTSessionCatalog.java:1448)
    	at org.apache.iceberg.spark.SparkCatalog.replaceView(SparkCatalog.java:641)
    	at org.apache.spark.sql.execution.datasources.v2.CreateV2ViewExec.replaceView(CreateV2ViewExec.scala:103)
    	at org.apache.spark.sql.execution.datasources.v2.CreateV2ViewExec.run(CreateV2ViewExec.scala:64)
    	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
    	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
    	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
    	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
    	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
    	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
    	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
    	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
    	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
    	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
    	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
    	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
    	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
    	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    .
[0m15:11:12.188884 [info ] [Thread-1 (]: 2 of 3 START sql view model default_default_silver.stg_users ................... [RUN]
[0m15:11:12.191505 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline_poc.stg_events, now model.data_pipeline_poc.stg_users)
[0m15:11:12.192339 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_poc.stg_users
[0m15:11:12.195570 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_poc.stg_users"
[0m15:11:12.201789 [debug] [Thread-1 (]: Began executing node model.data_pipeline_poc.stg_users
[0m15:11:12.204319 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_poc.stg_users"
[0m15:11:12.212518 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:11:12.213514 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.stg_users"
[0m15:11:12.214350 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_users: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.stg_users"} */
create or replace view default_default_silver.stg_users
  
  
  as
    

-- Staging model for raw users from bronze layer
-- This model cleans and standardizes the raw user data

SELECT
    user_id,
    user_name,
    user_email,
    created_at,
    current_timestamp() as dbt_loaded_at
FROM bronze.raw_users
WHERE user_id IS NOT NULL

[0m15:11:12.215085 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:11:12.325185 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='42703', errorCode=0, errorMessage="org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `user_email` cannot be resolved. Did you mean one of the following? [`email`, `user_id`, `user_name`, `created_at`].; line 14 pos 4;\n'CreateIcebergView SELECT\n    user_id,\n    user_name,\n    user_email,\n    created_at,\n    current_timestamp() as dbt_loaded_at\nFROM bronze.raw_users\nWHERE user_id IS NOT NULL, false, true, false\n:- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@766d4dfe, default_default_silver.stg_users\n+- 'Project [user_id#76, user_name#77, 'user_email, created_at#79, current_timestamp() AS dbt_loaded_at#75]\n   +- Filter isnotnull(user_id#76)\n      +- SubqueryAlias spark_catalog.bronze.raw_users\n         +- RelationV2[user_id#76, user_name#77, email#78, created_at#79] spark_catalog.bronze.raw_users spark_catalog.bronze.raw_users\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `user_email` cannot be resolved. Did you mean one of the following? [`email`, `user_id`, `user_name`, `created_at`].; line 14 pos 4;\n'CreateIcebergView SELECT\n    user_id,\n    user_name,\n    user_email,\n    created_at,\n    current_timestamp() as dbt_loaded_at\nFROM bronze.raw_users\nWHERE user_id IS NOT NULL, false, true, false\n:- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@766d4dfe, default_default_silver.stg_users\n+- 'Project [user_id#76, user_name#77, 'user_email, created_at#79, current_timestamp() AS dbt_loaded_at#75]\n   +- Filter isnotnull(user_id#76)\n      +- SubqueryAlias spark_catalog.bronze.raw_users\n         +- RelationV2[user_id#76, user_name#77, email#78, created_at#79] spark_catalog.bronze.raw_users spark_catalog.bronze.raw_users\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)\n\tat scala.collection.immutable.Stream.foreach(Stream.scala:533)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n", taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m15:11:12.326764 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m15:11:12.327871 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.stg_users"} */
create or replace view default_default_silver.stg_users
  
  
  as
    

-- Staging model for raw users from bronze layer
-- This model cleans and standardizes the raw user data

SELECT
    user_id,
    user_name,
    user_email,
    created_at,
    current_timestamp() as dbt_loaded_at
FROM bronze.raw_users
WHERE user_id IS NOT NULL

[0m15:11:12.329089 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `user_email` cannot be resolved. Did you mean one of the following? [`email`, `user_id`, `user_name`, `created_at`].; line 14 pos 4;
  'CreateIcebergView SELECT
      user_id,
      user_name,
      user_email,
      created_at,
      current_timestamp() as dbt_loaded_at
  FROM bronze.raw_users
  WHERE user_id IS NOT NULL, false, true, false
  :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@766d4dfe, default_default_silver.stg_users
  +- 'Project [user_id#76, user_name#77, 'user_email, created_at#79, current_timestamp() AS dbt_loaded_at#75]
     +- Filter isnotnull(user_id#76)
        +- SubqueryAlias spark_catalog.bronze.raw_users
           +- RelationV2[user_id#76, user_name#77, email#78, created_at#79] spark_catalog.bronze.raw_users spark_catalog.bronze.raw_users
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `user_email` cannot be resolved. Did you mean one of the following? [`email`, `user_id`, `user_name`, `created_at`].; line 14 pos 4;
  'CreateIcebergView SELECT
      user_id,
      user_name,
      user_email,
      created_at,
      current_timestamp() as dbt_loaded_at
  FROM bronze.raw_users
  WHERE user_id IS NOT NULL, false, true, false
  :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@766d4dfe, default_default_silver.stg_users
  +- 'Project [user_id#76, user_name#77, 'user_email, created_at#79, current_timestamp() AS dbt_loaded_at#75]
     +- Filter isnotnull(user_id#76)
        +- SubqueryAlias spark_catalog.bronze.raw_users
           +- RelationV2[user_id#76, user_name#77, email#78, created_at#79] spark_catalog.bronze.raw_users spark_catalog.bronze.raw_users
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
  	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m15:11:12.330396 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_users: ROLLBACK
[0m15:11:12.331250 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:11:12.332283 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_users: Close
[0m15:11:12.338917 [debug] [Thread-1 (]: Runtime Error in model stg_users (models/staging/stg_users.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `user_email` cannot be resolved. Did you mean one of the following? [`email`, `user_id`, `user_name`, `created_at`].; line 14 pos 4;
    'CreateIcebergView SELECT
        user_id,
        user_name,
        user_email,
        created_at,
        current_timestamp() as dbt_loaded_at
    FROM bronze.raw_users
    WHERE user_id IS NOT NULL, false, true, false
    :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@766d4dfe, default_default_silver.stg_users
    +- 'Project [user_id#76, user_name#77, 'user_email, created_at#79, current_timestamp() AS dbt_loaded_at#75]
       +- Filter isnotnull(user_id#76)
          +- SubqueryAlias spark_catalog.bronze.raw_users
             +- RelationV2[user_id#76, user_name#77, email#78, created_at#79] spark_catalog.bronze.raw_users spark_catalog.bronze.raw_users
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `user_email` cannot be resolved. Did you mean one of the following? [`email`, `user_id`, `user_name`, `created_at`].; line 14 pos 4;
    'CreateIcebergView SELECT
        user_id,
        user_name,
        user_email,
        created_at,
        current_timestamp() as dbt_loaded_at
    FROM bronze.raw_users
    WHERE user_id IS NOT NULL, false, true, false
    :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@766d4dfe, default_default_silver.stg_users
    +- 'Project [user_id#76, user_name#77, 'user_email, created_at#79, current_timestamp() AS dbt_loaded_at#75]
       +- Filter isnotnull(user_id#76)
          +- SubqueryAlias spark_catalog.bronze.raw_users
             +- RelationV2[user_id#76, user_name#77, email#78, created_at#79] spark_catalog.bronze.raw_users spark_catalog.bronze.raw_users
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m15:11:12.340562 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b9cba657-fdec-450a-98ad-e9f93a6794ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x769a3454bcd0>]}
[0m15:11:12.341784 [error] [Thread-1 (]: 2 of 3 ERROR creating sql view model default_default_silver.stg_users .......... [[31mERROR[0m in 0.15s]
[0m15:11:12.343251 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.stg_users
[0m15:11:12.344549 [debug] [Thread-4 (]: Marking all children of 'model.data_pipeline_poc.stg_users' to be skipped because of status 'error'.  Reason: Runtime Error in model stg_users (models/staging/stg_users.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `user_email` cannot be resolved. Did you mean one of the following? [`email`, `user_id`, `user_name`, `created_at`].; line 14 pos 4;
    'CreateIcebergView SELECT
        user_id,
        user_name,
        user_email,
        created_at,
        current_timestamp() as dbt_loaded_at
    FROM bronze.raw_users
    WHERE user_id IS NOT NULL, false, true, false
    :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@766d4dfe, default_default_silver.stg_users
    +- 'Project [user_id#76, user_name#77, 'user_email, created_at#79, current_timestamp() AS dbt_loaded_at#75]
       +- Filter isnotnull(user_id#76)
          +- SubqueryAlias spark_catalog.bronze.raw_users
             +- RelationV2[user_id#76, user_name#77, email#78, created_at#79] spark_catalog.bronze.raw_users spark_catalog.bronze.raw_users
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `user_email` cannot be resolved. Did you mean one of the following? [`email`, `user_id`, `user_name`, `created_at`].; line 14 pos 4;
    'CreateIcebergView SELECT
        user_id,
        user_name,
        user_email,
        created_at,
        current_timestamp() as dbt_loaded_at
    FROM bronze.raw_users
    WHERE user_id IS NOT NULL, false, true, false
    :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@766d4dfe, default_default_silver.stg_users
    +- 'Project [user_id#76, user_name#77, 'user_email, created_at#79, current_timestamp() AS dbt_loaded_at#75]
       +- Filter isnotnull(user_id#76)
          +- SubqueryAlias spark_catalog.bronze.raw_users
             +- RelationV2[user_id#76, user_name#77, email#78, created_at#79] spark_catalog.bronze.raw_users spark_catalog.bronze.raw_users
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    .
[0m15:11:12.346331 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.fct_events_enriched
[0m15:11:12.347190 [info ] [Thread-1 (]: 3 of 3 SKIP relation default_default_gold.fct_events_enriched .................. [[33mSKIP[0m]
[0m15:11:12.348147 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.fct_events_enriched
[0m15:11:12.350091 [debug] [MainThread]: On master: ROLLBACK
[0m15:11:12.351180 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:11:12.381124 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m15:11:12.382561 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:11:12.383382 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:11:12.384069 [debug] [MainThread]: On master: ROLLBACK
[0m15:11:12.384769 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m15:11:12.385448 [debug] [MainThread]: On master: Close
[0m15:11:12.388052 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:11:12.388819 [debug] [MainThread]: Connection 'model.data_pipeline_poc.stg_users' was properly closed.
[0m15:11:12.390363 [info ] [MainThread]: 
[0m15:11:12.391405 [info ] [MainThread]: Finished running 1 table model, 2 view models in 0 hours 0 minutes and 2.69 seconds (2.69s).
[0m15:11:12.393143 [debug] [MainThread]: Command end result
[0m15:11:12.424721 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/app/dbt/target/manifest.json
[0m15:11:12.428756 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/app/dbt/target/semantic_manifest.json
[0m15:11:12.435327 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/app/dbt/target/run_results.json
[0m15:11:12.436023 [info ] [MainThread]: 
[0m15:11:12.437096 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m15:11:12.438101 [info ] [MainThread]: 
[0m15:11:12.439246 [error] [MainThread]:   Runtime Error in model stg_events (models/staging/stg_events.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.UnsupportedOperationException: Server does not support endpoint: POST /v1/{prefix}/namespaces/{namespace}/views
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:46)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: java.lang.UnsupportedOperationException: Server does not support endpoint: POST /v1/{prefix}/namespaces/{namespace}/views
    	at org.apache.iceberg.rest.Endpoint.check(Endpoint.java:140)
    	at org.apache.iceberg.rest.RESTSessionCatalog$RESTViewBuilder.create(RESTSessionCatalog.java:1400)
    	at org.apache.iceberg.rest.RESTSessionCatalog$RESTViewBuilder.createOrReplace(RESTSessionCatalog.java:1448)
    	at org.apache.iceberg.spark.SparkCatalog.replaceView(SparkCatalog.java:641)
    	at org.apache.spark.sql.execution.datasources.v2.CreateV2ViewExec.replaceView(CreateV2ViewExec.scala:103)
    	at org.apache.spark.sql.execution.datasources.v2.CreateV2ViewExec.run(CreateV2ViewExec.scala:64)
    	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
    	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
    	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
    	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
    	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
    	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
    	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
    	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
    	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
    	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
    	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
    	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
    	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
    	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m15:11:12.441113 [info ] [MainThread]: 
[0m15:11:12.442503 [error] [MainThread]:   Runtime Error in model stg_users (models/staging/stg_users.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `user_email` cannot be resolved. Did you mean one of the following? [`email`, `user_id`, `user_name`, `created_at`].; line 14 pos 4;
    'CreateIcebergView SELECT
        user_id,
        user_name,
        user_email,
        created_at,
        current_timestamp() as dbt_loaded_at
    FROM bronze.raw_users
    WHERE user_id IS NOT NULL, false, true, false
    :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@766d4dfe, default_default_silver.stg_users
    +- 'Project [user_id#76, user_name#77, 'user_email, created_at#79, current_timestamp() AS dbt_loaded_at#75]
       +- Filter isnotnull(user_id#76)
          +- SubqueryAlias spark_catalog.bronze.raw_users
             +- RelationV2[user_id#76, user_name#77, email#78, created_at#79] spark_catalog.bronze.raw_users spark_catalog.bronze.raw_users
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `user_email` cannot be resolved. Did you mean one of the following? [`email`, `user_id`, `user_name`, `created_at`].; line 14 pos 4;
    'CreateIcebergView SELECT
        user_id,
        user_name,
        user_email,
        created_at,
        current_timestamp() as dbt_loaded_at
    FROM bronze.raw_users
    WHERE user_id IS NOT NULL, false, true, false
    :- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@766d4dfe, default_default_silver.stg_users
    +- 'Project [user_id#76, user_name#77, 'user_email, created_at#79, current_timestamp() AS dbt_loaded_at#75]
       +- Filter isnotnull(user_id#76)
          +- SubqueryAlias spark_catalog.bronze.raw_users
             +- RelationV2[user_id#76, user_name#77, email#78, created_at#79] spark_catalog.bronze.raw_users spark_catalog.bronze.raw_users
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m15:11:12.443774 [info ] [MainThread]: 
[0m15:11:12.444784 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=1 TOTAL=3
[0m15:11:12.446142 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 4.9311676, "process_in_blocks": "3184", "process_kernel_time": 0.275551, "process_mem_max_rss": "112564", "process_out_blocks": "120", "process_user_time": 2.301668}
[0m15:11:12.447161 [debug] [MainThread]: Command `dbt run` failed at 15:11:12.447059 after 4.93 seconds
[0m15:11:12.448015 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x769a384f3110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x769a3bcc3d50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x769a384ebfd0>]}
[0m15:11:12.448818 [debug] [MainThread]: Flushing usage events
[0m15:11:14.213974 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:13:09.367638 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d7cd22af90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d7cd6291d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d7cd22aa10>]}


============================== 15:13:09.370825 | 3f88ad71-bcc9-401e-b443-6191e649cd50 ==============================
[0m15:13:09.370825 [info ] [MainThread]: Running with dbt=1.9.0
[0m15:13:09.371826 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/usr/app/dbt/logs', 'debug': 'False', 'profiles_dir': '/usr/app/dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt run', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:13:09.418010 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m15:13:09.419182 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m15:13:09.419950 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m15:13:09.516128 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3f88ad71-bcc9-401e-b443-6191e649cd50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d7ccb54590>]}
[0m15:13:09.556731 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3f88ad71-bcc9-401e-b443-6191e649cd50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d7cd76f050>]}
[0m15:13:09.558174 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m15:13:09.619368 [debug] [MainThread]: checksum: c99e828bba267739642b5a3ce85f17518764ea526e0e6c4fdc649171c1a66bff, vars: {}, profile: , target: , version: 1.9.0
[0m15:13:09.701563 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m15:13:09.702657 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '3f88ad71-bcc9-401e-b443-6191e649cd50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d7ce0036d0>]}
[0m15:13:11.551377 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- seeds.data_pipeline_poc
- snapshots.data_pipeline_poc
[0m15:13:11.560296 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3f88ad71-bcc9-401e-b443-6191e649cd50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d7c554d250>]}
[0m15:13:11.632823 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/app/dbt/target/manifest.json
[0m15:13:11.637859 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/app/dbt/target/semantic_manifest.json
[0m15:13:11.651838 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3f88ad71-bcc9-401e-b443-6191e649cd50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d7c523b890>]}
[0m15:13:11.652734 [info ] [MainThread]: Found 3 models, 4 data tests, 2 sources, 583 macros
[0m15:13:11.653806 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3f88ad71-bcc9-401e-b443-6191e649cd50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d7c54cf8d0>]}
[0m15:13:11.655668 [info ] [MainThread]: 
[0m15:13:11.656570 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:13:11.657381 [info ] [MainThread]: 
[0m15:13:11.658566 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m15:13:11.664545 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m15:13:11.671854 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m15:13:11.672619 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m15:13:11.673301 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:13:11.727607 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:13:11.729735 [debug] [ThreadPool]: SQL status: OK in 0.056 seconds
[0m15:13:11.733763 [debug] [ThreadPool]: On list_schemas: Close
[0m15:13:11.738805 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m15:13:11.739893 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m15:13:11.740597 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:13:11.795595 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:13:11.797793 [debug] [ThreadPool]: SQL status: OK in 0.057 seconds
[0m15:13:11.802261 [debug] [ThreadPool]: On list_schemas: Close
[0m15:13:11.807394 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default_default_gold)
[0m15:13:11.812043 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:13:11.813021 [debug] [ThreadPool]: Using spark connection "list_None_default_default_gold"
[0m15:13:11.813834 [debug] [ThreadPool]: On list_None_default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show table extended in default_default_gold like '*'
  
[0m15:13:11.814610 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:13:11.849206 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#105, tableName#106, isTemporary#107, information#108]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@37c0e9f8, [default_default_gold]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#105, tableName#106, isTemporary#107, information#108]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@37c0e9f8, [default_default_gold]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m15:13:11.850390 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m15:13:11.851298 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show table extended in default_default_gold like '*'
  
[0m15:13:11.852315 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#105, tableName#106, isTemporary#107, information#108]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@37c0e9f8, [default_default_gold]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#105, tableName#106, isTemporary#107, information#108]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@37c0e9f8, [default_default_gold]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m15:13:11.853423 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m15:13:11.854448 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#105, tableName#106, isTemporary#107, information#108]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@37c0e9f8, [default_default_gold]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#105, tableName#106, isTemporary#107, information#108]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@37c0e9f8, [default_default_gold]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m15:13:11.858114 [debug] [ThreadPool]: Using spark connection "list_None_default_default_gold"
[0m15:13:11.859048 [debug] [ThreadPool]: On list_None_default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show tables in default_default_gold like '*'
  
[0m15:13:11.878202 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:13:11.879325 [debug] [ThreadPool]: SQL status: OK in 0.019 seconds
[0m15:13:11.882692 [debug] [ThreadPool]: On list_None_default_default_gold: ROLLBACK
[0m15:13:11.883455 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m15:13:11.884213 [debug] [ThreadPool]: On list_None_default_default_gold: Close
[0m15:13:11.888330 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default_default_gold, now list_None_default_default_silver)
[0m15:13:11.890663 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:13:11.891393 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m15:13:11.892364 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show table extended in default_default_silver like '*'
  
[0m15:13:11.893305 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:13:11.927425 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#120, tableName#121, isTemporary#122, information#123]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7a9597bb, [default_default_silver]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#120, tableName#121, isTemporary#122, information#123]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7a9597bb, [default_default_silver]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m15:13:11.928678 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m15:13:11.929869 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show table extended in default_default_silver like '*'
  
[0m15:13:11.930990 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#120, tableName#121, isTemporary#122, information#123]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7a9597bb, [default_default_silver]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#120, tableName#121, isTemporary#122, information#123]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7a9597bb, [default_default_silver]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m15:13:11.931935 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m15:13:11.932856 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#120, tableName#121, isTemporary#122, information#123]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7a9597bb, [default_default_silver]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#120, tableName#121, isTemporary#122, information#123]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7a9597bb, [default_default_silver]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m15:13:11.935373 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m15:13:11.936119 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show tables in default_default_silver like '*'
  
[0m15:13:11.956258 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:13:11.957253 [debug] [ThreadPool]: SQL status: OK in 0.020 seconds
[0m15:13:11.960562 [debug] [ThreadPool]: On list_None_default_default_silver: ROLLBACK
[0m15:13:11.961359 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m15:13:11.962192 [debug] [ThreadPool]: On list_None_default_default_silver: Close
[0m15:13:11.967008 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3f88ad71-bcc9-401e-b443-6191e649cd50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d7cd070610>]}
[0m15:13:11.968320 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:13:11.969326 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:13:11.971694 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.stg_events
[0m15:13:11.972582 [info ] [Thread-1 (]: 1 of 3 START sql table model default_default_silver.stg_events ................. [RUN]
[0m15:13:11.973880 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default_default_silver, now model.data_pipeline_poc.stg_events)
[0m15:13:11.974907 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_poc.stg_events
[0m15:13:11.981360 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_poc.stg_events"
[0m15:13:11.987752 [debug] [Thread-1 (]: Began executing node model.data_pipeline_poc.stg_events
[0m15:13:12.003253 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.stg_events"
[0m15:13:12.004203 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_events: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.stg_events"} */
drop table if exists default_default_silver.stg_events
[0m15:13:12.005062 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:13:12.057629 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:13:12.061775 [debug] [Thread-1 (]: SQL status: OK in 0.057 seconds
[0m15:13:12.089037 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_poc.stg_events"
[0m15:13:12.096479 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:13:12.097251 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.stg_events"
[0m15:13:12.097972 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_events: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.stg_events"} */

  
    
        create or replace table default_default_silver.stg_events
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Staging model for raw events from bronze layer
-- This model cleans and standardizes the raw event data

SELECT
    event_id,
    event_type,
    user_id,
    event_timestamp,
    event_data,
    current_timestamp() as dbt_loaded_at
FROM bronze.raw_events
WHERE event_id IS NOT NULL
  
[0m15:13:13.880570 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:13:13.881701 [debug] [Thread-1 (]: SQL status: OK in 1.783 seconds
[0m15:13:13.894908 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_events: ROLLBACK
[0m15:13:13.895965 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:13:13.896743 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_events: Close
[0m15:13:13.905038 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3f88ad71-bcc9-401e-b443-6191e649cd50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d7c52191d0>]}
[0m15:13:13.906500 [info ] [Thread-1 (]: 1 of 3 OK created sql table model default_default_silver.stg_events ............ [[32mOK[0m in 1.93s]
[0m15:13:13.907663 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.stg_events
[0m15:13:13.909346 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.stg_users
[0m15:13:13.910500 [info ] [Thread-1 (]: 2 of 3 START sql table model default_default_silver.stg_users .................. [RUN]
[0m15:13:13.911867 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline_poc.stg_events, now model.data_pipeline_poc.stg_users)
[0m15:13:13.912782 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_poc.stg_users
[0m15:13:13.916032 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_poc.stg_users"
[0m15:13:13.922261 [debug] [Thread-1 (]: Began executing node model.data_pipeline_poc.stg_users
[0m15:13:13.926365 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.stg_users"
[0m15:13:13.927330 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_users: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.stg_users"} */
drop table if exists default_default_silver.stg_users
[0m15:13:13.928129 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:13:13.982407 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:13:13.983682 [debug] [Thread-1 (]: SQL status: OK in 0.055 seconds
[0m15:13:13.986207 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_poc.stg_users"
[0m15:13:13.993647 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:13:13.994616 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.stg_users"
[0m15:13:13.995414 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_users: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.stg_users"} */

  
    
        create or replace table default_default_silver.stg_users
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Staging model for raw users from bronze layer
-- This model cleans and standardizes the raw user data

SELECT
    user_id,
    user_name,
    email as user_email,
    created_at,
    current_timestamp() as dbt_loaded_at
FROM bronze.raw_users
WHERE user_id IS NOT NULL
  
[0m15:13:14.614660 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:13:14.617821 [debug] [Thread-1 (]: SQL status: OK in 0.622 seconds
[0m15:13:14.619950 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_users: ROLLBACK
[0m15:13:14.620981 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:13:14.621927 [debug] [Thread-1 (]: On model.data_pipeline_poc.stg_users: Close
[0m15:13:14.627677 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3f88ad71-bcc9-401e-b443-6191e649cd50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d7cd786890>]}
[0m15:13:14.629053 [info ] [Thread-1 (]: 2 of 3 OK created sql table model default_default_silver.stg_users ............. [[32mOK[0m in 0.72s]
[0m15:13:14.630276 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.stg_users
[0m15:13:14.631911 [debug] [Thread-1 (]: Began running node model.data_pipeline_poc.fct_events_enriched
[0m15:13:14.633308 [info ] [Thread-1 (]: 3 of 3 START sql table model default_default_gold.fct_events_enriched .......... [RUN]
[0m15:13:14.634693 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline_poc.stg_users, now model.data_pipeline_poc.fct_events_enriched)
[0m15:13:14.635654 [debug] [Thread-1 (]: Began compiling node model.data_pipeline_poc.fct_events_enriched
[0m15:13:14.639681 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline_poc.fct_events_enriched"
[0m15:13:14.652690 [debug] [Thread-1 (]: Began executing node model.data_pipeline_poc.fct_events_enriched
[0m15:13:14.658114 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.fct_events_enriched"
[0m15:13:14.659713 [debug] [Thread-1 (]: On model.data_pipeline_poc.fct_events_enriched: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.fct_events_enriched"} */
drop table if exists default_default_gold.fct_events_enriched
[0m15:13:14.661201 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:13:14.737062 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:13:14.738724 [debug] [Thread-1 (]: SQL status: OK in 0.078 seconds
[0m15:13:14.742148 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline_poc.fct_events_enriched"
[0m15:13:14.757194 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:13:14.758591 [debug] [Thread-1 (]: Using spark connection "model.data_pipeline_poc.fct_events_enriched"
[0m15:13:14.760699 [debug] [Thread-1 (]: On model.data_pipeline_poc.fct_events_enriched: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "model.data_pipeline_poc.fct_events_enriched"} */

  
    
        create or replace table default_default_gold.fct_events_enriched
      
      
    using iceberg
      
      
      
      
      
      

      as
      

-- Mart model: Events enriched with user information
-- This model joins event data with user data for analytics

WITH events AS (
    SELECT * FROM default_default_silver.stg_events
),

users AS (
    SELECT * FROM default_default_silver.stg_users
),

enriched AS (
    SELECT
        e.event_id,
        e.event_type,
        e.event_timestamp,
        e.event_data,
        e.user_id,
        u.user_name,
        u.user_email,
        u.created_at as user_created_at,
        e.dbt_loaded_at
    FROM events e
    LEFT JOIN users u ON e.user_id = u.user_id
)

SELECT * FROM enriched
  
[0m15:13:15.731623 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:13:15.733415 [debug] [Thread-1 (]: SQL status: OK in 0.970 seconds
[0m15:13:15.735959 [debug] [Thread-1 (]: On model.data_pipeline_poc.fct_events_enriched: ROLLBACK
[0m15:13:15.737235 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:13:15.738389 [debug] [Thread-1 (]: On model.data_pipeline_poc.fct_events_enriched: Close
[0m15:13:15.743409 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3f88ad71-bcc9-401e-b443-6191e649cd50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d7c41151d0>]}
[0m15:13:15.744799 [info ] [Thread-1 (]: 3 of 3 OK created sql table model default_default_gold.fct_events_enriched ..... [[32mOK[0m in 1.11s]
[0m15:13:15.746197 [debug] [Thread-1 (]: Finished running node model.data_pipeline_poc.fct_events_enriched
[0m15:13:15.748507 [debug] [MainThread]: On master: ROLLBACK
[0m15:13:15.749708 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:13:15.784025 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m15:13:15.785225 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:13:15.786109 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:13:15.787041 [debug] [MainThread]: On master: ROLLBACK
[0m15:13:15.788073 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m15:13:15.788898 [debug] [MainThread]: On master: Close
[0m15:13:15.796595 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:13:15.797761 [debug] [MainThread]: Connection 'model.data_pipeline_poc.fct_events_enriched' was properly closed.
[0m15:13:15.798716 [info ] [MainThread]: 
[0m15:13:15.799576 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 4.14 seconds (4.14s).
[0m15:13:15.801362 [debug] [MainThread]: Command end result
[0m15:13:15.835104 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/app/dbt/target/manifest.json
[0m15:13:15.839158 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/app/dbt/target/semantic_manifest.json
[0m15:13:15.847343 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/app/dbt/target/run_results.json
[0m15:13:15.848222 [info ] [MainThread]: 
[0m15:13:15.849382 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:13:15.850372 [info ] [MainThread]: 
[0m15:13:15.851289 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
[0m15:13:15.852758 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 6.537694, "process_in_blocks": "0", "process_kernel_time": 0.324311, "process_mem_max_rss": "112376", "process_out_blocks": "0", "process_user_time": 2.525579}
[0m15:13:15.853809 [debug] [MainThread]: Command `dbt run` succeeded at 15:13:15.853698 after 6.54 seconds
[0m15:13:15.854945 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d7cd0912d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d7d0a28dd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d7d0ae6590>]}
[0m15:13:15.855840 [debug] [MainThread]: Flushing usage events
[0m15:13:23.511023 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:13:56.639209 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77105efee7d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77105f3e9410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77105f043d10>]}


============================== 15:13:56.642171 | c097a02e-7d7d-4291-894c-ae020b7173ff ==============================
[0m15:13:56.642171 [info ] [MainThread]: Running with dbt=1.9.0
[0m15:13:56.643399 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/app/dbt', 'debug': 'False', 'version_check': 'True', 'log_path': '/usr/app/dbt/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt test', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m15:13:56.685832 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m15:13:56.686720 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m15:13:56.688406 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m15:13:56.776093 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c097a02e-7d7d-4291-894c-ae020b7173ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77105e910e10>]}
[0m15:13:56.815403 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c097a02e-7d7d-4291-894c-ae020b7173ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77105f73b350>]}
[0m15:13:56.816621 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m15:13:56.870038 [debug] [MainThread]: checksum: c99e828bba267739642b5a3ce85f17518764ea526e0e6c4fdc649171c1a66bff, vars: {}, profile: , target: , version: 1.9.0
[0m15:13:57.680272 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:13:57.681058 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:13:57.686238 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- seeds.data_pipeline_poc
- snapshots.data_pipeline_poc
[0m15:13:57.714763 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c097a02e-7d7d-4291-894c-ae020b7173ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77105e8fef50>]}
[0m15:13:57.789345 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/app/dbt/target/manifest.json
[0m15:13:57.793810 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/app/dbt/target/semantic_manifest.json
[0m15:13:57.820001 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c097a02e-7d7d-4291-894c-ae020b7173ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77105e33a190>]}
[0m15:13:57.821374 [info ] [MainThread]: Found 3 models, 4 data tests, 2 sources, 583 macros
[0m15:13:57.822441 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c097a02e-7d7d-4291-894c-ae020b7173ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77105e331450>]}
[0m15:13:57.824997 [info ] [MainThread]: 
[0m15:13:57.825850 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:13:57.826624 [info ] [MainThread]: 
[0m15:13:57.827629 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m15:13:57.832679 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_default_default_silver'
[0m15:13:57.841075 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:13:57.842129 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m15:13:57.842847 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show table extended in default_default_silver like '*'
  
[0m15:13:57.843553 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:13:57.887884 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#228, tableName#229, isTemporary#230, information#231]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7dd7ff85, [default_default_silver]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#228, tableName#229, isTemporary#230, information#231]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7dd7ff85, [default_default_silver]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m15:13:57.890129 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m15:13:57.891363 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show table extended in default_default_silver like '*'
  
[0m15:13:57.892839 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#228, tableName#229, isTemporary#230, information#231]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7dd7ff85, [default_default_silver]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#228, tableName#229, isTemporary#230, information#231]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7dd7ff85, [default_default_silver]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m15:13:57.894079 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m15:13:57.894959 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#228, tableName#229, isTemporary#230, information#231]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7dd7ff85, [default_default_silver]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#228, tableName#229, isTemporary#230, information#231]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@7dd7ff85, [default_default_silver]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m15:13:57.898627 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m15:13:57.899952 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
show tables in default_default_silver like '*'
  
[0m15:13:57.958883 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:13:57.959940 [debug] [ThreadPool]: SQL status: OK in 0.059 seconds
[0m15:13:57.966143 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m15:13:57.967007 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
describe extended default_default_silver.stg_events
  
[0m15:13:58.050600 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:13:58.051616 [debug] [ThreadPool]: SQL status: OK in 0.084 seconds
[0m15:13:58.058491 [debug] [ThreadPool]: Using spark connection "list_None_default_default_silver"
[0m15:13:58.059377 [debug] [ThreadPool]: On list_None_default_default_silver: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_silver"} */
describe extended default_default_silver.stg_users
  
[0m15:13:58.091099 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:13:58.092089 [debug] [ThreadPool]: SQL status: OK in 0.032 seconds
[0m15:13:58.095744 [debug] [ThreadPool]: On list_None_default_default_silver: ROLLBACK
[0m15:13:58.096496 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m15:13:58.097381 [debug] [ThreadPool]: On list_None_default_default_silver: Close
[0m15:13:58.101286 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default_default_silver, now list_None_default_default_gold)
[0m15:13:58.103873 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:13:58.104710 [debug] [ThreadPool]: Using spark connection "list_None_default_default_gold"
[0m15:13:58.105629 [debug] [ThreadPool]: On list_None_default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show table extended in default_default_gold like '*'
  
[0m15:13:58.106317 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:13:58.134439 [debug] [ThreadPool]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#304, tableName#305, isTemporary#306, information#307]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@98c720a, [default_default_gold]\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;\nShowTableExtended *, [namespace#304, tableName#305, isTemporary#306, information#307]\n+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@98c720a, [default_default_gold]\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m15:13:58.135850 [debug] [ThreadPool]: Spark adapter: Poll status: 5
[0m15:13:58.136648 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show table extended in default_default_gold like '*'
  
[0m15:13:58.138448 [debug] [ThreadPool]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#304, tableName#305, isTemporary#306, information#307]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@98c720a, [default_default_gold]
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
  ShowTableExtended *, [namespace#304, tableName#305, isTemporary#306, information#307]
  +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@98c720a, [default_default_gold]
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m15:13:58.139407 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m15:13:58.140259 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1200] org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#304, tableName#305, isTemporary#306, information#307]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@98c720a, [default_default_gold]
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    Caused by: org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
    ShowTableExtended *, [namespace#304, tableName#305, isTemporary#306, information#307]
    +- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@98c720a, [default_default_gold]
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m15:13:58.142501 [debug] [ThreadPool]: Using spark connection "list_None_default_default_gold"
[0m15:13:58.143219 [debug] [ThreadPool]: On list_None_default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
show tables in default_default_gold like '*'
  
[0m15:13:58.168914 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:13:58.170045 [debug] [ThreadPool]: SQL status: OK in 0.026 seconds
[0m15:13:58.177003 [debug] [ThreadPool]: Using spark connection "list_None_default_default_gold"
[0m15:13:58.178005 [debug] [ThreadPool]: On list_None_default_default_gold: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "connection_name": "list_None_default_default_gold"} */
describe extended default_default_gold.fct_events_enriched
  
[0m15:13:58.212288 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:13:58.217494 [debug] [ThreadPool]: SQL status: OK in 0.039 seconds
[0m15:13:58.222391 [debug] [ThreadPool]: On list_None_default_default_gold: ROLLBACK
[0m15:13:58.223335 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m15:13:58.224054 [debug] [ThreadPool]: On list_None_default_default_gold: Close
[0m15:13:58.227631 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c097a02e-7d7d-4291-894c-ae020b7173ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77105e874290>]}
[0m15:13:58.228478 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:13:58.229183 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:13:58.231298 [debug] [Thread-1 (]: Began running node test.data_pipeline_poc.not_null_fct_events_enriched_event_id.fe5b63d652
[0m15:13:58.232048 [info ] [Thread-1 (]: 1 of 4 START test not_null_fct_events_enriched_event_id ........................ [RUN]
[0m15:13:58.233083 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default_default_gold, now test.data_pipeline_poc.not_null_fct_events_enriched_event_id.fe5b63d652)
[0m15:13:58.233903 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_poc.not_null_fct_events_enriched_event_id.fe5b63d652
[0m15:13:58.245668 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_poc.not_null_fct_events_enriched_event_id.fe5b63d652"
[0m15:13:58.255963 [debug] [Thread-1 (]: Began executing node test.data_pipeline_poc.not_null_fct_events_enriched_event_id.fe5b63d652
[0m15:13:58.265789 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_poc.not_null_fct_events_enriched_event_id.fe5b63d652"
[0m15:13:58.277416 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:13:58.278204 [debug] [Thread-1 (]: Using spark connection "test.data_pipeline_poc.not_null_fct_events_enriched_event_id.fe5b63d652"
[0m15:13:58.278961 [debug] [Thread-1 (]: On test.data_pipeline_poc.not_null_fct_events_enriched_event_id.fe5b63d652: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "test.data_pipeline_poc.not_null_fct_events_enriched_event_id.fe5b63d652"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select event_id
from default_default_gold.fct_events_enriched
where event_id is null



      
    ) dbt_internal_test
[0m15:13:58.279638 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:13:58.552539 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:13:58.554706 [debug] [Thread-1 (]: SQL status: OK in 0.275 seconds
[0m15:13:58.563228 [debug] [Thread-1 (]: On test.data_pipeline_poc.not_null_fct_events_enriched_event_id.fe5b63d652: ROLLBACK
[0m15:13:58.564505 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:13:58.565485 [debug] [Thread-1 (]: On test.data_pipeline_poc.not_null_fct_events_enriched_event_id.fe5b63d652: Close
[0m15:13:58.569552 [info ] [Thread-1 (]: 1 of 4 PASS not_null_fct_events_enriched_event_id .............................. [[32mPASS[0m in 0.34s]
[0m15:13:58.571848 [debug] [Thread-1 (]: Finished running node test.data_pipeline_poc.not_null_fct_events_enriched_event_id.fe5b63d652
[0m15:13:58.572951 [debug] [Thread-1 (]: Began running node test.data_pipeline_poc.source_not_null_bronze_raw_events_event_id.54914cc225
[0m15:13:58.574057 [info ] [Thread-1 (]: 2 of 4 START test source_not_null_bronze_raw_events_event_id ................... [RUN]
[0m15:13:58.575263 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_poc.not_null_fct_events_enriched_event_id.fe5b63d652, now test.data_pipeline_poc.source_not_null_bronze_raw_events_event_id.54914cc225)
[0m15:13:58.576129 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_poc.source_not_null_bronze_raw_events_event_id.54914cc225
[0m15:13:58.580198 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_poc.source_not_null_bronze_raw_events_event_id.54914cc225"
[0m15:13:58.593849 [debug] [Thread-1 (]: Began executing node test.data_pipeline_poc.source_not_null_bronze_raw_events_event_id.54914cc225
[0m15:13:58.596471 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_poc.source_not_null_bronze_raw_events_event_id.54914cc225"
[0m15:13:58.609648 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:13:58.610554 [debug] [Thread-1 (]: Using spark connection "test.data_pipeline_poc.source_not_null_bronze_raw_events_event_id.54914cc225"
[0m15:13:58.611350 [debug] [Thread-1 (]: On test.data_pipeline_poc.source_not_null_bronze_raw_events_event_id.54914cc225: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "test.data_pipeline_poc.source_not_null_bronze_raw_events_event_id.54914cc225"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select event_id
from bronze.raw_events
where event_id is null



      
    ) dbt_internal_test
[0m15:13:58.612149 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:13:58.764083 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:13:58.765642 [debug] [Thread-1 (]: SQL status: OK in 0.153 seconds
[0m15:13:58.770673 [debug] [Thread-1 (]: On test.data_pipeline_poc.source_not_null_bronze_raw_events_event_id.54914cc225: ROLLBACK
[0m15:13:58.772051 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:13:58.773119 [debug] [Thread-1 (]: On test.data_pipeline_poc.source_not_null_bronze_raw_events_event_id.54914cc225: Close
[0m15:13:58.777379 [info ] [Thread-1 (]: 2 of 4 PASS source_not_null_bronze_raw_events_event_id ......................... [[32mPASS[0m in 0.20s]
[0m15:13:58.779158 [debug] [Thread-1 (]: Finished running node test.data_pipeline_poc.source_not_null_bronze_raw_events_event_id.54914cc225
[0m15:13:58.780196 [debug] [Thread-1 (]: Began running node test.data_pipeline_poc.source_not_null_bronze_raw_users_user_id.916ac5699f
[0m15:13:58.781230 [info ] [Thread-1 (]: 3 of 4 START test source_not_null_bronze_raw_users_user_id ..................... [RUN]
[0m15:13:58.782235 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_poc.source_not_null_bronze_raw_events_event_id.54914cc225, now test.data_pipeline_poc.source_not_null_bronze_raw_users_user_id.916ac5699f)
[0m15:13:58.783109 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_poc.source_not_null_bronze_raw_users_user_id.916ac5699f
[0m15:13:58.787084 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_poc.source_not_null_bronze_raw_users_user_id.916ac5699f"
[0m15:13:58.796378 [debug] [Thread-1 (]: Began executing node test.data_pipeline_poc.source_not_null_bronze_raw_users_user_id.916ac5699f
[0m15:13:58.799099 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_poc.source_not_null_bronze_raw_users_user_id.916ac5699f"
[0m15:13:58.807245 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:13:58.808212 [debug] [Thread-1 (]: Using spark connection "test.data_pipeline_poc.source_not_null_bronze_raw_users_user_id.916ac5699f"
[0m15:13:58.809036 [debug] [Thread-1 (]: On test.data_pipeline_poc.source_not_null_bronze_raw_users_user_id.916ac5699f: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "test.data_pipeline_poc.source_not_null_bronze_raw_users_user_id.916ac5699f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select user_id
from bronze.raw_users
where user_id is null



      
    ) dbt_internal_test
[0m15:13:58.809836 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:13:58.992824 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:13:58.994442 [debug] [Thread-1 (]: SQL status: OK in 0.185 seconds
[0m15:13:58.999268 [debug] [Thread-1 (]: On test.data_pipeline_poc.source_not_null_bronze_raw_users_user_id.916ac5699f: ROLLBACK
[0m15:13:59.000239 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:13:59.001045 [debug] [Thread-1 (]: On test.data_pipeline_poc.source_not_null_bronze_raw_users_user_id.916ac5699f: Close
[0m15:13:59.004934 [info ] [Thread-1 (]: 3 of 4 PASS source_not_null_bronze_raw_users_user_id ........................... [[32mPASS[0m in 0.22s]
[0m15:13:59.006614 [debug] [Thread-1 (]: Finished running node test.data_pipeline_poc.source_not_null_bronze_raw_users_user_id.916ac5699f
[0m15:13:59.007715 [debug] [Thread-1 (]: Began running node test.data_pipeline_poc.source_unique_bronze_raw_users_user_id.968e729bf0
[0m15:13:59.008766 [info ] [Thread-1 (]: 4 of 4 START test source_unique_bronze_raw_users_user_id ....................... [RUN]
[0m15:13:59.009995 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline_poc.source_not_null_bronze_raw_users_user_id.916ac5699f, now test.data_pipeline_poc.source_unique_bronze_raw_users_user_id.968e729bf0)
[0m15:13:59.010876 [debug] [Thread-1 (]: Began compiling node test.data_pipeline_poc.source_unique_bronze_raw_users_user_id.968e729bf0
[0m15:13:59.016232 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline_poc.source_unique_bronze_raw_users_user_id.968e729bf0"
[0m15:13:59.025665 [debug] [Thread-1 (]: Began executing node test.data_pipeline_poc.source_unique_bronze_raw_users_user_id.968e729bf0
[0m15:13:59.028253 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline_poc.source_unique_bronze_raw_users_user_id.968e729bf0"
[0m15:13:59.036029 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:13:59.036956 [debug] [Thread-1 (]: Using spark connection "test.data_pipeline_poc.source_unique_bronze_raw_users_user_id.968e729bf0"
[0m15:13:59.037791 [debug] [Thread-1 (]: On test.data_pipeline_poc.source_unique_bronze_raw_users_user_id.968e729bf0: /* {"app": "dbt", "dbt_version": "1.9.0", "profile_name": "data_pipeline_poc", "target_name": "dev", "node_id": "test.data_pipeline_poc.source_unique_bronze_raw_users_user_id.968e729bf0"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    user_id as unique_field,
    count(*) as n_records

from bronze.raw_users
where user_id is not null
group by user_id
having count(*) > 1



      
    ) dbt_internal_test
[0m15:13:59.038557 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:13:59.657120 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:13:59.659752 [debug] [Thread-1 (]: SQL status: OK in 0.621 seconds
[0m15:13:59.664329 [debug] [Thread-1 (]: On test.data_pipeline_poc.source_unique_bronze_raw_users_user_id.968e729bf0: ROLLBACK
[0m15:13:59.665481 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:13:59.666311 [debug] [Thread-1 (]: On test.data_pipeline_poc.source_unique_bronze_raw_users_user_id.968e729bf0: Close
[0m15:13:59.669755 [error] [Thread-1 (]: 4 of 4 FAIL 3 source_unique_bronze_raw_users_user_id ........................... [[31mFAIL 3[0m in 0.66s]
[0m15:13:59.671249 [debug] [Thread-1 (]: Finished running node test.data_pipeline_poc.source_unique_bronze_raw_users_user_id.968e729bf0
[0m15:13:59.673233 [debug] [MainThread]: On master: ROLLBACK
[0m15:13:59.674170 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:13:59.702145 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m15:13:59.703410 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:13:59.704488 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:13:59.705511 [debug] [MainThread]: On master: ROLLBACK
[0m15:13:59.706449 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m15:13:59.707332 [debug] [MainThread]: On master: Close
[0m15:13:59.710304 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:13:59.711088 [debug] [MainThread]: Connection 'test.data_pipeline_poc.source_unique_bronze_raw_users_user_id.968e729bf0' was properly closed.
[0m15:13:59.711963 [info ] [MainThread]: 
[0m15:13:59.712905 [info ] [MainThread]: Finished running 4 data tests in 0 hours 0 minutes and 1.88 seconds (1.88s).
[0m15:13:59.714406 [debug] [MainThread]: Command end result
[0m15:13:59.749453 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/app/dbt/target/manifest.json
[0m15:13:59.754031 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/app/dbt/target/semantic_manifest.json
[0m15:13:59.764988 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/app/dbt/target/run_results.json
[0m15:13:59.766002 [info ] [MainThread]: 
[0m15:13:59.767246 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m15:13:59.768602 [info ] [MainThread]: 
[0m15:13:59.769889 [error] [MainThread]: [31mFailure in test source_unique_bronze_raw_users_user_id (models/staging/sources.yml)[0m
[0m15:13:59.771564 [error] [MainThread]:   Got 3 results, configured to fail if != 0
[0m15:13:59.772800 [info ] [MainThread]: 
[0m15:13:59.774053 [info ] [MainThread]:   compiled code at target/compiled/data_pipeline_poc/models/staging/sources.yml/source_unique_bronze_raw_users_user_id.sql
[0m15:13:59.775421 [info ] [MainThread]: 
[0m15:13:59.776412 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=1 SKIP=0 TOTAL=4
[0m15:13:59.777900 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 3.1906054, "process_in_blocks": "2064", "process_kernel_time": 0.208922, "process_mem_max_rss": "108484", "process_out_blocks": "0", "process_user_time": 1.43835}
[0m15:13:59.778902 [debug] [MainThread]: Command `dbt test` failed at 15:13:59.778791 after 3.19 seconds
[0m15:13:59.779755 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7710629a9390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77106284ce10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77106284cd90>]}
[0m15:13:59.780622 [debug] [MainThread]: Flushing usage events
[0m15:14:00.577576 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:32:34.191729 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73c68242d2d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73c68242da10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73c68242cf50>]}


============================== 15:32:34.194934 | 73dce678-6efd-40d6-86e8-bf9c3f4902a2 ==============================
[0m15:32:34.194934 [info ] [MainThread]: Running with dbt=1.9.0
[0m15:32:34.195998 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/app/dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/usr/app/dbt/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt debug', 'send_anonymous_usage_stats': 'True'}
[0m15:32:34.205950 [info ] [MainThread]: dbt version: 1.9.0
[0m15:32:34.207075 [info ] [MainThread]: python version: 3.11.2
[0m15:32:34.208392 [info ] [MainThread]: python path: /usr/local/bin/python
[0m15:32:34.209514 [info ] [MainThread]: os info: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.31
[0m15:32:34.261347 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m15:32:34.262713 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m15:32:34.263630 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m15:32:34.288274 [info ] [MainThread]: Using profiles dir at /usr/app/dbt
[0m15:32:34.289987 [info ] [MainThread]: Using profiles.yml file at /usr/app/dbt/profiles.yml
[0m15:32:34.291335 [info ] [MainThread]: Using dbt_project.yml file at /usr/app/dbt/dbt_project.yml
[0m15:32:34.292380 [info ] [MainThread]: adapter type: spark
[0m15:32:34.293255 [info ] [MainThread]: adapter version: 1.9.0
[0m15:32:34.367266 [info ] [MainThread]: Configuration:
[0m15:32:34.368845 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m15:32:34.370452 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m15:32:34.371407 [info ] [MainThread]: Required dependencies:
[0m15:32:34.372583 [debug] [MainThread]: Executing "git --help"
[0m15:32:34.375682 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone             Clone a repository into a new directory\n   init              Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add               Add file contents to the index\n   mv                Move or rename a file, a directory, or a symlink\n   restore           Restore working tree files\n   rm                Remove files from the working tree and from the index\n   sparse-checkout   Initialize and modify the sparse-checkout\n\nexamine the history and state (see also: git help revisions)\n   bisect            Use binary search to find the commit that introduced a bug\n   diff              Show changes between commits, commit and working tree, etc\n   grep              Print lines matching a pattern\n   log               Show commit logs\n   show              Show various types of objects\n   status            Show the working tree status\n\ngrow, mark and tweak your common history\n   branch            List, create, or delete branches\n   commit            Record changes to the repository\n   merge             Join two or more development histories together\n   rebase            Reapply commits on top of another base tip\n   reset             Reset current HEAD to the specified state\n   switch            Switch branches\n   tag               Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch             Download objects and refs from another repository\n   pull              Fetch from and integrate with another repository or a local branch\n   push              Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m15:32:34.377165 [debug] [MainThread]: STDERR: "b''"
[0m15:32:34.378098 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m15:32:34.379045 [info ] [MainThread]: Connection:
[0m15:32:34.380328 [info ] [MainThread]:   host: spark-iceberg
[0m15:32:34.381521 [info ] [MainThread]:   port: 10000
[0m15:32:34.382431 [info ] [MainThread]:   cluster: None
[0m15:32:34.383350 [info ] [MainThread]:   endpoint: None
[0m15:32:34.384293 [info ] [MainThread]:   schema: default
[0m15:32:34.385220 [info ] [MainThread]:   organization: 0
[0m15:32:34.386631 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m15:32:34.443672 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m15:32:34.444854 [debug] [MainThread]: Using spark connection "debug"
[0m15:32:34.445818 [debug] [MainThread]: On debug: select 1 as id
[0m15:32:34.446696 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:32:34.519346 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m15:32:34.520814 [debug] [MainThread]: SQL status: OK in 0.074 seconds
[0m15:32:34.522377 [debug] [MainThread]: On debug: Close
[0m15:32:34.526487 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m15:32:34.527794 [info ] [MainThread]: [32mAll checks passed![0m
[0m15:32:34.529445 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 0.39035648, "process_in_blocks": "0", "process_kernel_time": 0.156163, "process_mem_max_rss": "99544", "process_out_blocks": "0", "process_user_time": 1.089138}
[0m15:32:34.530519 [debug] [MainThread]: Command `dbt debug` succeeded at 15:32:34.530403 after 0.39 seconds
[0m15:32:34.531347 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m15:32:34.532180 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73c685eed750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73c681e58ed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73c685ce2710>]}
[0m15:32:34.532991 [debug] [MainThread]: Flushing usage events
[0m15:32:35.513903 [debug] [MainThread]: An error was encountered while trying to flush usage events
