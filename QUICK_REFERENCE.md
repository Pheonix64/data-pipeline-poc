# QUICK REFERENCE - Guide de r√©f√©rence rapide

Guide de r√©f√©rence rapide pour les commandes courantes du projet Data Pipeline POC BCEAO.

## üöÄ D√©marrage rapide

```bash
# D√©marrer tous les services
docker-compose up -d

# V√©rifier l'√©tat des services
docker-compose ps

# Voir les logs en temps r√©el
docker-compose logs -f

# Arr√™ter tous les services
docker-compose down
```

## üìä dbt - Transformations de donn√©es

### Commandes de base

```bash
# Ex√©cuter tous les mod√®les
docker exec dbt bash -c "cd /usr/app/dbt && dbt run"

# Ex√©cuter un mod√®le sp√©cifique
docker exec dbt bash -c "cd /usr/app/dbt && dbt run --select mon_modele"

# Ex√©cuter plusieurs mod√®les par tag
docker exec dbt bash -c "cd /usr/app/dbt && dbt run --select tag:gold"

# Ex√©cuter un mod√®le et ses d√©pendances en amont
docker exec dbt bash -c "cd /usr/app/dbt && dbt run --select +mon_modele"

# Ex√©cuter un mod√®le et ses d√©pendances en aval
docker exec dbt bash -c "cd /usr/app/dbt && dbt run --select mon_modele+"

# Mode full refresh (recharger toutes les donn√©es)
docker exec dbt bash -c "cd /usr/app/dbt && dbt run --full-refresh"
```

### Tests

```bash
# Ex√©cuter tous les tests
docker exec dbt bash -c "cd /usr/app/dbt && dbt test"

# Tester un mod√®le sp√©cifique
docker exec dbt bash -c "cd /usr/app/dbt && dbt test --select mon_modele"

# Tester uniquement les sources
docker exec dbt bash -c "cd /usr/app/dbt && dbt test --select source:*"
```

### Documentation

```bash
# G√©n√©rer la documentation
docker exec dbt bash -c "cd /usr/app/dbt && dbt docs generate"

# Servir la documentation (pas disponible dans conteneur)
# Alternative: Copier catalog.json et index.html localement
```

### Debug

```bash
# V√©rifier la connexion
docker exec dbt bash -c "cd /usr/app/dbt && dbt debug"

# Compiler sans ex√©cuter (voir le SQL g√©n√©r√©)
docker exec dbt bash -c "cd /usr/app/dbt && dbt compile --select mon_modele"

# Voir le SQL compil√©
docker exec dbt bash -c "cat /usr/app/dbt/target/compiled/dbt_project/models/chemin/mon_modele.sql"

# Logs dbt
docker exec dbt bash -c "cat /usr/app/dbt/logs/dbt.log | tail -n 50"
```

## ‚ö° Spark - Traitement de donn√©es

### Spark Submit

```bash
# Ex√©cuter un script PySpark
docker exec spark-iceberg bash -lc "cd /opt/spark && ./bin/spark-submit \
  --jars /opt/spark/extra-jars/hadoop-aws-3.3.4.jar,/opt/spark/extra-jars/aws-java-sdk-bundle-1.12.262.jar \
  /tmp/mon_script.py"

# Avec arguments
docker exec spark-iceberg bash -lc "cd /opt/spark && ./bin/spark-submit \
  --jars /opt/spark/extra-jars/hadoop-aws-3.3.4.jar,/opt/spark/extra-jars/aws-java-sdk-bundle-1.12.262.jar \
  /tmp/mon_script.py arg1 arg2"

# Avec configuration custom
docker exec spark-iceberg bash -lc "cd /opt/spark && ./bin/spark-submit \
  --jars /opt/spark/extra-jars/hadoop-aws-3.3.4.jar,/opt/spark/extra-jars/aws-java-sdk-bundle-1.12.262.jar \
  --conf spark.driver.memory=4g \
  --conf spark.executor.memory=8g \
  /tmp/mon_script.py"
```

### PySpark Shell

```bash
# Lancer PySpark interactif
docker exec -it spark-iceberg /opt/spark/bin/pyspark

# Commandes dans PySpark
# >>> df = spark.read.parquet("s3a://lakehouse/bronze/...")
# >>> df.show()
# >>> df.printSchema()
# >>> df.count()
```

### Beeline (SQL CLI)

```bash
# Connexion Beeline
docker exec -it spark-iceberg beeline -u jdbc:hive2://localhost:10000

# Requ√™te en une ligne
docker exec spark-iceberg beeline -u jdbc:hive2://localhost:10000 \
  -e "SELECT COUNT(*) FROM bronze.indicateurs_economiques_uemoa;"

# Plusieurs requ√™tes
docker exec spark-iceberg beeline -u jdbc:hive2://localhost:10000 \
  -e "SHOW DATABASES; SHOW TABLES IN bronze;"

# Requ√™te depuis un fichier
docker exec spark-iceberg beeline -u jdbc:hive2://localhost:10000 \
  -f /tmp/ma_requete.sql
```

### Spark UI

```bash
# Acc√©der √† Spark UI
# Navigateur: http://localhost:4040

# Jobs actifs
# http://localhost:4040/jobs/

# Stages et DAG
# http://localhost:4040/stages/

# Storage (tables cached)
# http://localhost:4040/storage/

# Environment (configuration)
# http://localhost:4040/environment/
```

## üóÑÔ∏è MinIO - Stockage S3

### mc (MinIO Client)

```bash
# Lister les buckets
docker exec mc mc ls bceao-data/

# Lister le contenu d'un bucket
docker exec mc mc ls bceao-data/lakehouse/

# Lister r√©cursivement
docker exec mc mc ls bceao-data/lakehouse/bronze/ --recursive

# Obtenir des infos sur un objet
docker exec mc mc stat bceao-data/lakehouse/bronze/indicateurs_economiques_uemoa/

# T√©l√©charger un fichier
docker exec mc mc cp bceao-data/lakehouse/bronze/mon_fichier.parquet /tmp/

# Uploader un fichier
docker exec mc mc cp /tmp/mon_fichier.parquet bceao-data/lakehouse/bronze/

# Synchroniser un r√©pertoire (backup)
docker exec mc mc mirror bceao-data/lakehouse /backup/lakehouse

# Obtenir des stats serveur
docker exec mc mc admin info bceao-data
```

### MinIO Console

```bash
# Acc√©der √† l'interface web
# Navigateur: http://localhost:9001
# Login: admin
# Password: SuperSecret123

# Features web:
# - Browser: Naviguer dans les buckets
# - Monitoring: M√©triques en temps r√©el
# - Identity: Gestion des utilisateurs et policies
# - Settings: Configuration serveur
```

## üßä Iceberg - Tables ACID

### Catalog Operations

```sql
-- Lister tous les namespaces
SHOW NAMESPACES;

-- Cr√©er un namespace
CREATE NAMESPACE IF NOT EXISTS mon_namespace;

-- Lister les tables d'un namespace
SHOW TABLES IN bronze;

-- Obtenir le sch√©ma d'une table
DESCRIBE bronze.indicateurs_economiques_uemoa;

-- Obtenir les d√©tails √©tendus
DESCRIBE EXTENDED bronze.indicateurs_economiques_uemoa;
```

### Table Operations

```sql
-- Cr√©er une table Iceberg
CREATE TABLE mon_namespace.ma_table (
    id BIGINT,
    nom STRING,
    valeur DECIMAL(18,2)
) USING iceberg
PARTITIONED BY (annee);

-- Ins√©rer des donn√©es
INSERT INTO mon_namespace.ma_table VALUES (1, 'test', 100.50);

-- Cr√©er depuis SELECT
CREATE TABLE mon_namespace.ma_nouvelle_table
USING iceberg
AS SELECT * FROM mon_namespace.ma_table;

-- Cr√©er ou remplacer
CREATE OR REPLACE TABLE mon_namespace.ma_table
USING iceberg
AS SELECT * FROM source_table;
```

### Time Travel

```sql
-- Voir l'historique des snapshots
SELECT * FROM mon_namespace.ma_table.snapshots;

-- Requ√™te √† un timestamp sp√©cifique
SELECT * FROM mon_namespace.ma_table
TIMESTAMP AS OF '2024-01-15 10:30:00';

-- Requ√™te √† un snapshot ID sp√©cifique
SELECT * FROM mon_namespace.ma_table
VERSION AS OF 8901234567890123456;

-- Rollback √† un snapshot pr√©c√©dent
CALL local.system.rollback_to_snapshot(
    'mon_namespace.ma_table',
    8901234567890123456
);
```

### Maintenance

```sql
-- Expirer les vieux snapshots (lib√©rer espace)
CALL local.system.expire_snapshots(
    table => 'mon_namespace.ma_table',
    older_than => TIMESTAMP '2024-01-01 00:00:00',
    retain_last => 100
);

-- Compacter les petits fichiers
CALL local.system.rewrite_data_files(
    table => 'mon_namespace.ma_table',
    strategy => 'binpack',
    options => map('target-file-size-bytes', '536870912')
);

-- Supprimer les fichiers orphelins
CALL local.system.remove_orphan_files(
    table => 'mon_namespace.ma_table',
    older_than => TIMESTAMP '2024-01-01 00:00:00'
);
```

## üê≥ Docker - Gestion des conteneurs

### Conteneurs

```bash
# Lister tous les conteneurs
docker ps -a

# Voir les logs d'un conteneur
docker logs spark-iceberg

# Logs en temps r√©el
docker logs -f spark-iceberg

# Logs des 100 derni√®res lignes
docker logs --tail=100 spark-iceberg

# Entrer dans un conteneur
docker exec -it spark-iceberg bash

# Red√©marrer un conteneur
docker restart spark-iceberg

# Arr√™ter un conteneur
docker stop spark-iceberg

# D√©marrer un conteneur
docker start spark-iceberg
```

### Images

```bash
# Lister les images
docker images

# Reconstruire une image
docker-compose build spark-iceberg

# Reconstruire sans cache
docker-compose build --no-cache spark-iceberg

# Supprimer une image
docker rmi data-pipeline-poc_spark-iceberg
```

### Volumes

```bash
# Lister les volumes
docker volume ls

# Inspecter un volume
docker volume inspect data-pipeline-poc_minio_data

# Supprimer un volume (attention: perte de donn√©es!)
docker volume rm data-pipeline-poc_minio_data

# Supprimer tous les volumes non utilis√©s
docker volume prune
```

### Nettoyage

```bash
# Supprimer conteneurs arr√™t√©s
docker container prune

# Supprimer images non utilis√©es
docker image prune

# Supprimer volumes non utilis√©s
docker volume prune

# Nettoyage complet (ATTENTION: supprime tout!)
docker system prune -a --volumes
```

## üì¶ Copier des fichiers

### Vers conteneur

```bash
# Copier un fichier vers conteneur
docker cp mon_fichier.py spark-iceberg:/tmp/

# Copier un r√©pertoire
docker cp mon_dossier/ spark-iceberg:/tmp/

# Copier vers dbt
docker cp mon_modele.sql dbt:/usr/app/dbt/models/
```

### Depuis conteneur

```bash
# Copier depuis conteneur vers local
docker cp spark-iceberg:/tmp/resultat.csv ./

# Copier logs dbt
docker cp dbt:/usr/app/dbt/logs/dbt.log ./logs/

# Copier catalog dbt
docker cp dbt:/usr/app/dbt/target/catalog.json ./docs/
```

## üîç Diagnostic

### V√©rifier la sant√© du syst√®me

```bash
# V√©rifier tous les services
docker-compose ps

# V√©rifier l'utilisation des ressources
docker stats

# V√©rifier l'espace disque Docker
docker system df

# Tester la connexion MinIO
docker exec mc mc admin info bceao-data

# Tester la connexion Spark
docker exec spark-iceberg bash -lc "cd /opt/spark && ./bin/spark-submit --version"

# Tester la connexion dbt
docker exec dbt bash -c "cd /usr/app/dbt && dbt debug"
```

### Probl√®mes courants

```bash
# Port d√©j√† utilis√©
# Solution: Identifier et tuer le processus
netstat -ano | findstr :9000  # Windows
lsof -i :9000                 # Linux/Mac
# Puis kill le processus ou changer le port dans docker-compose.yml

# Conteneur ne d√©marre pas
docker logs nom_conteneur
# V√©rifier les logs pour identifier l'erreur

# Probl√®me de connexion r√©seau
docker network ls
docker network inspect data-pipeline-poc_default

# Probl√®me de permissions
# Windows: V√©rifier que Docker Desktop a acc√®s aux dossiers partag√©s
# Linux: sudo chown -R $USER:$USER ./
```

## üìä Monitoring

### M√©triques Spark

```bash
# Voir les applications actives
docker exec spark-iceberg bash -lc "ls /opt/spark/work"

# Voir la configuration Spark
docker exec spark-iceberg beeline -u jdbc:hive2://localhost:10000 \
  -e "SET -v;" | grep spark
```

### M√©triques MinIO

```bash
# Taille totale des buckets
docker exec mc mc du bceao-data/lakehouse

# Nombre d'objets
docker exec mc mc ls bceao-data/lakehouse --recursive | wc -l
```

### M√©triques dbt

```bash
# Derni√®re ex√©cution
docker exec dbt bash -c "cat /usr/app/dbt/logs/dbt.log | grep 'Completed successfully'"

# Temps d'ex√©cution par mod√®le
docker exec dbt bash -c "cat /usr/app/dbt/target/run_results.json" | jq '.results[] | {name: .unique_id, time: .execution_time}'
```

## üîß Configuration

### Fichiers de configuration importants

```bash
# Spark configuration
./spark-defaults.conf

# dbt profiles
./dbt_project/profiles.yml

# dbt project
./dbt_project/dbt_project.yml

# Docker services
./docker-compose.yml

# Environnement
./.env
```

### √âditer et recharger configuration

```bash
# √âditer la config Spark
vim spark-defaults.conf

# Reconstruire et red√©marrer
docker-compose build spark-iceberg
docker-compose restart spark-iceberg

# √âditer les profiles dbt
vim dbt_project/profiles.yml

# Pas besoin de rebuild, juste reload
docker exec dbt bash -c "cd /usr/app/dbt && dbt debug"
```

## üìö Ressources

### Documentation

- README.md: Vue d'ensemble du projet
- ARCHITECTURE.md: Architecture technique d√©taill√©e
- DEPLOYMENT_GUIDE.md: Guide de d√©ploiement complet
- CONTRIBUTING.md: Guide de contribution
- CHANGELOG.md: Historique des versions

### URLs utiles

```
MinIO Console:      http://localhost:9001
Jupyter Notebook:   http://localhost:8888
Spark UI:          http://localhost:4040
Iceberg REST:      http://localhost:8181
```

### Credentials par d√©faut

```
MinIO:
  - User: admin
  - Password: SuperSecret123

PostgreSQL/TimescaleDB:
  - Database: monetary_policy_dm
  - User: postgres
  - Password: PostgresPass123
```

## üÜò Aide

### Obtenir de l'aide

```bash
# Aide dbt
docker exec dbt bash -c "dbt --help"
docker exec dbt bash -c "dbt run --help"

# Aide Spark
docker exec spark-iceberg /opt/spark/bin/spark-submit --help

# Aide mc (MinIO)
docker exec mc mc --help
docker exec mc mc ls --help
```

### Communaut√©

- GitHub Issues: Signaler des bugs
- GitHub Discussions: Poser des questions
- Email: data-engineering@bceao.int

---

**Astuce**: Ajoutez cette page √† vos favoris pour un acc√®s rapide ! üîñ
