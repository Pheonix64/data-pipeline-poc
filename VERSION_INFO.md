# Informations de Version - Data Pipeline POC BCEAO

**Version**: 1.0.0  
**Date**: 27 octobre 2025  
**Statut**: Production Ready

---

## ğŸ“‹ Vue d'Ensemble

Ce projet implÃ©mente une architecture **Data Lakehouse** moderne utilisant le pattern **MÃ©daillon** (Bronze, Silver, Gold) avec Apache Iceberg comme format de table.

---

## ğŸ› ï¸ Stack Technique

### Technologies Principales

| Composant | Version | RÃ´le |
|-----------|---------|------|
| **Apache Iceberg** | 1.4.x | Format de table ouvert avec support ACID |
| **Apache Spark** | 3.5.x | Moteur de traitement distribuÃ© |
| **dbt-spark** | 1.9.0 | Orchestration des transformations SQL |
| **MinIO** | Latest | Stockage objet compatible S3 |
| **TimescaleDB** | pg15 | Base de donnÃ©es time-series |
| **ChromaDB** | Latest | Base de donnÃ©es vectorielle |
| **Iceberg REST Catalog** | Latest | Gestion des mÃ©tadonnÃ©es Iceberg |

### Images Docker

```yaml
# Images utilisÃ©es
- minio/minio:latest
- minio/mc:latest
- tabulario/iceberg-rest:latest
- timescale/timescaledb-ha:pg15-latest
- chromadb/chroma:latest
- ghcr.io/dbt-labs/dbt-spark:latest
- Custom: data-pipeline-poc/spark-iceberg:latest (basÃ©e sur Apache Spark 3.5)
```

---

## ğŸ“Š Architecture des DonnÃ©es

### Namespaces Iceberg

```
âœ… bronze                  â†’ DonnÃ©es brutes (raw_events, raw_users)
âœ… default_silver          â†’ DonnÃ©es nettoyÃ©es (stg_events, stg_users)
âœ… default_gold            â†’ DonnÃ©es analytiques (fct_events_enriched)
```

### Tables Principales

#### Bronze Layer (DonnÃ©es Brutes)
```sql
bronze.raw_events (
  event_id INT,
  event_type STRING,
  user_id INT,
  event_timestamp TIMESTAMP,
  event_data STRING
)

bronze.raw_users (
  user_id INT,
  user_name STRING,
  email STRING,
  created_at TIMESTAMP
)
```

#### Silver Layer (DonnÃ©es NettoyÃ©es)
```sql
default_silver.stg_events (
  event_id INT NOT NULL,
  event_type STRING,
  user_id INT,
  event_timestamp TIMESTAMP,
  event_data STRING,
  dbt_loaded_at TIMESTAMP
)

default_silver.stg_users (
  user_id INT NOT NULL,
  user_name STRING,
  user_email STRING,
  created_at TIMESTAMP,
  dbt_loaded_at TIMESTAMP
)
```

#### Gold Layer (DonnÃ©es Analytiques)
```sql
default_gold.fct_events_enriched (
  event_id INT NOT NULL,
  event_type STRING,
  event_timestamp TIMESTAMP,
  event_data STRING,
  user_id INT,
  user_name STRING,
  user_email STRING,
  user_created_at TIMESTAMP,
  dbt_loaded_at TIMESTAMP
)
```

---

## ğŸ”§ Configuration

### Ports ExposÃ©s

| Service | Port(s) | Description |
|---------|---------|-------------|
| MinIO Console | 9001 | Interface web MinIO |
| MinIO API | 9000 | API S3 MinIO |
| Spark UI | 4040 | Interface Spark Thrift Server |
| Jupyter Notebook | 8888 | Notebooks interactifs |
| Spark Thrift Server | 10000 | JDBC/ODBC endpoint |
| Iceberg REST | 8181 | Catalogue Iceberg REST |
| TimescaleDB | 5433 | PostgreSQL/TimescaleDB |
| ChromaDB | 8010 | API ChromaDB |

### Variables d'Environnement

```env
# MinIO
MINIO_ROOT_USER=admin
MINIO_ROOT_PASSWORD=password123

# TimescaleDB
POSTGRES_DB=datamart
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres123
```

### Configuration Spark (spark-defaults.conf)

```properties
# Iceberg Configuration
spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.spark_catalog.type=rest
spark.sql.catalog.spark_catalog.uri=http://rest:8181
spark.sql.catalog.spark_catalog.io-impl=org.apache.iceberg.aws.s3.S3FileIO
spark.sql.catalog.spark_catalog.warehouse=s3://lakehouse/
spark.sql.catalog.spark_catalog.s3.endpoint=http://minio:9000
spark.sql.catalog.spark_catalog.s3.path-style-access=true

# S3 Configuration
spark.hadoop.fs.s3a.endpoint=http://minio:9000
spark.hadoop.fs.s3a.path.style.access=true
spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.connection.ssl.enabled=false
```

### Configuration dbt (dbt_project.yml)

```yaml
name: 'data_pipeline_poc'
version: '1.0.0'
config-version: 2

models:
  data_pipeline_poc:
    +materialized: table
    +file_format: iceberg
    
    staging:
      +schema: default_silver
      
    marts:
      +schema: default_gold
```

### Configuration dbt Profiles (profiles.yml)

```yaml
data_pipeline_poc:
  target: dev
  outputs:
    dev:
      type: spark
      method: thrift
      host: spark-iceberg
      port: 10000
      user: root
      schema: default
      connect_retries: 3
      connect_timeout: 10
```

---

## ğŸ“ Structure du Projet

```
data-pipeline-poc/
â”œâ”€â”€ docker-compose.yml              # Orchestration des services
â”œâ”€â”€ spark.Dockerfile                # Image Spark personnalisÃ©e
â”œâ”€â”€ spark-defaults.conf             # Configuration Spark
â”œâ”€â”€ .env                            # Variables d'environnement
â”‚
â”œâ”€â”€ init-scripts/
â”‚   â””â”€â”€ init-lakehouse.sh          # Script d'initialisation auto
â”‚
â”œâ”€â”€ dbt_project/
â”‚   â”œâ”€â”€ dbt_project.yml            # Config projet dbt
â”‚   â”œâ”€â”€ profiles.yml               # Profils connexion dbt
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/               # Bronze â†’ Silver
â”‚   â”‚   â”‚   â”œâ”€â”€ sources.yml
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_events.sql
â”‚   â”‚   â”‚   â””â”€â”€ stg_users.sql
â”‚   â”‚   â””â”€â”€ marts/                 # Silver â†’ Gold
â”‚   â”‚       â”œâ”€â”€ schema.yml
â”‚   â”‚       â””â”€â”€ fct_events_enriched.sql
â”‚   â””â”€â”€ dbt_packages/
â”‚       â””â”€â”€ dbt_utils/             # Package dbt utils
â”‚
â”œâ”€â”€ minio_data/                     # Stockage MinIO (volumes)
â”‚   â”œâ”€â”€ bronze/
â”‚   â”œâ”€â”€ silver/
â”‚   â”œâ”€â”€ gold/
â”‚   â””â”€â”€ lakehouse/                 # MÃ©tadonnÃ©es Iceberg
â”‚
â”œâ”€â”€ spark_app_data/                 # Notebooks Jupyter
â”œâ”€â”€ spark_lakehouse_data/           # DonnÃ©es lakehouse
â”œâ”€â”€ postgres_data/                  # DonnÃ©es TimescaleDB
â”œâ”€â”€ chroma_data/                    # DonnÃ©es ChromaDB
â””â”€â”€ dbt_data/                       # Cache dbt
â”‚
â””â”€â”€ Documentation/
    â”œâ”€â”€ README.md                   # Documentation principale (EN)
    â”œâ”€â”€ README_FR.md                # Documentation complÃ¨te (FR)
    â”œâ”€â”€ QUICKSTART_FR.md            # Guide dÃ©marrage rapide
    â”œâ”€â”€ TRANSFORMATION_GUIDE_FR.md  # Guide transformations
    â”œâ”€â”€ AIRBYTE_MINIO_INTEGRATION.md # Integration Airbyte
    â”œâ”€â”€ MINIO_STRUCTURE_GUIDE.md    # Organisation MinIO
    â”œâ”€â”€ VERIFICATION_REPORT.md      # Rapport de vÃ©rification
    â””â”€â”€ VERSION_INFO.md             # Ce document
```

---

## ğŸš€ Commandes Essentielles

### Gestion du SystÃ¨me

```bash
# DÃ©marrer tous les services
docker-compose up -d

# VÃ©rifier l'Ã©tat
docker-compose ps

# Voir les logs
docker-compose logs -f spark-iceberg

# ArrÃªter le systÃ¨me
docker-compose down

# Reconstruction complÃ¨te
docker-compose down -v
docker-compose build --no-cache
docker-compose up -d
```

### dbt - Transformations

```bash
# ExÃ©cuter toutes les transformations
docker exec dbt dbt run

# ExÃ©cuter uniquement staging
docker exec dbt dbt run --select staging

# ExÃ©cuter uniquement marts
docker exec dbt dbt run --select marts

# Tester la qualitÃ© des donnÃ©es
docker exec dbt dbt test

# VÃ©rifier la connexion
docker exec dbt dbt debug

# GÃ©nÃ©rer la documentation
docker exec dbt dbt docs generate
```

### Spark SQL - RequÃªtes

```bash
# Lister les namespaces
docker exec spark-iceberg beeline -u jdbc:hive2://localhost:10000 -e "SHOW NAMESPACES;"

# Lister les tables d'un namespace
docker exec spark-iceberg beeline -u jdbc:hive2://localhost:10000 -e "SHOW TABLES IN bronze;"

# Compter les Ã©vÃ©nements
docker exec spark-iceberg beeline -u jdbc:hive2://localhost:10000 -e "SELECT COUNT(*) FROM bronze.raw_events;"

# RequÃªte sur les donnÃ©es enrichies
docker exec spark-iceberg beeline -u jdbc:hive2://localhost:10000 -e "SELECT * FROM default_gold.fct_events_enriched LIMIT 10;"

# Mode interactif Beeline
docker exec -it spark-iceberg beeline -u jdbc:hive2://localhost:10000
```

### MinIO - Gestion du Stockage

```bash
# AccÃ¨s web : http://localhost:9001
# Credentials : voir fichier .env

# Lister les buckets via CLI
docker exec mc mc ls bceao-data/

# VÃ©rifier le contenu de lakehouse
docker exec mc mc ls bceao-data/lakehouse/
```

---

## ğŸ“Š MÃ©triques du SystÃ¨me

### DonnÃ©es de Test

- **Bronze.raw_events**: 20 Ã©vÃ©nements de test
- **Bronze.raw_users**: 6 utilisateurs de test
- **Silver.stg_events**: 20 Ã©vÃ©nements nettoyÃ©s
- **Silver.stg_users**: 6 utilisateurs nettoyÃ©s
- **Gold.fct_events_enriched**: 20 Ã©vÃ©nements enrichis avec info utilisateurs

### Performance

- **Temps d'initialisation**: ~2 minutes
- **Temps de transformation dbt**: ~10-30 secondes
- **MÃ©moire requise**: Minimum 8GB RAM
- **Espace disque**: ~5GB (incluant toutes les images Docker)

---

## ğŸ¯ FonctionnalitÃ©s Principales

### âœ… ImplÃ©mentÃ©

- [x] Architecture MÃ©daillon (Bronze, Silver, Gold)
- [x] Tables Iceberg avec support ACID
- [x] Transformations SQL avec dbt
- [x] Jupyter Notebooks pour analyses ad-hoc
- [x] Spark Thrift Server pour connexions JDBC/ODBC
- [x] Stockage S3 avec MinIO
- [x] Catalogue Iceberg REST
- [x] TimescaleDB pour time-series
- [x] ChromaDB pour recherche vectorielle
- [x] Initialisation automatique du lakehouse
- [x] DonnÃ©es de test prÃ©-chargÃ©es
- [x] Tests de qualitÃ© de donnÃ©es dbt

### ğŸ”„ IntÃ©grations Possibles

- [ ] Airbyte pour ingestion de donnÃ©es
- [ ] Superset/Metabase pour visualisation
- [ ] Airflow pour orchestration
- [ ] Great Expectations pour data quality
- [ ] MLflow pour ML lifecycle

---

## ğŸ“– Documentation

### Guides Disponibles

1. **README.md** - Vue d'ensemble et quick start (EN)
2. **README_FR.md** - Documentation complÃ¨te (FR)
3. **QUICKSTART_FR.md** - Guide de dÃ©marrage rapide
4. **TRANSFORMATION_GUIDE_FR.md** - Guide des transformations dÃ©taillÃ©
5. **AIRBYTE_MINIO_INTEGRATION.md** - IntÃ©gration avec Airbyte
6. **MINIO_STRUCTURE_GUIDE.md** - Organisation des buckets MinIO
7. **VERIFICATION_REPORT.md** - Rapport de vÃ©rification systÃ¨me

### AccÃ¨s Web

| Interface | URL | Authentification |
|-----------|-----|------------------|
| MinIO Console | http://localhost:9001 | admin / password123 |
| Jupyter Notebook | http://localhost:8888 | Aucune (dÃ©sactivÃ©e) |
| Spark UI | http://localhost:4040 | Aucune |
| Iceberg REST | http://localhost:8181 | Aucune |
| ChromaDB | http://localhost:8010 | Aucune |

---

## ğŸ› DÃ©pannage

### ProblÃ¨mes Courants

**Services ne dÃ©marrent pas**
```bash
docker-compose logs
docker-compose down
docker-compose up -d
```

**dbt ne peut pas se connecter**
```bash
# VÃ©rifier que Thrift Server est prÃªt
docker-compose logs spark-iceberg | grep "ThriftBinaryCLIService"
# Attendre 2 minutes aprÃ¨s le dÃ©marrage
docker exec dbt dbt debug
```

**Jupyter n'est pas accessible**
```bash
docker exec spark-iceberg cat /opt/spark/logs/jupyter.log
docker-compose restart spark-iceberg
```

---

## ğŸ“ Notes de Version

### Version 1.0.0 (27 octobre 2025)

**Nouvelles fonctionnalitÃ©s**:
- âœ… Architecture complÃ¨te Bronze-Silver-Gold
- âœ… IntÃ©gration dbt avec Iceberg
- âœ… Documentation complÃ¨te en franÃ§ais
- âœ… Scripts d'initialisation automatique
- âœ… DonnÃ©es de test prÃ©-chargÃ©es

**AmÃ©liorations**:
- âœ¨ Configuration Spark optimisÃ©e pour Iceberg
- âœ¨ Support complet des transformations SQL
- âœ¨ Tests de qualitÃ© de donnÃ©es

**Corrections**:
- ğŸ› RÃ©solution des problÃ¨mes de connexion dbt
- ğŸ› Configuration correcte du catalogue Iceberg REST
- ğŸ› Gestion des chemins S3 dans MinIO

---

## ğŸ‘¥ Contributeurs

Projet dÃ©veloppÃ© pour la **BCEAO** (Banque Centrale des Ã‰tats de l'Afrique de l'Ouest)

---

## ğŸ“š Ressources Externes

- [Apache Iceberg Documentation](https://iceberg.apache.org/)
- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
- [dbt Documentation](https://docs.getdbt.com/)
- [MinIO Documentation](https://min.io/docs/)
- [TimescaleDB Documentation](https://docs.timescale.com/)
- [ChromaDB Documentation](https://docs.trychroma.com/)

---

**DerniÃ¨re mise Ã  jour**: 27 octobre 2025
