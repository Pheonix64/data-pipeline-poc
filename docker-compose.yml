networks:
  data-pipeline-net:
    driver: bridge

volumes:
  minio_data:
    driver: local
    driver_opts:
      type: none
      device: ./minio_data
      o: bind
  spark_app_data:
    driver: local
    driver_opts:
      type: none
      device: ./spark_app_data
      o: bind
  spark_lakehouse_data:
    driver: local
    driver_opts:
      type: none
      device: ./spark_lakehouse_data
      o: bind
  postgres_data:
    driver: local
    driver_opts:
      type: none
      device: ./postgres_data
      o: bind
  chroma_data:
    driver: local
    driver_opts:
      type: none
      device: ./chroma_data
      o: bind
  dbt_data:
    driver: local
    driver_opts:
      type: none
      device: ./dbt_data
      o: bind

services:
  # --- Data Lakehouse Storage ---
  minio:
    image: minio/minio
    container_name: minio
    networks:
      - data-pipeline-net
    volumes:
      - minio_data:/data
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      - MINIO_ROOT_USER=${MINIO_ROOT_USER}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
    command: server /data --console-address ":9001"
    healthcheck:
      test: [ "CMD-SHELL", "curl -fsS http://localhost:9000/minio/health/live || exit 1" ]
      interval: 30s
      timeout: 20s
      retries: 3
      start_period: 10s

  mc:
    image: minio/mc
    container_name: mc
    depends_on:
      minio:
        condition: service_healthy
    networks:
      - data-pipeline-net
    entrypoint: >
      /bin/sh -c "
        /usr/bin/mc alias set bceao-data http://minio:9000 ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD};
        /usr/bin/mc mb bceao-data/bronze || true;
        /usr/bin/mc mb bceao-data/silver || true;
        /usr/bin/mc mb bceao-data/gold || true;
        /usr/bin/mc mb bceao-data/lakehouse || true;
        exit 0;
      "

  # --- Transformation Engine: Spark with Iceberg ---
  spark-iceberg:
    build:
      context: .
      dockerfile: spark.Dockerfile
    image: data-pipeline-poc/spark-iceberg:latest
    container_name: spark-iceberg
    networks:
      - data-pipeline-net
    depends_on:
      minio:
        condition: service_healthy
      rest:
        condition: service_started
    volumes:
      - ./jars:/opt/spark/extra-jars
      - spark_app_data:/home/iceberg/notebooks/
      - spark_lakehouse_data:/home/iceberg/lakehouse/
      - ./spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
    ports:
      - "8888:8888" # Jupyter
      - "4040:4040" # Spark UI (Thrift Server UI)
      - "10000:10000" # Thrift Server
    environment:
      - MINIO_ROOT_USER=${MINIO_ROOT_USER}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
      - AWS_ACCESS_KEY_ID=${MINIO_ROOT_USER}
      - AWS_SECRET_ACCESS_KEY=${MINIO_ROOT_PASSWORD}
      - SPARK_HOME=/opt/spark
      - AWS_REGION=us-east-1
      - AWS_S3_FORCE_PATH_STYLE=true
      - SPARK_MASTER_HOST=spark-iceberg
      - SPARK_MASTER_PORT=7077
      - PYSPARK_PYTHON=python3
    entrypoint: [ "/bin/bash", "-c" ]
    command: >
      "sleep 15 && mkdir -p /opt/spark/logs && /opt/spark/init-scripts/init-lakehouse.sh && /opt/spark/sbin/start-thriftserver.sh && nohup jupyter notebook --ip=0.0.0.0 --port=8888 --no-browser --allow-root --NotebookApp.token='' --NotebookApp.password='' --notebook-dir=/home/iceberg/notebooks > /opt/spark/logs/jupyter.log 2>&1 & tail -f /opt/spark/logs/*.out /opt/spark/logs/jupyter.log 2>/dev/null || sleep infinity"
    healthcheck:
      test: [ "CMD-SHELL", "curl -f http://localhost:4040 || exit 1" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  rest:
    image: tabulario/iceberg-rest
    container_name: iceberg-rest
    networks:
      - data-pipeline-net
    ports:
      - "8181:8181"
    environment:
      - AWS_REGION=us-east-1
      - CATALOG_WAREHOUSE=s3://lakehouse/
      - CATALOG_IO__IMPL=org.apache.iceberg.aws.s3.S3FileIO
      - CATALOG_S3_ENDPOINT=http://minio:9000
      - CATALOG_S3_ACCESS__KEY__ID=${MINIO_ROOT_USER}
      - CATALOG_S3_SECRET__ACCESS__KEY=${MINIO_ROOT_PASSWORD}
      - CATALOG_S3_REGION=us-east-1
      - CATALOG_S3_PATH__STYLE__ACCESS=true
    volumes:
      - spark_lakehouse_data:/home/iceberg/lakehouse/

  # --- Serving Layer: Data Mart ---
  timescaledb:
    image: timescale/timescaledb-ha:pg15-latest
    container_name: timescaledb
    networks:
      - data-pipeline-net
    ports:
      - "5433:5433"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    environment:
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}

  # --- Intelligence Layer: Vector Index ---
  chromadb:
    image: chromadb/chroma
    container_name: chromadb
    networks:
      - data-pipeline-net
    ports:
      - "8010:8010"
    volumes:
      - chroma_data:/chroma/.chroma/
    environment:
      - ALLOW_RESET=true

  # --- Transformation Orchestrator: dbt ---
  dbt:
    image: ghcr.io/dbt-labs/dbt-spark
    container_name: dbt
    networks:
      - data-pipeline-net
    working_dir: /usr/app/dbt
    volumes:
      - ./dbt_project:/usr/app/dbt
      - ${USERPROFILE}/.dbt/profiles.yml:/root/.dbt/profiles.yml
    entrypoint: /bin/sh -c "sleep infinity"
