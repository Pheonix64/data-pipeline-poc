# Guide de Transformation des Donn√©es: Bronze ‚Üí Silver ‚Üí Gold

Ce guide d√©taille comment transformer les donn√©es √† travers les trois couches du Data Lakehouse.

## Table des Mati√®res

1. [Principes de Transformation](#principes-de-transformation)
2. [Bronze ‚Üí Silver avec dbt](#bronze--silver-avec-dbt)
3. [Silver ‚Üí Gold avec dbt](#silver--gold-avec-dbt)
4. [Bronze ‚Üí Silver ‚Üí Gold avec Spark/Jupyter](#avec-spark-et-jupyter)
5. [Bonnes Pratiques](#bonnes-pratiques)

---

## Principes de Transformation

### Architecture M√©daillon

```
BRONZE (Raw)          SILVER (Clean)          GOLD (Analytics)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ          ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ          ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚Ä¢ Donn√©es brutes      ‚Ä¢ Donn√©es nettoy√©es     ‚Ä¢ Donn√©es agr√©g√©es
‚Ä¢ Pas de validation   ‚Ä¢ Validation stricte    ‚Ä¢ Enrichissement
‚Ä¢ Format source       ‚Ä¢ Standardisation       ‚Ä¢ M√©triques m√©tier
‚Ä¢ Append-only         ‚Ä¢ D√©doublonnage         ‚Ä¢ Pr√™t pour BI
```

### R√®gles de Transformation

**Bronze ‚Üí Silver**:
- ‚úÖ Nettoyer les valeurs nulles
- ‚úÖ Standardiser les formats (dates, emails, etc.)
- ‚úÖ Valider les types de donn√©es
- ‚úÖ Ajouter m√©tadonn√©es de tra√ßabilit√©
- ‚ùå PAS de jointures
- ‚ùå PAS d'agr√©gations

**Silver ‚Üí Gold**:
- ‚úÖ Jointures entre tables
- ‚úÖ Agr√©gations et calculs
- ‚úÖ M√©triques m√©tier
- ‚úÖ D√©normalisation pour performance
- ‚úÖ Tables de faits et dimensions

---

## Bronze ‚Üí Silver avec dbt

### Exemple 1: Nettoyage des √âv√©nements

**Fichier**: `dbt_project/models/staging/stg_events.sql`

```sql
{{
  config(
    materialized='table',
    file_format='iceberg',
    schema='default_silver'
  )
}}

-- Transformation Bronze ‚Üí Silver pour les √©v√©nements
-- Objectif: Nettoyer et valider les donn√©es brutes

WITH source AS (
    SELECT * FROM {{ source('bronze', 'raw_events') }}
),

cleaned AS (
    SELECT
        -- Identifiants
        event_id,
        event_type,
        user_id,
        
        -- Nettoyage du timestamp
        CAST(event_timestamp AS TIMESTAMP) as event_timestamp,
        
        -- Nettoyage et standardisation des donn√©es
        TRIM(event_data) as event_data,
        
        -- M√©tadonn√©es de tra√ßabilit√©
        current_timestamp() as dbt_loaded_at,
        'bronze.raw_events' as source_table
        
    FROM source
    
    -- Validation: Supprimer les enregistrements invalides
    WHERE 
        event_id IS NOT NULL
        AND event_type IS NOT NULL
        AND user_id IS NOT NULL
        AND event_timestamp IS NOT NULL
)

SELECT * FROM cleaned
```

**Ex√©cution**:
```powershell
docker exec dbt dbt run --select stg_events
```

### Exemple 2: Nettoyage des Utilisateurs

**Fichier**: `dbt_project/models/staging/stg_users.sql`

```sql
{{
  config(
    materialized='table',
    file_format='iceberg',
    schema='default_silver'
  )
}}

-- Transformation Bronze ‚Üí Silver pour les utilisateurs
-- Objectif: Nettoyer, valider et standardiser

WITH source AS (
    SELECT * FROM {{ source('bronze', 'raw_users') }}
),

cleaned AS (
    SELECT
        -- Identifiant unique
        user_id,
        
        -- Nettoyage du nom
        TRIM(UPPER(SUBSTRING(user_name, 1, 1)) || LOWER(SUBSTRING(user_name, 2))) as user_name,
        
        -- Standardisation de l'email
        LOWER(TRIM(email)) as user_email,
        
        -- Validation du format email
        CASE 
            WHEN email LIKE '%@%.%' THEN LOWER(TRIM(email))
            ELSE NULL 
        END as user_email_validated,
        
        -- Dates
        CAST(created_at AS TIMESTAMP) as user_created_at,
        
        -- Calcul de l'anciennet√© (en jours)
        DATEDIFF(current_date(), CAST(created_at AS DATE)) as user_age_days,
        
        -- M√©tadonn√©es
        current_timestamp() as dbt_loaded_at,
        'bronze.raw_users' as source_table
        
    FROM source
    
    -- Validation
    WHERE 
        user_id IS NOT NULL
        AND user_name IS NOT NULL
        AND created_at IS NOT NULL
)

SELECT * FROM cleaned
```

### Exemple 3: D√©doublonnage des Donn√©es

**Fichier**: `dbt_project/models/staging/stg_events_dedup.sql`

```sql
{{
  config(
    materialized='table',
    file_format='iceberg',
    schema='default_silver'
  )
}}

-- D√©doublonnage des √©v√©nements
-- Prend la derni√®re version en cas de doublon

WITH source AS (
    SELECT * FROM {{ source('bronze', 'raw_events') }}
),

ranked AS (
    SELECT
        *,
        ROW_NUMBER() OVER (
            PARTITION BY event_id, user_id 
            ORDER BY event_timestamp DESC
        ) as rn
    FROM source
),

deduped AS (
    SELECT
        event_id,
        event_type,
        user_id,
        event_timestamp,
        event_data,
        current_timestamp() as dbt_loaded_at
    FROM ranked
    WHERE rn = 1
)

SELECT * FROM deduped
```

---

## Silver ‚Üí Gold avec dbt

### Exemple 1: Table de Faits - √âv√©nements Enrichis

**Fichier**: `dbt_project/models/marts/fct_events_enriched.sql`

```sql
{{
  config(
    materialized='table',
    file_format='iceberg',
    schema='default_gold'
  )
}}

-- Table de faits: √âv√©nements enrichis avec informations utilisateur
-- Objectif: Jointure et enrichissement pour l'analyse

WITH events AS (
    SELECT * FROM {{ ref('stg_events') }}
),

users AS (
    SELECT * FROM {{ ref('stg_users') }}
),

enriched AS (
    SELECT
        -- Dimensions √©v√©nement
        e.event_id,
        e.event_type,
        e.event_timestamp,
        e.event_data,
        
        -- Dimensions utilisateur (enrichissement)
        e.user_id,
        u.user_name,
        u.user_email,
        u.user_created_at,
        u.user_age_days,
        
        -- M√©triques calcul√©es
        CASE 
            WHEN e.event_type = 'login' THEN 1 
            ELSE 0 
        END as is_login,
        
        CASE 
            WHEN e.event_type = 'logout' THEN 1 
            ELSE 0 
        END as is_logout,
        
        -- Calcul de la dur√©e depuis la cr√©ation du compte
        DATEDIFF(e.event_timestamp, u.user_created_at) as days_since_registration,
        
        -- M√©tadonn√©es
        e.dbt_loaded_at
        
    FROM events e
    LEFT JOIN users u ON e.user_id = u.user_id
)

SELECT * FROM enriched
```

### Exemple 2: Table Agr√©g√©e - Statistiques Utilisateur

**Fichier**: `dbt_project/models/marts/dim_user_stats.sql`

```sql
{{
  config(
    materialized='table',
    file_format='iceberg',
    schema='default_gold'
  )
}}

-- Dimension: Statistiques par utilisateur
-- Objectif: Agr√©ger les m√©triques par utilisateur

WITH events AS (
    SELECT * FROM {{ ref('fct_events_enriched') }}
),

user_stats AS (
    SELECT
        -- Dimensions
        user_id,
        user_name,
        user_email,
        user_created_at,
        
        -- M√©triques d'activit√©
        COUNT(*) as total_events,
        COUNT(DISTINCT event_type) as unique_event_types,
        
        -- M√©triques par type d'√©v√©nement
        SUM(is_login) as total_logins,
        SUM(is_logout) as total_logouts,
        
        -- M√©triques temporelles
        MIN(event_timestamp) as first_event_date,
        MAX(event_timestamp) as last_event_date,
        DATEDIFF(MAX(event_timestamp), MIN(event_timestamp)) as activity_period_days,
        
        -- Taux d'engagement
        COUNT(*) / NULLIF(DATEDIFF(CURRENT_DATE(), user_created_at), 0) as avg_events_per_day,
        
        -- M√©tadonn√©es
        current_timestamp() as dbt_loaded_at
        
    FROM events
    GROUP BY 
        user_id,
        user_name,
        user_email,
        user_created_at
)

SELECT * FROM user_stats
```

### Exemple 3: Table Agr√©g√©e - Statistiques Quotidiennes

**Fichier**: `dbt_project/models/marts/fct_daily_activity.sql`

```sql
{{
  config(
    materialized='table',
    file_format='iceberg',
    schema='default_gold'
  )
}}

-- Fait: Activit√© quotidienne
-- Objectif: M√©triques agr√©g√©es par jour

WITH events AS (
    SELECT * FROM {{ ref('fct_events_enriched') }}
),

daily_stats AS (
    SELECT
        -- Dimensions temporelles
        CAST(event_timestamp AS DATE) as activity_date,
        EXTRACT(YEAR FROM event_timestamp) as activity_year,
        EXTRACT(MONTH FROM event_timestamp) as activity_month,
        EXTRACT(DAY FROM event_timestamp) as activity_day,
        DAYOFWEEK(event_timestamp) as day_of_week,
        
        -- M√©triques
        COUNT(*) as total_events,
        COUNT(DISTINCT user_id) as unique_users,
        COUNT(DISTINCT event_type) as unique_event_types,
        
        -- M√©triques par type
        SUM(is_login) as total_logins,
        SUM(is_logout) as total_logouts,
        
        -- M√©triques de qualit√©
        COUNT(*) / COUNT(DISTINCT user_id) as avg_events_per_user,
        
        -- M√©tadonn√©es
        current_timestamp() as dbt_loaded_at
        
    FROM events
    GROUP BY 
        activity_date,
        activity_year,
        activity_month,
        activity_day,
        day_of_week
)

SELECT * FROM daily_stats
ORDER BY activity_date DESC
```

**Ex√©cution des mod√®les Gold**:
```powershell
# Ex√©cuter tous les mod√®les marts
docker exec dbt dbt run --select marts

# Ex√©cuter un mod√®le sp√©cifique
docker exec dbt dbt run --select fct_daily_activity
```

---

## Avec Spark et Jupyter

### Configuration Initiale dans Jupyter

Ouvrez Jupyter: http://localhost:8888

**Cr√©er un nouveau notebook et configurer Spark**:

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *

# Cr√©er une session Spark avec Iceberg
spark = SparkSession.builder \
    .appName("Data Transformation") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.iceberg.spark.SparkCatalog") \
    .config("spark.sql.catalog.spark_catalog.type", "rest") \
    .config("spark.sql.catalog.spark_catalog.uri", "http://rest:8181") \
    .getOrCreate()

print("Spark session cr√©√©e avec succ√®s!")
spark.sql("SHOW NAMESPACES").show()
```

### Bronze ‚Üí Silver avec PySpark

```python
# ============================================
# TRANSFORMATION BRONZE ‚Üí SILVER avec PySpark
# ============================================

# 1. Lire les donn√©es brutes depuis Bronze
print("üìñ Lecture des donn√©es Bronze...")
df_raw_events = spark.table("bronze.raw_events")
df_raw_users = spark.table("bronze.raw_users")

print(f"√âv√©nements bruts: {df_raw_events.count()} lignes")
print(f"Utilisateurs bruts: {df_raw_users.count()} lignes")

# 2. Nettoyer les √©v√©nements
print("\nüßπ Nettoyage des √©v√©nements...")

df_events_clean = df_raw_events \
    .filter(col("event_id").isNotNull()) \
    .filter(col("event_type").isNotNull()) \
    .filter(col("user_id").isNotNull()) \
    .withColumn("event_timestamp", col("event_timestamp").cast("timestamp")) \
    .withColumn("event_data", trim(col("event_data"))) \
    .withColumn("dbt_loaded_at", current_timestamp()) \
    .withColumn("source_table", lit("bronze.raw_events"))

# Afficher un aper√ßu
df_events_clean.show(5, truncate=False)

# 3. D√©doublonner les √©v√©nements
print("\nüîÑ D√©doublonnage...")

from pyspark.sql.window import Window

window_spec = Window.partitionBy("event_id", "user_id").orderBy(col("event_timestamp").desc())

df_events_dedup = df_events_clean \
    .withColumn("rn", row_number().over(window_spec)) \
    .filter(col("rn") == 1) \
    .drop("rn")

print(f"Apr√®s d√©doublonnage: {df_events_dedup.count()} lignes")

# 4. Nettoyer les utilisateurs
print("\nüßπ Nettoyage des utilisateurs...")

df_users_clean = df_raw_users \
    .filter(col("user_id").isNotNull()) \
    .filter(col("user_name").isNotNull()) \
    .withColumn("user_name", trim(col("user_name"))) \
    .withColumn("user_email", lower(trim(col("email")))) \
    .withColumn("user_created_at", col("created_at").cast("timestamp")) \
    .withColumn("user_age_days", datediff(current_date(), col("created_at"))) \
    .withColumn("dbt_loaded_at", current_timestamp()) \
    .select(
        "user_id",
        "user_name",
        "user_email",
        "user_created_at",
        "user_age_days",
        "dbt_loaded_at"
    )

df_users_clean.show(5, truncate=False)

# 5. √âcrire dans Silver
print("\nüíæ √âcriture dans Silver...")

# Cr√©er le namespace Silver si n√©cessaire
spark.sql("CREATE NAMESPACE IF NOT EXISTS silver")

# √âcrire les √©v√©nements nettoy√©s
df_events_dedup.writeTo("silver.stg_events") \
    .using("iceberg") \
    .createOrReplace()

print("‚úÖ silver.stg_events cr√©√©!")

# √âcrire les utilisateurs nettoy√©s
df_users_clean.writeTo("silver.stg_users") \
    .using("iceberg") \
    .createOrReplace()

print("‚úÖ silver.stg_users cr√©√©!")

# 6. V√©rification
print("\nüîç V√©rification des tables Silver...")
spark.sql("SHOW TABLES IN silver").show()

print("\nContenu de silver.stg_events:")
spark.table("silver.stg_events").show(5)

print("\nContenu de silver.stg_users:")
spark.table("silver.stg_users").show(5)
```

### Silver ‚Üí Gold avec PySpark

```python
# ============================================
# TRANSFORMATION SILVER ‚Üí GOLD avec PySpark
# ============================================

# 1. Lire les donn√©es Silver
print("üìñ Lecture des donn√©es Silver...")
df_events = spark.table("silver.stg_events")
df_users = spark.table("silver.stg_users")

# 2. Enrichissement: Joindre √©v√©nements et utilisateurs
print("\nüîó Jointure et enrichissement...")

df_enriched = df_events.alias("e") \
    .join(
        df_users.alias("u"),
        col("e.user_id") == col("u.user_id"),
        "left"
    ) \
    .select(
        # Colonnes √©v√©nement
        col("e.event_id"),
        col("e.event_type"),
        col("e.event_timestamp"),
        col("e.event_data"),
        
        # Colonnes utilisateur
        col("e.user_id"),
        col("u.user_name"),
        col("u.user_email"),
        col("u.user_created_at"),
        col("u.user_age_days"),
        
        # M√©triques calcul√©es
        when(col("e.event_type") == "login", 1).otherwise(0).alias("is_login"),
        when(col("e.event_type") == "logout", 1).otherwise(0).alias("is_logout"),
        datediff(col("e.event_timestamp"), col("u.user_created_at")).alias("days_since_registration"),
        
        # M√©tadonn√©es
        col("e.dbt_loaded_at")
    )

df_enriched.show(10, truncate=False)

# 3. Agr√©gation: Statistiques par utilisateur
print("\nüìä Calcul des statistiques par utilisateur...")

df_user_stats = df_enriched.groupBy(
    "user_id",
    "user_name",
    "user_email",
    "user_created_at"
).agg(
    count("*").alias("total_events"),
    countDistinct("event_type").alias("unique_event_types"),
    sum("is_login").alias("total_logins"),
    sum("is_logout").alias("total_logouts"),
    min("event_timestamp").alias("first_event_date"),
    max("event_timestamp").alias("last_event_date"),
    (datediff(max("event_timestamp"), min("event_timestamp"))).alias("activity_period_days")
).withColumn(
    "avg_events_per_day",
    col("total_events") / when(col("activity_period_days") > 0, col("activity_period_days")).otherwise(1)
).withColumn(
    "dbt_loaded_at",
    current_timestamp()
)

df_user_stats.show(truncate=False)

# 4. Agr√©gation: Activit√© quotidienne
print("\nüìÖ Calcul de l'activit√© quotidienne...")

df_daily_activity = df_enriched \
    .withColumn("activity_date", to_date(col("event_timestamp"))) \
    .withColumn("activity_year", year(col("event_timestamp"))) \
    .withColumn("activity_month", month(col("event_timestamp"))) \
    .withColumn("activity_day", dayofmonth(col("event_timestamp"))) \
    .withColumn("day_of_week", dayofweek(col("event_timestamp"))) \
    .groupBy(
        "activity_date",
        "activity_year",
        "activity_month",
        "activity_day",
        "day_of_week"
    ).agg(
        count("*").alias("total_events"),
        countDistinct("user_id").alias("unique_users"),
        countDistinct("event_type").alias("unique_event_types"),
        sum("is_login").alias("total_logins"),
        sum("is_logout").alias("total_logouts")
    ).withColumn(
        "avg_events_per_user",
        col("total_events") / col("unique_users")
    ).withColumn(
        "dbt_loaded_at",
        current_timestamp()
    ).orderBy(col("activity_date").desc())

df_daily_activity.show(truncate=False)

# 5. √âcrire dans Gold
print("\nüíæ √âcriture dans Gold...")

# Cr√©er le namespace Gold
spark.sql("CREATE NAMESPACE IF NOT EXISTS gold")

# √âcrire la table enrichie
df_enriched.writeTo("gold.fct_events_enriched") \
    .using("iceberg") \
    .createOrReplace()

print("‚úÖ gold.fct_events_enriched cr√©√©!")

# √âcrire les statistiques utilisateur
df_user_stats.writeTo("gold.dim_user_stats") \
    .using("iceberg") \
    .createOrReplace()

print("‚úÖ gold.dim_user_stats cr√©√©!")

# √âcrire l'activit√© quotidienne
df_daily_activity.writeTo("gold.fct_daily_activity") \
    .using("iceberg") \
    .createOrReplace()

print("‚úÖ gold.fct_daily_activity cr√©√©!")

# 6. V√©rification finale
print("\nüîç V√©rification des tables Gold...")
spark.sql("SHOW TABLES IN gold").show()

print("\nüìä R√©sum√© des donn√©es Gold:")
print(f"√âv√©nements enrichis: {spark.table('gold.fct_events_enriched').count()} lignes")
print(f"Statistiques utilisateur: {spark.table('gold.dim_user_stats').count()} lignes")
print(f"Activit√© quotidienne: {spark.table('gold.fct_daily_activity').count()} lignes")
```

---

## Bonnes Pratiques

### 1. Nommage des Tables

```
Bronze:  raw_<nom>              ex: raw_events, raw_users
Silver:  stg_<nom>              ex: stg_events, stg_users
Gold:    fct_<nom> / dim_<nom>  ex: fct_events_enriched, dim_user_stats
```

### 2. Tra√ßabilit√©

Toujours ajouter:
- `dbt_loaded_at`: Timestamp de chargement
- `source_table`: Table source
- `processing_date`: Date de traitement (pour partitionnement)

### 3. Validation des Donn√©es

```sql
-- Tests dbt dans schema.yml
tests:
  - not_null
  - unique
  - relationships
  - accepted_values
```

### 4. Partitionnement Iceberg

Pour les grandes tables:

```sql
{{
  config(
    materialized='table',
    file_format='iceberg',
    partition_by=['year(event_timestamp)', 'month(event_timestamp)']
  )
}}
```

### 5. Incr√©mental avec dbt

Pour mise √† jour incr√©mentale:

```sql
{{
  config(
    materialized='incremental',
    file_format='iceberg',
    unique_key='event_id'
  )
}}

SELECT *
FROM {{ source('bronze', 'raw_events') }}

{% if is_incremental() %}
  WHERE event_timestamp > (SELECT MAX(event_timestamp) FROM {{ this }})
{% endif %}
```

### 6. Documentation

Documenter chaque mod√®le dans `schema.yml`:

```yaml
models:
  - name: stg_events
    description: √âv√©nements nettoy√©s depuis bronze
    columns:
      - name: event_id
        description: Identifiant unique de l'√©v√©nement
        tests:
          - not_null
          - unique
```

---

## Commandes Utiles

```powershell
# Ex√©cuter toute la cha√Æne
docker exec dbt dbt run

# Ex√©cuter Bronze ‚Üí Silver uniquement
docker exec dbt dbt run --select staging

# Ex√©cuter Silver ‚Üí Gold uniquement
docker exec dbt dbt run --select marts

# Tester la qualit√© des donn√©es
docker exec dbt dbt test

# V√©rifier les tables
docker exec spark-iceberg beeline -u jdbc:hive2://localhost:10000 -e "SHOW NAMESPACES;"
```

## Conclusion

Ce guide vous a montr√© comment:
- ‚úÖ Nettoyer les donn√©es (Bronze ‚Üí Silver)
- ‚úÖ Enrichir et agr√©ger (Silver ‚Üí Gold)
- ‚úÖ Utiliser dbt et Spark/Jupyter
- ‚úÖ Suivre les bonnes pratiques

**Prochaine √©tape**: Consultez le [Guide dbt Avanc√©](./DBT_ADVANCED_FR.md) pour des transformations plus complexes!
